<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="http://nielsberglund.com/atom.xml" rel="self"/>
  <link href="http://nielsberglund.com"/>
  <updated>2018-10-07T09:34:44+02:00</updated>
  <id>http://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 40]]></title>
    <link href="http://nielsberglund.com/2018/10/07/interesting-stuff---week-40/" rel="alternate" type="text/html"/>
    <updated>2018-10-07T09:34:44+02:00</updated>
    <id>http://nielsberglund.com/2018/10/07/interesting-stuff---week-40/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/04/update-on-net-core-3-0-and-net-framework-4-8/">Update on .NET Core 3.0 and .NET Framework 4.8</a>. A blog post from the .NET engineering team, where they talk about the future of the .NET Framework and .NET Core. I wonder if this post was prompted by speculations recently about the future of the .NET Framework, where there were questions whether the .NET Framework 4.8 would be the last version, and all development would be concentrated on .NET Core.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/enabling-real-time-data-warehousing-with-azure-sql-data-warehouse/">Enabling real-time data warehousing with Azure SQL Data Warehouse</a>. This post is an announcement how <a href="https://www.striim.com/">Striim</a> now fully supports SQL Data Warehouse as a target for Striim for Azure. Striim is a system which enables continuous non-intrusive performant ingestion of enterprise data from a variety of sources in real time.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/event-streaming-new-big-thing-finance">Is Event Streaming the New Big Thing for Finance?</a>. An excellent blog post by <a href="https://twitter.com/benstopford">Ben Stopford</a> where he discusses the use of event streaming in the financial sector.</li>
<li><a href="https://www.confluent.io/blog/troubleshooting-ksql-part-2">Troubleshooting KSQL – Part 2: What’s Happening Under the Covers?</a>. The second post by <a href="https://twitter.com/rmoff">Robin Moffat</a> about debugging of KSQL. In this post - Robin, as the title says, goes under the covers to figure out what happens with KSQL queries.</li>
<li><a href="https://data-artisans.com/blog/6-things-to-consider-when-defining-your-apache-flink-cluster-size">6 things to consider when defining your Apache Flink cluster size</a>.  This post discusses how to plan and calculate a Flink cluster size. In other words; how to define the number of resources you need to run a specific Flink job.</li>
</ul>

<h2 id="ms-ignite">MS Ignite</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/2018/10/02/syllabuck-ignite-2018-conference/">Syllabuck: Ignite 2018 Conference</a>. A great list of MS Ignite sessions that <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a> found interesting! Now I know what to do in my spare time!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/10/03/customized-regression-model-for-airbnb-dynamic-pricing/">Customized regression model for Airbnb dynamic pricing</a>. This post by <a href="https://twitter.com/adriancolyer">Adrian</a> is about a white-paper which details the methods that Airbnb use to suggest prices to listing hosts.</li>
<li><a href="https://towardsdatascience.com/cleaning-and-preparing-data-in-python-494a9d51a878">Cleaning and Preparing Data in Python</a>. A post which lists Python methods and functions that helps to clean and prepare data.</li>
<li><a href="https://www.microsoft.com/en-us/research/blog/the-microsoft-infer-net-machine-learning-framework-goes-open-source/">The Microsoft Infer.NET machine learning framework goes open source</a>. A blog post from Microsoft Research, in which they announce the open-sourcing of <a href="https://dotnet.github.io/infer/">Infer.NET</a>. Is anyone else but me somewhat confused about the various data science frameworks that Microsoft has?</li>
<li><a href="https://towardsdatascience.com/how-to-build-a-simple-recommender-system-in-python-375093c3fb7d">How to build a Simple Recommender System in Python</a>. A blog post which discusses what a recommender system is and how you can use Python to build one.</li>
</ul>

<h2 id="what-is-niels-doing-wind">What Is Niels Doing (WIND)</h2>

<p>That is a good question! As you know, I wrote two blog posts about SQL Server 2019:</p>

<ul>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a></li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a></li>
</ul>

<p>My plan was to relatively quickly follow up those two posts with a third post how to run <strong>SQL Server Machine Learning Services</strong> on <strong>SQL Server 2019 on Linux</strong>, and do it inside a Docker container. After having spent some time trying to get it to work, (with no luck), I gave up and contacted a couple of persons in MS asking for help. The response was that, right now in <strong>SQL Server 2019 on Linux CTP 2.0</strong>, you cannot do it - bummer! The functionality will be in a future release.</p>

<p>I am now reworking the post I had started on to cover <strong>SQL Server Machine Learning Services</strong> in an <strong>Ubuntu</strong> based <strong>SQL Server 2019 on Linux</strong>. I should be able to publish something within a week or two.</p>

<p>I am also working on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series (still). Right now I have no idea when I can publish it - Sorry!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 39]]></title>
    <link href="http://nielsberglund.com/2018/09/30/interesting-stuff---week-39/" rel="alternate" type="text/html"/>
    <updated>2018-09-30T13:13:43+02:00</updated>
    <id>http://nielsberglund.com/2018/09/30/interesting-stuff---week-39/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://adamsitnik.com/ETW-Profiler/">Profiling .NET Code with BenchmarkDotNet</a>. If you want to benchmark your .NET code, you probably use <a href="https://benchmarkdotnet.org/">BenchMarkDotNet</a> (if you do not, you should). The man behind BenchMarkDotNet is <a href="https://twitter.com/SitnikAdam">Adam Sitnik</a>, and in the linked blog post he announces how you, soon, can use the EtwProfiler to profile benchmarked code! Very cool!</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/09/26/the-design-and-implementation-of-modern-column-oriented-database-systems/">The design and implementation of modern column-oriented database systems</a>. In this post, <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white paper about column-oriented databases. Having worked a little bit with SQL Server&rsquo;s column store indexes, it is very cool to get the &ldquo;lowdown&rdquo; on the design behind it.</li>
</ul>

<h2 id="azure-cloud">Azure Cloud</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/azure-databricks-delta-in-preview-9-regions-added-and-other-exciting-announcements/">Azure Databricks – Delta in preview, 9 regions added, and other exciting announcements</a>. A blog post announcing that Azure Databricks Delta is available in preview. This is very interesting since I have been &ldquo;chomping at the bits&rdquo;, to do some tests with Databricks Delta.</li>
<li><a href="https://azure.microsoft.com/en-us/blog/spark-debugging-and-diagnosis-toolset-for-azure-hdinsight/">Spark Debugging and Diagnosis Toolset for Azure HDInsight</a>. This post is another announcement from Microsoft. This time it is how <strong>Spark Diagnosis Toolset for HDInsight</strong> is now available in preview. The toolset allows you to identify low parallelization, to detect data skew and run data skew analysis, and quite a lot more.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-presence-detection-apache-kafka-aws">Real-Time Presence Detection at Scale with Apache Kafka on AWS</a>. This post discusses how <a href="https://www.zenreach.com/">Zenreach</a> has implemented a framework for real-time presence detection, using Kafka Streams.</li>
<li><a href="https://data-artisans.com/blog/state-ttl-for-apache-flink-how-to-limit-the-lifetime-of-state">State TTL for Apache Flink: How to Limit the Lifetime of State</a>. Instead of me summarising the post, I shamelessly copy the opening paragraph: <em>A common requirement for many stateful streaming applications is the ability to control how long application state can be accessed (e.g., due to legal regulations like GDPR) and when to discard it. This blog post introduces the state time-to-live (TTL) feature that was added to Apache Flink with the 1.6.0 release</em>. It is very, very interesting. I need to start to play around with Flink!</li>
<li><a href="https://www.confluent.io/blog/troubleshooting-ksql-part-1">Troubleshooting KSQL – Part 1: Why Isn’t My KSQL Query Returning Data?</a>. The obligatory Kafka link. The post is the first in a series how to troubleshoot KSQL. This and future posts in the series is, and, will be required reading for our Kafka team!</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/azure-data-studio-for-sql-server/">Azure Data Studio for SQL Server</a>. A post by <a href="https://twitter.com/vickyharp">Vicky Harp</a>. Vicky is Principal Program Manager Lead at Microsoft for SQL Server tooling, and in the post, she introduces <strong>Azure Data Studio</strong> (the artist formerly known as SQL Operations Studio). Azure Data Studio is a new cross-platform desktop environment for both on-premises and cloud data platforms on Windows, MacOS, and Linux.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/24/sql-server-2019-preview-combines-sql-server-and-apache-spark-to-create-a-unified-data-platform/">SQL Server 2019 preview combines SQL Server and Apache Spark to create a unified data platform</a>. An announcement by Microsoft how SQL Server 2019 comes with support for both Spark as well as Hadoop File System (HDFS). We do live in exciting times!</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/introducing-microsoft-sql-server-2019-big-data-clusters/">Introducing Microsoft SQL Server 2019 Big Data Clusters</a>. This post builds on top of the post above. It discusses how we can create big data clusters utilising the support in SQL Server 2019 of Spark and HDFS.</li>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a>. A post by yours truly. I do a quick look at what is new in SQL Server 2019, and I especially look at the Java language extension.</li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a>. Another post my myself. Since SQL Server 2019 for Linux now have support for SQL Server Machine Learning Services, I want to have a look at how it works. For that I obviously need it installed and I decided to install it as a Docker for Windows container. The post walks through what I did to get it installed. The post also discusses Azure Data Studio briefly.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 for Linux in Docker on Windows]]></title>
    <link href="http://nielsberglund.com/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/" rel="alternate" type="text/html"/>
    <updated>2018-09-29T12:06:09+02:00</updated>
    <id>http://nielsberglund.com/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/</id>
    <content type="html"><![CDATA[<p>By the time I publish this blog post <a href="https://www.microsoft.com/en-us/ignite">MS Ignite</a> is over. During Ignite, Microsoft announced quite a few new things, amongst them <strong>SQL Server 2019</strong> with a whole lot of new features and functionality.</p>

<p>I touched briefly on some of them in my <a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a> post. A couple of things that caught my eye were that <strong>SQL Server 2019 for Linux</strong> now supports In-Database analytics, what we know as <strong>SQL Server Machine Learning Services</strong> (R and Python), as well as the Java language extension.</p>

<blockquote>
<p><strong>NOTE:</strong> You may ask yourself what the Java language extension is; well, that is the ability to access Java code from T-SQL. It is a little bit like SQLCLR, but it executes outside of the SQL Server memory and process space.</p>
</blockquote>

<p>Seeing that I never really have played around with <em>SQL Server for Linux</em>, mostly due to that in previous versions (2017) it did not have support for In-Database analytics, I thought that now would be a good time to have a look.</p>

<p></p>

<p>Cool, so install <em>SQL Server 2019 for Linux</em> and start to play around! Hmm, what do I install it on - I am a Windows guy, this whole Linux thing is &ldquo;scary&rdquo;. Ok, I guess I could spin up a virtual machine and install it there, but I am lazy. Create a VM, and then install SQL Server seemed like too much work.</p>

<p>Then I thought about my mate and colleague <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>, a.k.a <a href="https://charlla.com/kafka-donuts/">The Donut Maker</a>, and how he raves about Docker. So maybe I should run <em>SQL Server 2019 for Linux</em> in a container, problem solved. Uh, maybe not; you see - I have never used Docker. I am an old guy (some would even call me a &ldquo;Grumpy Old Man&rdquo;, a <em>GOM</em>), and you know the saying about old dogs and new tricks.</p>

<p>So anyway, I decided to give it a go; how hard can it be (it turns out not hard at all), and this post is about the steps I took to get <em>SQL Server 2019 for Linux</em> running in Docker on Windows.</p>

<h2 id="docker-for-windows">Docker for Windows</h2>

<p>This post does not cover how to download and install Docker for Windows, as there are lots of posts out there about it. If you want somewhere to start; <a href="https://docs.docker.com/v17.09/docker-for-windows/">Get started with Docker for Windows</a> is an excellent starting point.</p>

<p>I do, however, want to point out a couple of things, that caught me out:</p>

<ul>
<li>Hyper-V needs to be enabled on your host computer. This means you cannot run Virtual Box VM&rsquo;s at the same time.</li>
<li>When you install Docker, you decide whether you want to run Linux or Windows containers. So, if you install Docker for Windows intending to run <em>SQL Server 2019 for Linux</em>, you choose Linux containers.</li>
</ul>

<p>You can change the choice between Linux and Windows containers from the Docker icon in the system tray (right click on the icon):</p>

<p><img src="/images/posts/sql_2k19Docker_container_type.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Change Container Type</em></p>

<p>In <em>Figure 1</em> we see the menu entry to change the Docker container type to Windows. To change container type works the other way around as well; changing from Windows to Linux.</p>

<h2 id="docker-basics">Docker Basics</h2>

<p>Before we look at how to get and install the SQL Server &ldquo;stuff&rdquo; let us discuss some basics, and let us start with some vocabulary:</p>

<ul>
<li>Layer: a set of read-only files or commands that describe how to set up the underlying system beneath the container</li>
<li>Image: this is the piece of &ldquo;something&rdquo;, in our case <em>SQL Server 2019 for Linux</em>, that you want to install. The image consists of one or more layers.</li>
<li>Container: you download an image and create a container, and this is what you interact with.</li>
<li>Registry: where images are stored and delivered from.</li>
</ul>

<p>In our case we:</p>

<ul>
<li>Connect to a registry which contains a <em>SQL Server 2019 for Linux</em> image.</li>
<li>We download the image and create a container.</li>
<li>We &ldquo;run&rdquo; the container and interact with SQL Server.</li>
</ul>

<p>The interaction with Docker (download image, create a container, etc.) is via CLI (Command Line Interface), using the <code>docker</code> base command followed by child commands and options/parameters (<code>docker childcommand</code>). Examples of child commands:</p>

<ul>
<li><code>login</code>: logs in to a Docker registry.</li>
<li><code>pull</code>: retrieve an image from a registry.</li>
<li><code>images</code>: returns a list of images on the machine.</li>
<li><code>run</code>: creates a new container from an image and starts it. If the image has not been <code>pull</code>:ed yet, it also pulls the image.</li>
<li><code>ps</code>: Lists containers.</li>
<li><code>exec</code>: executes a command in a container. For example, you want to run a command shell in the container.</li>
<li><code>stop</code>: stops a running container.</li>
<li><code>start</code>: starts up an existing stopped container.</li>
<li><code>rm</code>: removes a container.</li>
</ul>

<p>To see a full list of commands you can go <a href="https://docs.docker.com/engine/reference/commandline/docker/">here</a>.</p>

<p>As I mentioned above, we interact with Docker via the command line, and when you are on Windows, you most likely use <em>Powershell</em>. In this post I do it somewhat differently in that I do not use the actual <em>Powershell</em> shell, but instead <strong>Azure Data Studio</strong>.</p>

<h2 id="azure-data-studio">Azure Data Studio</h2>

<p>What is Azure Data Studio then? Well, it is the evolution of SQL Operations Studio. The blog post <a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/azure-data-studio-for-sql-server/">Azure Data Studio for SQL Server</a>, introduces it like so:</p>

<p>*Azure Data Studio is a new cross-platform desktop environment for data professionals using the family of on-premises and cloud data platforms on Windows, MacOS, and Linux. Previously released under the preview name SQL Operations Studio, Azure Data Studio offers a modern editor experience with lightning fast IntelliSense, code snippets, source control integration, and an <strong>integrated terminal</strong>. It is engineered with the data platform user in mind, with built-in charting of query resultsets and customizable dashboards.*</p>

<p>We can think what we want about the &ldquo;blurb&rdquo; above, but <em>ADS</em> does have some interesting features, and for the Docker CLI work we use the integrated terminal:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Azure Data Studio and Integrated Terminal</em></p>

<p>What we see in <em>Figure 2</em> is <em>ADS</em> with visualised resultsets, some dashboards and - outlined in red - the integrated terminal. Now, let us get down to business.</p>

<h2 id="getting-the-sql-server-2019-for-linux-image">Getting the SQL Server 2019 for Linux Image</h2>

<p>We get the <em>SQL Server 2019 for Linux</em> Docker image from the <a href="https://azure.microsoft.com/en-us/blog/microsoft-syndicates-container-catalog/">Microsoft Container Registry</a> (MCR). MCR acts as a single download source for Microsoft’s container images. Regardless of where customers discover Microsoft images, the pull source is <a href="https://azure.microsoft.com/en-us/services/container-registry/">mcr.microsoft.com</a>.</p>

<p>To get the image I open <em>Azure Data Studio</em>:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio2.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Helper Files</em></p>

<p>We see in <em>Figure 3</em> how I have the <code>2k19_linux.ps</code> file open in the <em>ADS</em> editor, and how that file contains some Docker commands. I open the integrated terminal in <em>ADS</em> through <strong>Ctrl + `</strong>, or by using the menu: &ldquo;View | Command Palette | View: Toggle Integrated Terminal&rdquo;:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio3.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Integrated Terminal</em></p>

<p>In <em>Figure 4</em> we see how the terminal is open (outlined in red) and it is the Powershell terminal (highlighted in red).</p>

<blockquote>
<p><strong>NOTE:</strong> The reason I use <em>ADS</em> is that I wanted to see what I can do with it, I could as easily have used the <em>Powershell</em> shell.</p>
</blockquote>

<p>Let us now get the SQL Server 2019 image, and I do it by copying the <code>docker pull ...</code>command from the file to the terminal and hit enter. In the terminal you now see something like so (output edited for readability):</p>

<pre><code class="language-bash">PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt; 
    docker pull mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
vNext-CTP2.0-ubuntu: Pulling from mssql/server
b234f539f7a1: Downloading [========&gt; ]  7.519MB/43.12MB
55172d420b43: Download complete
5ba5bbeb6b91: Download complete
43ae2841ad7a: Download complete
f6c9c6de4190: Download complete
28f02293f049: Download complete
5eb40916d530: Downloading [&gt;         ]   1.08MB/70.39MB
46e88947bdd0: Downloading [=&gt;        ]  8.634MB/414.5MB
26983ce22a89: Waiting
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Doing a Pull</em></p>

<p>We see in <em>Code Snippet 1</em> how Docker retrieves the image. In fact, it retrieves the layers the image consists of. The layers are identified by the <code>b234f539f7a1</code>, <code>55172d420b43</code>, and so forth as we see in <em>Code Snippet 1</em>. Eventually, the <code>pull</code> finishes, and we see in the terminal:</p>

<pre><code class="language-bash">PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt; 
    docker pull mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
vNext-CTP2.0-ubuntu: Pulling from mssql/server
b234f539f7a1: Pull complete
55172d420b43: Pull complete
5ba5bbeb6b91: Pull complete
43ae2841ad7a: Pull complete
f6c9c6de4190: Pull complete
28f02293f049: Pull complete
5eb40916d530: Pull complete
46e88947bdd0: Pull complete
26983ce22a89: Pull complete
Digest: sha256:87e691e2e5f738fd64a427ebe935e4e5ccd...
Status: Downloaded newer image for 
    mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Pull Finished</em></p>

<p>After the <code>pull</code> command has finished, we can check what images we have by executing <code>docker images</code>. When I do it on my machine I see this:</p>

<p><img src="/images/posts/sql_2k19Docker_pulled images.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Pulled Docker Images</em></p>

<p>We see from <em>Figure 5</em> how the SQL Server image now exists on the machine.</p>

<h4 id="creating-a-container">Creating a Container</h4>

<p>Cool, we have an image. However, an image is just that, an image, and you cannot interact with it. To relate it to SQL Server, think about the image as an <code>.iso</code> install file. We need to &ldquo;install&rdquo; the image, e.g. create and run a container. For this we use the second <code>docker</code> command from  <em>Figure 3</em> above, and it looks like so:</p>

<pre><code class="language-bash">docker run -e &quot;ACCEPT_EULA=Y&quot; \ 
           -e &quot;SA_PASSWORD=&lt;Strong!Passw0rd&gt;&quot; \
           -p 1433:1433 \
           --name sql2k19_1 \
           -d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create &amp; Run a Container</em></p>

<p>In <em>Code Snippet 3</em> we see how we use the <code>docker run</code> command to create the container. Let us look at the options:</p>

<ul>
<li><code>-e &quot;ACCEPT_EULA=Y&quot;</code>: As creating the container also installs SQL Server, we need to accept the SQL Server EULA. The <code>-e</code> option (also <code>--env</code>) sets environment variables. In this case, environment variables SQL Server requires.</li>
<li><code>-e &quot;SA_PASSWORD=&lt;Strong!Passw0rd&gt;&quot;</code>: A second environment variable. When running SQL Server in a container, you need to set a password which follows the SQL Server default password policy. Otherwise, the container can not setup SQL server and will stop working. By default, the password must be at least 8 characters long and contain characters from three of the following four sets: Uppercase letters, Lowercase letters, Base 10 digits, and Symbols.</li>
<li><code>-p 1433:1433</code>: The <code>-p</code> (or <code>--expose</code>) option binds a port on the host machine (to the left of the colon) to a port on the container. If you run multiple SQL Server containers, the SQL Server container uses port 1433 by default, and you should use different port numbers for the host machine: <code>-p 1401:1433</code> for example.</li>
<li><code>--name sql2k19_1</code>: The <code>--name</code> option assigns a name to the container. This is like a SQL Server instance name.</li>
<li><code>-d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu</code>: This indicates which image to create a container from. The <code>-d</code> option tells Docker we want to run the container detached from the calling process. In other words, it is still up and running after you close the terminal.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> I mentioned above about the <code>-p</code> option that if you run multiple instances you should have different host ports. This is also true if you run a non Docker SQL Server instance on you machine.</p>
</blockquote>

<p>After we execute the code in <em>Code Snippet 3</em> we can check that we have a new container: <code>docker ps</code>:</p>

<p><img src="/images/posts/sql_2k19Docker_created_container.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Docker Container</em></p>

<p>From what we see in <em>Figure 6</em>, it looks like we are in business! If we want to we can connect into the container and, for example, run a bash shell:</p>

<pre><code class="language-bash">docker exec -it sql2k19_1  /bin/bash
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Run bash Shell in the Container</em></p>

<p>That is all well and good, but what about SQL Server?</p>

<h2 id="test-the-container">Test the Container</h2>

<p>Right, so now we have a container, and that container hopefully runs SQL Server. Let us try and connect to the SQL Server via <em>ADS</em>.</p>

<p>So I switch from the <em>Explorer</em> view to <em>Servers</em>: <strong>Ctrl + G</strong>, and I click <em>New Connection</em>:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_new_connection.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>New Connection</em></p>

<p>The <em>New Connection</em> is what is highlighted in red in <em>Figure 7</em>, and clicking it I get a connection dialog:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_connect.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>New Connection</em></p>

<p>In the connection dialog we see, in <em>Figure 8</em>, how I want to connect to localhost (the highlighted &ldquo;.&rdquo; in the <code>Server</code> text box), the password is whatever password I set in <em>Code Snippet 3</em>, and I chose to give the connection a name (the highlighted part in the <code>Name</code> text box). So if everything works, when I click on <em>Connect</em> I should see something like so:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_connected.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Successful Connection</em></p>

<p>As we see in <em>Figure 9</em> everything worked, and I am now connected to SQL Server 2019 for Linux, running in Docker container! To further prove all works I click on the &ldquo;New Query&rdquo; button (highlighted in red), and I execute a trivial <code>SELECT</code> statement: <code>SELECT * FROM sys.databases</code>:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_query_result.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Result from Select</em></p>

<p>In <em>Figure 10</em> we see how we get the result back! We can now continue working with <em>SQL Server 2019 for Linux</em>. If you for some reason want to shut down the container you run <code>docker stop &lt;containername&gt;</code> , and to start it up again - surprise, surprise - <code>docker start &lt;containername&gt;</code>.</p>

<h2 id="summary">Summary</h2>

<p>In this post we covered how we can run <em>SQL Server 2019 for Linux</em> in a Docker container on our Windows machine. We mentioned the Docker commands to use:</p>

<ul>
<li><code>docker pull</code></li>
<li><code>docker run</code></li>
<li><code>docker images</code></li>
<li><code>docker ps</code></li>
<li><code>docker stop</code></li>
<li><code>docker start</code></li>
</ul>

<p>We mentioned how we map a port on the hosting machine to a port on the container, and how we should use different host ports when we have multiple SQL Server instances. The SQL Server in the container is by default using port 1433.</p>

<p>In the post I also spoke about <em>Azure Data Studio</em> and some of its new functionality.</p>

<p>In future blog posts I will talk more about <em>SQL Server 2019 for Linux</em>, especially the In-Database analytics and the Java extensions, as well as <em>Azure Data Studio</em>.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is New in SQL Server 2019 Public Preview]]></title>
    <link href="http://nielsberglund.com/2018/09/24/what-is-new-in-sql-server-2019-public-preview/" rel="alternate" type="text/html"/>
    <updated>2018-09-24T19:17:06+02:00</updated>
    <id>http://nielsberglund.com/2018/09/24/what-is-new-in-sql-server-2019-public-preview/</id>
    <content type="html"><![CDATA[<p>If you read my roundup for <a href="/2018/09/23/interesting-stuff---week-38/">week 38</a>, which I published yesterday, you probably noticed that <a href="https://www.microsoft.com/en-us/ignite"><strong>MS Ignite</strong></a> started today. I mentioned in the post that I was particularly interested in some of the <strong>SQL Server</strong> sessions, as they looked very interesting.</p>

<p>However, even before the sessions started, Microsoft released SQL Server 2019 CTP 2.0 for public preview and, naturally, I jumped on the <a href="https://www.microsoft.com/en-us/evalcenter/evaluate-sql-server-2019-ctp">download link</a> and started downloading. I managed to get to the link in time before the rest of the world started the download, so I managed to get it down and then did an install.</p>

<p>The rest of this post is about my initial findings mostly in the SQL Server Machine Learning Services space.</p>

<blockquote>
<p><strong>NOTE:</strong> I have looked at SQL Server 2019 the grand total of an hour, so this is a short post.</p>
</blockquote>

<p></p>

<h2 id="installation-versions">Installation &amp; Versions</h2>

<p>First of all, the installation took forever, at least it felt that way. I believe it took around an hour, just for the install. So if you install, make sure you are not in a hurry.</p>

<p>I chose to install R and Python services in-database. After the installation finished, (finally), I enabled the machine learning services:</p>

<pre><code class="language-sql">EXEC sp_configure 'external scripts enabled', 1
RECONFIGURE WITH OVERRIDE
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Enable External Scripts</em></p>

<p>After executing the code in <em>Code Snippet 1</em>, I restarted the SQL Server 2019 instance, and then executed my regular &ldquo;check everything works&rdquo; code:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
              @language = N'R'
        , @script = N'd&lt;-42'

EXEC sp_execute_external_script 
              @language = N'Python'
        , @script = N'd=42'

EXEC sp_execute_external_script
                  @language = N'R' ,
                  @script = N'print(R.Version()$version)'

EXEC sp_execute_external_script 
              @language = N'Python'
, @script = N'
import sys
print (sys.version)'
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Test Code</em></p>

<p>As you see, the code is exceptionally advanced (not), but at least the code indicates if there are any issues. The last two <code>sp_execute_external_script</code> statements return the R and Python versions. For R the engine is now running on version <code>3.4.4</code> whereas in SQL Server 2017 it is <code>3.3.3</code>. For Python, it is the same version in both 2017 and 2019: <code>3.5.2</code>.</p>

<h2 id="extensibility-framework">Extensibility Framework</h2>

<p>So, when I read <a href="https://docs.microsoft.com/en-us/sql/sql-server/what-s-new-in-sql-server-ver15?view=sql-server-ver15">What&rsquo;s new in SQL Server 2019</a>, I came across a lot of interesting &ldquo;stuff&rdquo;, but one thing that stood out was <em>Java language programmability extensions</em>. In essence, it allows us to execute Java code in SQL Server by using a pre-built Java language extension! The way it works is as with R and Python; the code executes outside of the SQL Server engine, and you use <code>sp_execute_external_script</code> as the entry-point.</p>

<p>I haven&rsquo;t had time to execute any Java code as of yet, but in the coming days, I definitely will drill into this. Something I noticed is that the architecture for SQL Server Machine Learning Services has changed (or had additions to it). If you remember from my <a href="/sql_server_2k16_r_services">SQL Server Machine Learning Services</a> posts, the flow when executing <code>sp_execute_external_script</code> looked something like so:</p>

<ul>
<li>We execute <code>sp_execute_external_script</code>.</li>
<li>SQL Server connects to the Launchpad service.</li>
<li>Based on the <code>@language</code> parameter, Launchpad calls into either <code>rlauncher.dll</code> or <code>pythonlauncher.dll</code>.</li>
<li>The respective launcher then launches the external engine.</li>
</ul>

<p>If now Java is supported is there also a Java launcher? No, as it turns out, there is not, at least not what I could find. However what I did find was this:</p>

<p><img src="/images/posts/sql_2k19_ml_impr1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Common Launcher</em></p>

<p>In the same directory as the R and Python launchers, I see this new <code>commonlauncher.dll</code> together with a config file. When looking at the config file I did not see anything giving any hints to what goes on, but - as I said above - I will investigate.</p>

<p>At this stage I have two theories about what happens when you execute Java code:</p>

<ol>
<li>The Launchpad service knows about the Java extension: <code>javaextension.dll</code>, which is in the same directory as the launchers, and routes everything with <code>@language = Java</code> to the extension.</li>
<li>For any <code>@language</code> parameter that is not <code>R</code> or <code>Python</code>, the Launchpad service calls the <code>commonlauncher.dll</code>.</li>
</ol>

<p>That&rsquo;s more or less what I found out after an hours &ldquo;playing around&rdquo; with SQL Server 2019 CTP 2.0.</p>

<h2 id="other-interesting-stuff">Other Interesting Stuff</h2>

<p>In the beginning of this post I mentioned about interesting things I found in the <a href="https://docs.microsoft.com/en-us/sql/sql-server/what-s-new-in-sql-server-ver15?view=sql-server-ver15">What&rsquo;s new &hellip;</a> article. In no particular order:</p>

<h3 id="big-data-clusters">Big Data Clusters</h3>

<ul>
<li>Deploy a Big Data cluster with SQL Server and Spark Linux containers on Kubernetes</li>
<li>Access your big data from HDFS</li>
<li>Run Advanced analytics and machine learning with Spark</li>
<li>Use Spark streaming to data to SQL data pools</li>
<li>Run Query books that provide a notebook experience in Azure Data Studio.</li>
</ul>

<h3 id="data-discovery-and-classification">Data discovery and classification</h3>

<ul>
<li>Helps meet data privacy standards and regulatory compliance requirements.</li>
<li>Supports security scenarios, such as monitoring (auditing), and alerting on anomalous access to sensitive data.</li>
<li>Makes it easier to identify where sensitive data resides in the enterprise, so that administrators can take the right steps to secure the database.</li>
</ul>

<h3 id="sql-server-machine-learning-services-failover-clusters-and-partition-based-modeling">SQL Server Machine Learning Services failover clusters and partition based modeling</h3>

<ul>
<li>Partition-based modeling: Process external scripts per partition of your data using the new parameters added to <code>sp_execute_external_script</code>. This functionality supports training many small models (one model per partition of data) instead of one large model.</li>
<li>Windows Server Failover Cluster: Configure high availability for Machine Learning Services on a Windows Server Failover Cluster.</li>
</ul>

<h3 id="azure-data-studio">Azure Data Studio</h3>

<p>Previously released under the preview name SQL Operations Studio, Azure Data Studio is a lightweight, modern, open source, cross-platform desktop tool for the most common tasks in data development and administration. With Azure Data Studio you can connect to SQL Server on premises and in the cloud on Windows, macOS, and Linux.</p>

<h2 id="other-resources">Other Resources</h2>

<p><a href="https://twitter.com/aaronbertrand">Aaron Bertrand</a> has an <a href="https://www.mssqltips.com/sqlservertip/5710/whats-new-in-the-first-public-ctp-of-sql-server-2019/">awesome writeup</a> of what&rsquo;s new in SQL Server 2019 from a more database engine perspective. In that writeup he also points to more resources about SQL Server 2019.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 38]]></title>
    <link href="http://nielsberglund.com/2018/09/23/interesting-stuff---week-38/" rel="alternate" type="text/html"/>
    <updated>2018-09-23T06:47:28+02:00</updated>
    <id>http://nielsberglund.com/2018/09/23/interesting-stuff---week-38/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/changing-face-etl">The Changing Face of ETL</a>. An article by <a href="https://twitter.com/rmoff">Robin Moffat</a> about the &ldquo;new&rdquo; ETL, based on event-driven architectures and streaming platforms.</li>
<li><a href="https://charlla.com/kafka-donuts/">Kafka Donuts</a>. This post is the introduction and TOC to a series of posts about Kafka. The author is my colleague <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>, and in the series, he discusses the use of Kafka in a company who manufactures and sells Donuts. Reading the series introduction post, it is clear that this series is a <strong>MUST</strong> for everyone interested in Kafka. The first episode: <strong>Donut Broker</strong> is <a href="https://charlla.com/kafka-donuts-1/">here</a>, and the second episode <strong>Donut Baker</strong> is <a href="https://charlla.com/kafka-donuts-2/">here</a>.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.paraesthesia.com/archive/2018/09/20/docker-on-wsl-with-virtualbox-and-docker-machine/">Docker on Windows Subsystem for Linux using VirtualBox and Docker Machine</a>. This post by <a href="https://twitter.com/tillig">Travis Illig</a> discusses how you can enable both <strong>VirtualBox</strong> as well as <strong>Docker for Windows</strong> on the same Windows box.</li>
</ul>

<h2 id="microsoft-ignite">Microsoft Ignite</h2>

<p>So, <strong>Microsoft Ignite</strong> starts tomorrow (September 24). It looks to be an awesome conference with lots and lots of announcements of new &ldquo;stuff&rdquo;, I for one cannot wait!</p>

<p>If you, like me, are not attending but still want to follow the key-notes and various sessions, <a href="https://www.microsoft.com/en-us/ignite">this link</a> takes you to the live stream.</p>

<p>The other day I looked at the sessions and here are some that interests me:</p>

<ul>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65955?source=sessions">BRK2416 - The roadmap for SQL Server</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65356?source=sessions">BRK2183 - SQL Server Machine Learning Services: An E2E platform for machine learning</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65957?source=sessions">BRK3228 - What’s new in SQL Server on Linux and containers</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65956?source=sessions">BRK2229 - The future of SQL Server and big data</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66199?source=sessions">THR2168 - The next generation of SQL Server tools</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65967?source=sessions">BRK4021 - Deep dive on SQL Server and big data</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66202?source=sessions">THR2171 - Deploying a highly available SQL Server solution in Kubernetes</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/64634?source=sessions">BRK3154 - SQL Server in containers for application development and DevOps</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66961?source=sessions">THR2308 - SQL Server vNext meets AI and Big Data</a></li>
</ul>

<p>As you see, mostly SQL Server related sessions, and I must say that the sessions around SQL Server and Big Data intrigues me.</p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/how-machine-learning-works-3-resources-to-learn-and-develop-ml-applications">How Machine Learning Works: 3 Resources To Learn And Develop ML Applications</a>. The <a href="https://www.lightbend.com/">Lightbend</a> team has put together some resources about how to design, build, run and manage machine learning applications in production.</li>
<li><a href="https://databricks.com/blog/2018/09/18/simplify-market-basket-analysis-using-fp-growth-on-databricks.html">Simplify Market Basket Analysis using FP-growth on Databricks</a>. In retail, you want to recommend to shoppers what to purchase, and often you base the recommendations on items that are frequently purchased together. A key technique to uncover associations between different items is known as market basket analysis. This blog post talks about how you run your market basket analysis using <strong>Apache Spark MLlib</strong> <code>FP-growth</code> algorithm on <strong>Databricks</strong>.</li>
<li><a href="https://ziedhy.github.io/2018/08/Introduction_Deep_Learning.html">Introduction to Deep Learning</a>. This blog post is the first in a series about <strong>Deep Learning</strong>. At a quick glance, the series looks very informative.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I am still working on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it soon:ish.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 37]]></title>
    <link href="http://nielsberglund.com/2018/09/16/interesting-stuff---week-37/" rel="alternate" type="text/html"/>
    <updated>2018-09-16T08:17:48+02:00</updated>
    <id>http://nielsberglund.com/2018/09/16/interesting-stuff---week-37/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://www.red-gate.com/products/dba/sql-monitor/entrypage/execution-plans">SQL Server Execution Plans, 3rd Edition</a>. The third edition of <a href="https://twitter.com/gfritchey">Grant Fritchey&rsquo;s</a> excellent book about SQL Server Query Plans. If you are a developer or a DBA, you need to get this book (and read it).</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/event-flow-distributed-systems">Complex Event Flows in Distributed Systems</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation how lightweight and highly-scalable state machines ease the handling of complex logic and flows in distributed systems.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/real-time-data-analytics-and-azure-data-lake-storage-gen2/">Real-time data analytics and Azure Data Lake Storage Gen2</a>. Microsoft recently announced <a href="https://azure.microsoft.com/en-us/services/storage/data-lake-storage/">Azure Data Lake Storage Gen 2</a> (ADLS2), and this blog post looks at how ADLS2 can be used for real-time analytics. ADLS2 is at the moment in preview. I certainly hope that MS releases it soon.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/netflix-techblog/keystone-real-time-stream-processing-platform-a3ee651812a">Keystone Real-time Stream Processing Platform</a>. This is a blog post about Keystone; Netflix’s data backbone. It is an essential piece of infrastructure focusing on data analytics. I found this post very interesting, and if you are interested in stream processing, you should really read this post.</li>
<li><a href="https://www.confluent.io/blog/streams-tables-two-sides-same-coin">Streams and Tables: Two Sides of the Same Coin</a>. This blog post announces the availability of the white-paper <a href="https://www.confluent.io/thank-you/streams-and-tables-two-sides-of-the-same-coin/">Streams and Tables: Two Sides of the Same Coin</a>. The paper introduces the Dual Streaming Model, which is used to reason about physical and logical order in data stream processing. This is a <strong>MUST</strong> read!</li>
<li><a href="https://www.confluent.io/blog/building-streaming-application-ksql/">Hands on: Building a Streaming Application with KSQL</a>. In this blog post, we see how we can build a demo streaming application with KSQL, the streaming SQL engine for Apache Kafka. The application continuously computes, in real time, top music charts based on a stream of song play events.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>So the SQL Saturday &ldquo;season&rdquo; is over for me for this year. I did one talk in <a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Johannesburg</a>, two in Cape Town (<a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">this</a> and <a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">this</a>), and one in <a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">Durban</a>.</p>

<p>In addition to the conference talks I also did a full-day workshop in Cape Town and Durban about SQL Server Machine Learning Services: <strong><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">A Day of SQL Server Machine Learning Services with Niels Berglund</a></strong>.</p>

<p>When we talk about SQL Saturdays I want to thank the organisers in the various cities:</p>

<ul>
<li><a href="https://twitter.com/MikeJohnsonZA/">Michael Johnson</a> and team in Johannesburg.</li>
<li><a href="https://twitter.com/Jody_WP">Jody Roberts</a> and <a href="https://twitter.com/TheSQLGirl">Jeanne Combrink</a> and their team in Cape Town.</li>
<li><a href="https://www.linkedin.com/in/jodi-craig-1827b844/">Jodi Craig</a> and team in Durban.</li>
</ul>

<p>They are doing a fantastic work, entirely voluntarily. A HUGE, HUGE <strong>THANK YOU</strong> to all of you!</p>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>Now when SQL Saturday is over, I plan to get back to write about <strong>SQL Server Machine Learning Services</strong>. I am working right now on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it in a week or two.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 36]]></title>
    <link href="http://nielsberglund.com/2018/09/09/interesting-stuff---week-36/" rel="alternate" type="text/html"/>
    <updated>2018-09-09T21:11:32+02:00</updated>
    <id>http://nielsberglund.com/2018/09/09/interesting-stuff---week-36/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/articles/Async-Streams">Async Streams in C# 8</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about, as the title says, C# support for async streams. This, proposed, new functionality in C# is to combine the async/awaiting feature with a yielding operator. Very interesting!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/microservices-post-kubernetes">Microservices in a Post-Kubernetes Era</a>. Another article from <a href="https://www.infoq.com/">InfoQ</a>. This article questions some of the original microservices ideas and acknowledges the fact that they are not standing as strong in the post-Kubernetes era as they were before. Well worth a read!</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/09/05/introduction-to-azure-durable-functions/">Introduction to Azure Durable Functions</a>. This post, from one of the .NET engineering teams is about Azure Durable Functions. Azure Durable functions is a new programming model based on Microsoft serverless’ platform Azure Functions. It allows you to write a workflow as code and have the execution run with the scalability and the reliability of serverless with high throughput.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/category/devops/">DevOps for Data Science </a>. A series of posts by <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a> where he discusses various aspects of DevOps in a Data Science world. This is a must read!</li>
<li><a href="https://www.confluent.io/blog/putting-power-apache-kafka-hands-data-scientists/">Putting the Power of Apache Kafka into the Hands of Data Scientists</a>. A blogpost which combines two of my favorite topics: Kafka and Data Science, how good is that?! The post discusses how they at <a href="https://www.stitchfix.com/">Stitch Fix</a> exposes a multitude of data sources to their Data Scientists, and allow the Data Scientists to create new topics etc., on the fly. This is a <strong>MUST</strong> read post!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/ebook/i-heart-logs-event-data-stream-processing-and-data-integration/">I Heart Logs: Event Data, Stream Processing, and Data Integration</a>. Registration page for downloading <a href="https://twitter.com/jaykreps">Jay Kreps</a> e-book &ldquo;I Heart Logs&rdquo;. If you are interested in streaming in general, you <strong>SHOULD</strong> really get this book!</li>
<li><a href="https://data-artisans.com/blog/serializable-acid-transactions-on-streaming-data">Serializable ACID Transactions on Streaming Data</a>. Guys, (and girls), this is <strong>BIG</strong>. This blogpost introduces <em>data Artisans Streaming Ledger</em>, a new technology that brings serializable ACID transactions to applications built on a streaming architecture!</li>
<li><a href="https://www.confluent.io/blog/data-wrangling-apache-kafka-ksql">Data Wrangling with Apache Kafka and KSQL</a>. A post by <a href="https://twitter.com/rmoff">Robin Moffat</a> about how we can use Kafka and KSQL to manipulate data.</li>
<li><a href="https://www.infoq.com/articles/democratizing-stream-processing-kafka-ksql-part2">Democratizing Stream Processing with Apache Kafka® and KSQL - Part 2</a>. Part 2 in a series of posts by <a href="https://twitter.com/rmoff">Robin Moffat</a> about Kafka and KSQL. In this post Robin covers how Apache Kafka® and KSQL can be used to build powerful data integration and processing applications. You find Part 1 in the series <a href="https://www.infoq.com/articles/democratizing-stream-processing-apache-kafka-ksql">here</a>.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>This weekend I did two talks at SQL Saturday in Cape Town:</p>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>I also did a full day workshop on the Friday (September 7) before the event: <strong><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a></strong>. I had 16 people attending, and I believe it went quite well!</p>

<p>Now there is only one SQL Saturday left, in Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<p>In Durban I also do a full day workshop on the Friday, (September 14), before the event: <strong><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">A Day of SQL Server Machine Learning Services with Niels Berglund</a></strong>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 35]]></title>
    <link href="http://nielsberglund.com/2018/09/02/interesting-stuff---week-35/" rel="alternate" type="text/html"/>
    <updated>2018-09-02T08:38:38+02:00</updated>
    <id>http://nielsberglund.com/2018/09/02/interesting-stuff---week-35/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<p>The content this week is a bit meagre, due to me not having had time to browse around that much, as I have been &ldquo;prepping&rdquo; for the upcoming <strong>SQL Saturdays</strong>.</p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://www.microsoft.com/en-us/research/blog/optimizing-imperative-functions-in-relational-databases-with-froid/">Optimizing imperative functions in relational databases with Froid</a>. A post from Microsoft Research, where they discuss <strong>Froid</strong>. Froid is an extensible framework for optimising imperative programs in relational databases, and the purpose is to enable developers to use the abstraction of UDFs without compromising on performance.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/hadoop-for-beginners">Hadoop for Beginners- Part 1</a>. The first post in a series about, as the title says, Hadoop. This series is really worthwhile reading if you are interested in what Hadoop is and what it can do for you.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/apache-kafka-talk-series/">Apache Kafka: Online Talk Series</a>. This is a registration page for an on-demand video series about Kafka. I bring the popcorn, and you bring the coke!</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>In a couple of previous roundups I have mentioned that the SQL Saturday &ldquo;season&rdquo; is here, and yesterday, (September, 1), I flew out to Johannesburg and did a presentation about SQL Server Machine Learning Services, <a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a> to around 40 people.</p>

<p>The event was a smashing success thanks to the awesome arrangements by <a href="https://twitter.com/MikeJohnsonZA/">Michael Johnson</a> and his fellow volunteers!</p>

<p>Having done Johannesburg, next in turn is Cape Town:</p>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>and finally Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 34]]></title>
    <link href="http://nielsberglund.com/2018/08/26/interesting-stuff---week-34/" rel="alternate" type="text/html"/>
    <updated>2018-08-26T10:22:37+02:00</updated>
    <id>http://nielsberglund.com/2018/08/26/interesting-stuff---week-34/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="http://mattwarren.org/2018/08/21/Monitoring-and-Observability-in-the-.NET-Runtime/">Monitoring and Observability in the .NET Runtime</a>. Yet another awesome blog-post by <a href="https://twitter.com/matthewwarren">Matthew</a>. This post covers how we can monitor .NET through <em>Diagnostics</em>, <em>Profiling</em> and <em>Debugging</em>.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/chaos-engineering-introduction">Chaos Engineering: Building Immunity in Production Systems</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation discussing Chaos Engineering, its purpose, how to go about it, metrics to collect, the purpose of monitoring and logging, etc.</li>
</ul>

<h2 id="databases-storage">Databases / Storage</h2>

<ul>
<li><a href="https://queue.acm.org/detail.cfm?id=3236388">Mind Your State for Your State of Mind</a>. A paper by <a href="https://twitter.com/pathelland">Pat Helland</a>, where Pat explores the evolution of computation from a single process to microservices, the evolution of storage from files to key-value, and how they interact. Just as a side-note, you should read anything by Pat. He certainly knows what he is talking about!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/pakk-your-alpakka-reactive-streams-integrations-for-aws-azure-google-cloud">Pakk Your Alpakka: Reactive Streams Integrations For AWS, Azure, &amp; Google Cloud</a>. The link of this post had Cloud in the title, but I believe it fits better into <em>Streaming</em>. Anyway, this post links to a webinar about <a href="https://akka.io/blog/2016/08/23/intro-alpakka">Alpakka</a>. Alpakka is an integration framework for Akka Streams and the webinar looks at how Alpakka can be used for integrations with other systems.</li>
<li><a href="https://www.youtube.com/watch?v=p9LBi11KR2c">Pat Helland | Kafka Summit 2017 Keynote (Standing on the Distributed Shoulders of Giants)</a>. Speaking about <a href="https://twitter.com/pathelland">Pat</a>. Here is a YouTube video from his Kafka Summit keynote in 2017. It is based on a paper he published in 2016: <a href="https://queue.acm.org/detail.cfm?id=2953944">Standing on Distributed Shoulders of Giants</a>.</li>
</ul>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/logical-index-organization-in-cosmos-db.html">Logical index organization in Cosmos DB</a>. Another Cosmos DB post by <a href="https://twitter.com/muratdemirbas">Murat</a>. In this post, he looks at Cosmos DB&rsquo;s logical indexing subsystem.</li>
<li><a href="https://towardsdatascience.com/from-big-data-to-micro-services-how-to-serve-spark-trained-models-through-aws-lambdas-ebe129f4849c">From Big Data to micro-services: how to serve Spark-trained models through AWS lambdas</a>. This blog-post looks at how you take a Spark trained model, deploy it to AWS and expose it as an AWS Lambda endpoint. Very cool!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/08/22/snorkel-rapid-training-data-creation-with-weak-supervision/">Snorkel: rapid training data creation with weak supervision</a>. In this post <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white paper which tackles one of the central questions in supervised machine learning: how do you get a large enough set of training data to power modern deep models?</li>
<li><a href="http://luisquintanilla.me/2018/08/21/serverless-machine-learning-mlnet-azure-functions/">Serverless Machine Learning with ML.NET and Azure Functions</a>. Earlier in this weeks roundup, I linked to a post about Spark models and AWS Lambdas. This post talks about training a classification model using <a href="https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet">ML.NET</a> and deploy it with <a href="https://azure.microsoft.com/en-us/services/functions/">Azure Functions</a>.</li>
<li><a href="https://blogs.msdn.microsoft.com/data_insights_global_practice/2018/08/22/measuring-model-goodness-part-1/">Measuring Model Goodness – Part 1</a>. This post, which is part one of two-part series, is focused on measuring model goodness, specifically looking at quantifying business value and converting typical machine learning performance metrics (like precision, recall, RMSE, etc.) to business metrics.</li>
<li><a href="http://101.datascience.community/2018/08/24/microsoft-weekly-data-science-news-for-august-24-2018">MICROSOFT WEEKLY DATA SCIENCE NEWS FOR AUGUST 24, 2018</a>. I found the <a href="https://blogs.msdn.microsoft.com/data_insights_global_practice/2018/08/22/measuring-model-goodness-part-1/">Measuring Model Goodness</a> post above thanks to <a href="https://twitter.com/ryanswanstrom">Ryan&rsquo;s</a> blog and this post. Ryan&rsquo;s blog is awesome if you are interested in what Microsoft does in data science and AI!</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>The third post in the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series, which I promised a couple of weeks ago would soon be finished has to wait a bit. Reason for this is my prep for the upcoming SQL Saturdays.</p>

<p>As usual I present in Johannesburg, Cape Town and Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/785/EventHome.aspx">Johannesburg</a>, September 1:

<ul>
<li><a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a>.</li>
</ul></li>
</ul>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 33]]></title>
    <link href="http://nielsberglund.com/2018/08/19/interesting-stuff---week-33/" rel="alternate" type="text/html"/>
    <updated>2018-08-19T09:20:03+02:00</updated>
    <id>http://nielsberglund.com/2018/08/19/interesting-stuff---week-33/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/canopy-scalable-tracing-analytics-facebook">Canopy: Scalable Distributed Tracing &amp; Analysis @ Facebook</a>. This post links to an <a href="https://www.infoq.com/">InfoQ</a> presentation about <strong>Canopy</strong>, which is Facebook&rsquo;s performance and efficiency tracing infrastructure. The presentation covers lessons learned applying Canopy and present case studies of its use in solving various performance and efficiency challenges. Very interesting!</li>
<li><a href="https://azure.microsoft.com/en-us/resources/designing-distributed-systems/">Designing Distributed Systems</a>. A download link to an e-book: <strong>Designing Distributed Systems</strong>. The e-book provides repeatable, generic patterns, and reusable components to make developing reliable systems easier and more efficient. It is written by <a href="https://twitter.com/brendandburns">Brendan Burns</a> who is a Distinguished Engineer at Microsoft and works on Azure.</li>
</ul>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/schema-agnostic-indexing-with-azure.html">Schema-Agnostic Indexing with Azure Cosmos DB</a>. In this blog post, <a href="https://twitter.com/muratdemirbas">Murat</a> dissects a white paper about the schema-agnostic indexing subsystem of Cosmos DB. The post (and the paper) is very interesting, go ahead and read it, please!</li>
<li><a href="https://azure.microsoft.com/en-us/blog/azure-hdinsight-interactive-query-simplifying-big-data-analytics-architecture-and-operations/">Azure #HDInsight Interactive Query: simplifying big data analytics architecture</a>. This post discusses a new feature of Hive 2, Low Latency Analytics Processing (LLAP). LLAP produces significantly faster queries on raw data stored in commodity storage systems such as Azure Blob store or Azure Data Lake Store. This is quite exciting, and I need to check it out!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/build-udf-udaf-ksql-5-0">How to Build a UDF and/or UDAF in KSQL 5.0</a>. Not one week without at least one Kafka related post - that is the &ldquo;law&rdquo;. This post discusses a new feature in <strong>KSQL</strong> 5, the ability for the users o write their own functions for KSQL to use. Think about the possibilities that open up!</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/neural-networks-from-a-bayesian-perspective">Neural Networks from a Bayesian Perspective</a>. This post covers different ways to obtain uncertainty in Deep Neural Networks from a Bayesian perspective. The post is quite theoretical but very interesting!</li>
<li><a href="https://databricks.com/blog/2018/08/15/100x-faster-bridge-between-spark-and-r-with-user-defined-functions-on-databricks.html">100x Faster Bridge between Apache Spark and R with User-Defined Functions on Databricks</a>. Spark exposes an API, SparkR User Defined Function API, which acts as a bridge between Spark and R. Unfortunately the bridge is far from efficient. Databricks has made the bridge more efficient when you run Spark on Databricks, and this post talks about how it is done.</li>
<li><a href="https://towardsdatascience.com/the-most-important-part-of-a-data-science-project-is-writing-a-blog-post-50715f37833a">The most important part of a data science project is writing a blog post</a>. A somewhat provocative title of this blog post, but it makes a good point. Always document your data science projects so other data scientists can see what you have achieved!</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>It is that time of the year again: SQL Saturday season! As usual I present in Johannesburg, Cape Town and Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/785/EventHome.aspx">Johannesburg</a>, September 1:

<ul>
<li><a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a>.</li>
</ul></li>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Test Post and Feed Apologies]]></title>
    <link href="http://nielsberglund.com/2018/08/18/test-post-and-feed-apologies/" rel="alternate" type="text/html"/>
    <updated>2018-08-18T09:38:50+02:00</updated>
    <id>http://nielsberglund.com/2018/08/18/test-post-and-feed-apologies/</id>
    <content type="html"><![CDATA[<p>In my <a href="/2018/08/18/goodbye-jekyll-welcome-hugo/">Goodbye Jekyll, Welcome Hugo!</a> post I wrote how I have changed blog engine from Jekyll to Hugo, and how everything seemed to work. Saying that obviously jinxed it, as after I deployed to the hosting site my Atom feed was completely hosed.</p>

<p>I have now spent 4 hours trying to fix everything, but I am not sure I have succeeded. So this is a test post to see if things look OK.</p>

<p></p>

<p>I also want to apologise if I have messed up your feeds, as at one stage I saw in my feeds a lot of duplicates posts from me. Sorry!</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Goodbye Jekyll, Welcome Hugo!]]></title>
    <link href="http://nielsberglund.com/2018/08/18/goodbye-jekyll-welcome-hugo/" rel="alternate" type="text/html"/>
    <updated>2018-08-18T05:03:30+02:00</updated>
    <id>http://nielsberglund.com/2018/08/18/goodbye-jekyll-welcome-hugo/</id>
    <content type="html"><![CDATA[<p>Back in 2013 I wrote a <a href="/2013/10/02/moving-to-a-new-blog-engine/">post</a> about how I moved from a self hosted <strong>WordPress</strong> blog to a static blog; <a href="http://octopress.org"><strong>OctoPress</strong></a>. OctoPress spoke to my &ldquo;geeky&rdquo; side, as the foundation of it is the static blog-generator <a href="https://jekyllrb.com/"><strong>Jekyll</strong></a>, which in turn is Ruby based. In fact, OctoPress is more or less just some extra Ruby plugins on top of Jekyll, and to generate sites with OctoPress / Ruby you need Ruby installed on you machine.</p>

<p>I thought that by changing the blog platform, I might write more posts just due to the &ldquo;geekiness&rdquo; of the blog engine. Fast forward to the end of 2016, and the number of posts I had written since the switch came to a grand total of three. Those three includes the post where I announced the switch. Yeah, switching increased indeed my productivity - NOT!</p>

<p>So what does this have to do with anything?</p>

<p></p>

<p>Before I get into the reason for this post, let us look at what a static site generator is.</p>

<h2 id="static-site">Static Site</h2>

<p>A static site generator is a framework that takes source files and generates an entirely static website. We deploy the files that make up the site to the web server, and when a user requests a page, the web server returns that page to the user. This opposed to something like WordPress, where WordPress builds the page from a number of templates, gets the content and other site data from the database and sends the complete HTML page back to the user.</p>

<p>The advantages of a static site are that it is usually less complicated than a dynamic site; no templates, no database and so forth. Quite often serving a page to the user is better performing as the page is not dynamically created.</p>

<p>The downside of a static site is that you build it for each time you do a change to a page, adding a post, and so on, and depending on the size of your site (number of pages etc.), the build can take a while. So that is where my problem lies and the reason for this post.</p>

<h2 id="problem">Problem</h2>

<p>I mentioned above how I didn&rsquo;t manage to write any blog posts up to the end of 2016. Since then, however, I have been reasonably productive in my writing - and managed at least one post per week. It helps when you have cool stuff like <strong>SQL Server Machine Learning Services</strong> to write about. While it is cool that I produce posts, what I noticed was that the build time of the site took longer and longer. I did not think much about it until a couple of weeks ago when I had just finished the <a href="/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/">sp_execute_external_script and SQL Compute Context - III</a> post and tried to generate the site. It did not work; it just hung, what to do?</p>

<p>I ended up removing all OctoPress plugins and edited all files that referenced the plugins, to be able to run a bare-bones Jekyll generated site. I eventually managed to get it to work again, but this made me look around for other site generators.</p>

<h2 id="hugo">Hugo</h2>

<p>In my looking around for static site generators I came across <a href="https://gohugo.io/">Hugo</a>. Like Jekyll it is an open source static site generator, but it is built on <a href="https://golang.org/"><strong>Go</strong></a> instead of Ruby. One of the differences between Hugo and Jekyll is that when you generate a site with Jekyll you execute Ruby commands, and, as I mentioned above, you need Ruby installed. Hugo, on the other hand, comes as an executable <code>Hugo.exe</code> and you do not need Go installed at all. That Hugo is a self contained executable is a big plus in my book, since I have had versioning issues with Ruby a couple of times.</p>

<p>The most significant difference between Jekyll and Hugo though is speed when generating a site, or at least that is what the Hugo website says: &ldquo;<em>The world’s fastest framework for building websites</em>&rdquo;.</p>

<p>With the above points in mind, I decided to try and convert this blog from Jekyll to Hugo.</p>

<h2 id="blog-conversion">Blog Conversion</h2>

<p>So, I spent a couple of hours a day for around a week converting my posts and pages to Hugo, and it was not that difficult. The biggest issue was how Jekyll refers to posts: <code>{% post_url 2018-08-04-sp-execute-external-script-and-sql-compute-context---iii %}</code> versus <code>/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/</code>, and I had a lot of those. I eventually wrote a small <code>C#</code> console application that trawled through the posts and did the conversion. Apart from that everything is pretty straightforward. Obviously there are differences, but nothing earthshattering, as far as I can tell.</p>

<p>What about the speed then? Well, to build the Jekyll site took around 20 seconds, to build the site using Hugo takes around 2 seconds! Based on this, I decided to switch to Hugo, and this is the first post that I publish using Hugo as blog site generator. I hope I have not missed anything and that all is Ok. If not, well then, hopefully, you, my readers, <a href="mailto:niels.it.berglund@gmail.com">tell me</a> if you notice something amiss.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 32]]></title>
    <link href="http://nielsberglund.com/2018/08/12/interesting-stuff---week-32/" rel="alternate" type="text/html"/>
    <updated>2018-08-12T12:02:02+02:00</updated>
    <id>http://nielsberglund.com/2018/08/12/interesting-stuff---week-32/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/the-many-faces-of-consistency.html">The many faces of consistency</a>. A blog post by <a href="https://twitter.com/muratdemirbas">Murat</a> where he dissects a white paper about consistency. The paper talks about two types of consistency: <em>state</em> and <em>operation</em>. Seeing that Murat now does sabbatical work at Microsoft (see below), he compares the two consistency types with what Cosmos DB provides. The post is a must read if you are the least interested in distributed computing and consistency.</li>
</ul>

<h2 id="cloud-big-data">Cloud / Big Data</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/globally-replicated-data-lakes-with-livedata-using-wandisco-on-azure/">Globally replicated data lakes with LiveData using WANdisco on Azure</a>. The post discusses how you can achieve globally replicated Azure Data Lakes. The replication can be both hybrid: on-prem to Azure as well as Azure to Azure.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/08/09/cloud-data-and-ai-services-training-roundup-august-2018/">Cloud data and AI services training roundup August 2018</a>. This post lists some free data and AI training sessions.</li>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/azure-cosmos-db.html">Azure Cosmos DB</a>. This post by <a href="https://twitter.com/muratdemirbas">Murat</a> is about his first impressions of Azure Cosmos DB. Murat has taken a sabbatical and spends a year at Microsoft in the Cosmos DB team. I look forward to more posts by Murat about Cosmos DB, and other Azure related topics.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://charlla.com/howto-docker-on-windows/">HowTo - Docker on Windows</a>. My mate and colleague, <a href="https://twitter.com/charllamprecht">Charl</a> continues his blogging journey. This post is how to run Docker on Windows.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/streaming-data-dominates-over-2000-developers-say-only-batch-is-almost-extinct">Streaming Data Dominates: Over 2000 Developers Say “Only Batch” Is Almost Extinct</a>. A survey by <a href="https://www.lightbend.com/">Lightbend</a>, (formerly known as <a href="https://en.wikipedia.org/wiki/Lightbend">Typesafe</a>), makes it clear that developers now moves more and more towards real-time processing as opposed to batch. That, my friends, is music to my ears!</li>
<li><a href="https://data-artisans.com/blog/apache-flink-1-6-0-whats-new-in-the-latest-apache-flink-release">Apache Flink 1.6.0: What’s new in the latest Apache Flink release</a>. What the title says; this is a post detailing some of the new features in the Flink 1.6.0 release.</li>
<li><a href="https://www.confluent.io/blog/getting-started-apache-kafka-kubernetes/">Getting Started with Apache Kafka and Kubernetes</a>. A blog-post about the work done to enable Kafka to run on Kubernetes. The post points to a white paper: <a href="https://www.confluent.io/resources/recommendations-for-deploying-apache-kafka-on-kubernetes">Run Confluent Platform on Kubernetes Using Best Practices</a> which is really good!</li>
<li><a href="https://www.confluent.io/blog/kafka-streams-action">Kafka Streams in Action</a>. A post about the upcoming book: <a href="https://www.manning.com/books/kafka-streams-in-action">Kafka Streams in Action</a>. Apart from announcing the book, the post also contains the foreword to the book. This book is a must if you are interested in Kafka Streams!</li>
<li><a href="https://databricks.com/blog/2018/08/09/building-a-real-time-attribution-pipeline-with-databricks-delta.html">Building a Real-Time Attribution Pipeline with Databricks Delta</a>. this blog post looks at how to use the Databricks DataFrame API to build Structured Streaming applications and use Databricks Delta to query the streams in near-real-time.</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://dzone.com/articles/model-serving-stream-processing-vs-rpc-rest-with-j">Model Serving: Stream Processing vs. RPC/REST With Java, gRPC, Apache Kafka, TensorFlow</a>. A short and sweet blog-post comparing stream processing applications with a model serving infrastructure, like <strong>TensorFlow Serving</strong>, for serving machine learning models.</li>
<li><a href="http://blog.revolutionanalytics.com/2018/08/ieee-language-rankings-2018.html">IEEE Language Rankings 2018</a>. A post by <a href="https://twitter.com/revodavid">David</a> about the latest IEEE Spectrum language rankings.</li>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/scalable-iot-ml-platform-with-apache-kafka-deep-learning-mqtt">Scalable IoT ML Platform with Apache Kafka + Deep Learning + MQTT</a>. This post describes a hybrid machine learning infrastructure leveraging Apache Kafka as a scalable central nervous system. Very interesting!</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I have started on the third post in the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it in a week or so.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 31]]></title>
    <link href="http://nielsberglund.com/2018/08/05/interesting-stuff---week-31/" rel="alternate" type="text/html"/>
    <updated>2018-08-05T08:01:44+02:00</updated>
    <id>http://nielsberglund.com/2018/08/05/interesting-stuff---week-31/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/08/02/tiered-compilation-preview-in-net-core-2-1/">Tiered Compilation Preview in .NET Core 2.1</a>. A blog-post about a new feature in .NET Core 2.1: <strong>Tiered Compilation</strong>. Tiered Compilation allows .NET to have multiple compilations for the same method that can be hot-swapped at runtime. This should improve compile times drastically!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://charlla.com/whatsthat-kafka/">Apache Kafka - Whats That</a>. This post about Kafka is by a colleague and a good friend of mine, <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>. In the post, he takes us through a very succinct overview of Kafka. Charl is &ldquo;Mr Kafka&rdquo; at <a href="/Derivco">Derivco</a>, and he knows his &ldquo;stuff&rdquo;. Please be sure to follow his <a href="https://charlla.com/">blog</a> for more about Kafka (Charl, no pressure, hey?!).</li>
<li><a href="https://www.confluent.io/blog/decoupling-systems-with-apache-kafka-schema-registry-and-avro/">Decoupling Systems with Apache Kafka, Schema Registry and Avro</a>. An excellent post on how to decouple the systems you integrate via Kafka by using the <a href="https://www.confluent.io/confluent-schema-registry/">Confluent Schema Registry</a>. An added bonus in this post is that the code is .NET code!</li>
<li><a href="https://data-artisans.com/blog/a-practical-guide-to-broadcast-state-in-apache-flink">A Practical Guide to Broadcast State in Apache Flink</a>. This article discusses <strong>Broadcast State</strong>, a new feature in Apache Flink 1.5. With Broadcast State you can evaluate dynamic patterns on event streams by combining and jointly process two streams of events in a specific way.</li>
<li><a href="https://www.confluent.io/blog/introducing-confluent-platform-5-0/">Introducing Confluent Platform 5.0</a>. As the title says, this post introduces the latest version of Confluent Platform: 5.0. Lots and lots of new interesting features. Go and have a look!</li>
<li><a href="https://www.confluent.io/landing-page/microservices-online-talk-series/">Apache Kafka for Microservices: A Confluent Online Talk Series</a>. This post is a link to a three-part online talk series which introduces fundamental concepts, use cases and best practices for getting started with microservices and Kafka.</li>
</ul>

<h2 id="big-data-cloud">Big Data / Cloud</h2>

<ul>
<li><a href="https://databricks.com/blog/2018/07/31/processing-petabytes-of-data-in-seconds-with-databricks-delta.html">Processing Petabytes of Data in Seconds with Databricks Delta</a>. In my roundups lately, I have covered Databricks Delta quite a bit and discussed how efficient it is processing lots and lots of data. This blog post takes a look under the hood and examines what makes Databricks Delta capable of sifting through petabytes of data within seconds. If you, like me, are interested in knowing how &ldquo;stuff&rdquo; works under the covers, then this post is a must-read!</li>
<li><a href="https://eng.uber.com/databook/">Databook: Turning Big Data into Knowledge with Metadata at Uber</a>. This post is about <strong>Databook</strong>, Uber’s in-house platform that surfaces and manages metadata about the internal locations and owners of specific datasets, and allows Uber to turn data into knowledge.</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="http://blog.revolutionanalytics.com/2018/08/r-python-in-sql-server.html">Video: How to run R and Python in SQL Server from a Jupyter notebook</a>. A short post by <a href="https://twitter.com/revodavid">David</a> linking to a video showing how to run Python and R from inside SQL Server.</li>
<li><a href="https://www.infoq.com/minibooks/emag-real-world-machine-learning">The InfoQ eMag: Real-World Machine Learning: Case Studies, Techniques and Risks</a>. An <a href="https://www.infoq.com/">InfoQ</a> link to an eMag focusing on the current landscape of machine-learning technologies and real-world case studies of applied machine learning.</li>
<li><a href="https://blogs.technet.microsoft.com/machinelearning/2018/07/31/3-steps-to-build-your-first-intelligent-app-conference-buddy/">3 Steps to Build Your First Intelligent App – Conference Buddy</a>. A blog-post which takes us through how to build an application utilising AI.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<ul>
<li><a href="/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/">sp_execute_external_script and SQL Compute Context - III</a>. I finally managed to finish and publish the third post in the <a href="/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series. In this post we use WinDbg, Process Monitor and WireShark to look in detail what happens in SQL Server when we use RxSqlServerData to pull data.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[sp_execute_external_script and SQL Compute Context - III]]></title>
    <link href="http://nielsberglund.com/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/" rel="alternate" type="text/html"/>
    <updated>2018-08-04T16:05:46+02:00</updated>
    <id>http://nielsberglund.com/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/</id>
    <content type="html"><![CDATA[<p>In the <a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services - sp_execute_external_script - III</a> post I wrote about <code>sp_execute_external_script</code> (SPEES) and the <strong>SQL Server Compute Context</strong> (SQLCC). Afterwards I realised I had some things wrong, so I wrote a followup post: <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">sp_execute_external_script and SQL Compute Context - I</a> where I tried to correct my mistakes from the <a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">initial post</a>. That post led to <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a> and now we have a mini-series.</p>

<p>To see other posts (including this) in the series, go to <a href="/spees_and_sql_compute_context"><strong>sp_execute_external_script and SQL Server Compute Context</strong></a>.</p>

<p>In the previous post in this series, we looked at how data is sent to the SqlSatellite from SQL Server when we are in the SQLCC. This post was meant to look at what goes on inside SQL Server when we execute in SQLCC, but I realised that it would make more sense if, before we look at the internal working when in SQLCC, I covered what happens when pulling data in the local context. So that is what this post is all about.</p>

<p></p>

<p>Before we dive into todays topic let us recap.</p>

<h2 id="recap">Recap</h2>

<p>In <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> we discussed what the SQLCC is and we said that as part of RevoScaleR, you can define where a workload executes. By default, it executes on your local machine, but you can also set it to execute in the context of somewhere else: Hadoop, Spark and also SQL Server. So, in essence, you can run some code on your development machine and have it execute in the environments mentioned above. To use the SQLCC in SQL Server we use <code>RxInSqlServer</code> and <code>rxSetComputeContext</code>:</p>

<pre><code class="language-r"># set up the connection string
sqlServerConnString &lt;- &quot;Driver=SQL Server;
                        server=.; # localhost
                        database=testParallel;
                        uid=some_uid;pwd=some_pwd&quot;

# set up the context
sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                        numTasks = 1)
# set the compute context to be the sql context
rxSetComputeContext(sqlCtx)    
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Set up SQL Server Compute Context</em></p>

<p>The connection string we see in <em>Code Snippet 1</em> indicates the where we execute, not necessarily where the data we work with lives. The <code>numTasks</code> argument defines the maximum number of tasks SQL Server can use. Something interesting when setting <code>numTasks</code> to be greater than 1 is that when we run the code, we run it hosted in a <code>mpiexec.exe</code> process. If we run in SQLCC under <code>numTasks = 1</code> we do not see <code>mpiexec.exe</code>, but we see another <code>bxlserver.exe</code> process. We also said that when we run in SQLCC. <code>sp_execute_external_script</code> executes multiple times.</p>

<p>The most interesting thing that came out of <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> was the performance benefit of using SQLCC when loading large datasets. So in <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a>, we tried to see where that performance benefit came from.</p>

<p>In <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> we said we had three ways of getting data into an R script:</p>

<ul>
<li>Push using <code>@input_data_1</code>.</li>
<li>Pull using <code>RxSqlServerData</code> in the local context.</li>
<li>Pull using <code>RxSqlServerData</code> and the SQLCC.</li>
</ul>

<p>We saw that to use SQLCC we need to pull the data; we could not push it. We also saw that for large datasets there was a significant performance difference between pulling in the local context and pulling using SQLCC. The interesting point was that pushing data and pulling using SQLCC had the same performance characteristics.</p>

<p>When pulling the data (<code>RxSqlServerData</code>), we use ODBC, and the protocol is TDS. However, when we use the SQLCC, the BXL protocol is also used and that gives us very efficient processing of data which is the reason we see good performance.</p>

<h2 id="housekeeping">Housekeeping</h2>

<p>Before we go any further let us look at the code and the tools we use today. This section is here for those who want to follow along in what we are doing in the post.</p>

<h4 id="helper-tools">Helper Tools</h4>

<p>To help us figure out the things we want, we use:</p>

<ul>
<li><em>Process Monitor</em> - to filter out TCP traffic.</li>
<li><em>WinDbg</em> - to see what happens inside SQL Server. If you need help with setting it up, we covered that in <a href="/2017/03/18/microsoft-sql-server-r-services---internals-i/">Microsoft SQL Server R Services - Internals I</a>.</li>
<li><em>WireShark</em> - to &ldquo;sniff&rdquo; network packets. We covered setting up <em>WireShark</em> in <a href="/2017/08/29/microsoft-sql-server-r-services---internals-x/">Internals - X</a>. Please remember that if you run SSMS and SQL Server on the same machine, then you need the <a href="https://nmap.org/npcap/"><strong>Npcap</strong></a> packet sniffer library instead of the default <strong>WinPcap</strong>.</li>
</ul>

<h4 id="code">Code</h4>

<p>This is the database objects we use in this post:</p>

<pre><code class="language-sql">USE master;
GO

SET NOCOUNT ON;
GO

DROP DATABASE IF EXISTS TestParallel;
GO

CREATE DATABASE TestParallel;
GO

USE TestParallel;
GO

DROP TABLE IF EXISTS dbo.tb_Rand_50M
GO
CREATE TABLE dbo.tb_Rand_50M
(
  RowID bigint identity PRIMARY KEY, 
  y int NOT NULL, rand1 int NOT NULL, 
  rand2 int NOT NULL, rand3 int NOT NULL, 
  rand4 int NOT NULL, rand5 int NOT NULL,
);
GO

INSERT INTO dbo.tb_Rand_50M(y, rand1, rand2, rand3, rand4, rand5)
SELECT TOP(50000000) CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 25 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 50 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 100 AS INT)
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO

</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Setup of Database, Table and Data</em></p>

<p>We use more or less the same database and database object as in the <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> post:</p>

<ul>
<li>A database: <code>TestParallel</code>.</li>
<li>A table: <code>dbo.tb_Rand_50M</code>. This table contains the data we want to analyse.</li>
</ul>

<p>In addition to creating the database and the table <em>Code Snippet 2</em> also loads 50 million records into the <code>dbo.tb_Rand_50M</code>. Be aware that when you run the code in <em>Code Snippet 2</em> it may take some time to finish due to the loading of the data. Yes, I know - the data is entirely useless, but it is a lot of it, and it helps to illustrate what we want to do.</p>

<p>Not only is the database and database objects similar to what we used in <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a>, the code we use is also almost the same:</p>

<pre><code class="language-sql">DECLARE @isCtx bit = 0;
DECLARE @numTasks int = 1;
EXEC sp_execute_external_script
      @language = N'R'
    , @script = N'
      # set up the connection string
      sqlServerConnString &lt;- &quot;Driver=SQL Server;server=.;
                              database=testParallel;
                              uid=&lt;username&gt;;pwd=&lt;userpwd&gt;&quot;
      
      if(useContext == 1) {
        sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                                numTasks = tasks)
        # set the compute context to be the sql context
        rxSetComputeContext(sqlCtx)
      }

      mydata &lt;- RxSqlServerData(sqlQuery = &quot;SELECT y, rand1, rand2, 
                                            rand3, rand4, rand5 
                                            FROM dbo.tb_Rand_50M&quot;,
                                connectionString = sqlServerConnString);
                        
      myModel &lt;- rxLinMod(y ~ rand1 + rand2 + rand3 + rand4 + rand5, 
                      data=mydata)'
    , @params = N'@tasks int, @useContext bit'
    , @tasks = @numTasks
    , @useContext = @isCtx
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Test Code</em></p>

<p>As we see in <em>Code Snippet 3</em> we parameterise the <code>sp_execute_external_script</code> call, and we have parameters for whether to use the SQLCC and also how many tasks to run when executing in the context. The default is to execute in the local context, and when we execute in SQLCC the default for the number of tasks is 1 (<code>numTasks = 1</code>).</p>

<h2 id="local-context-pull-vs-push">Local Context Pull vs Push</h2>

<p>Let us try and see what happens in SQL Server when we execute in the local context, and we pull data (<code>RxSqlServerData</code>) compared to when we push data (<code>@input_data_1</code>).</p>

<p>First, let us try and understand when and how the <code>SELECT</code> statement in <code>Code Snippet 3</code> executes. To check this we use the same technique as we did in <a href="/2017/11/11/microsoft-sql-server-r-services---internals-xiii/">Microsoft SQL Server R Services - Internals XIII</a>, we execute a <code>SELECT</code> statement which causes a division by zero exception. We capture that exception in <em>WinDbg</em> and we check the call stack.</p>

<p>So, run <em>WinDbg</em> as admin and enable an event-filter, so the debugger breaks at a <code>C++ EH</code> exception. In the <em>Event Filters</em> dialog you can set how that particular exception should be handled. We want it to be enabled, but not handled:</p>

<p><img src="/images/posts/sql_r_services_windbg_eventfilters_enable.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Enable C++ EH Exception</em></p>

<p>When the <em>Event Filter</em> is enabled as in <em>Figure 1</em> we:</p>

<ul>
<li>Attach <em>WinDbg</em> to the <code>sqlservr.exe</code> process (please do not do this on a production machine - <em>#justsaying</em>).</li>
<li>Set a breakpoint at <code>bp sqllang!SpExecuteExternalScript</code>.</li>
<li>Change the <code>SELECT</code> statement in <em>Code Snippet 3</em> slightly, so it generates the division by zero exception mentioned above: <code>SELECT TOP(1) (1 / y - y), y, rand1, ...</code>.</li>
</ul>

<p>After changing the <code>SELECT</code> statement, we execute the code. Continue through the breakpoint at <code>SpExecuteExternalScript</code>, and when the execution breaks at the exception we check the call stack: <code>k</code>:</p>

<pre><code class="language-cpp">KERNELBASE!RaiseException+0x68
...
sqllang!CXStmtQuery::ErsqExecuteQuery+0x5a2
sqllang!CXStmtSelect::XretExecute+0x2f2
sqllang!CMsqlExecContext::ExecuteStmts&lt;1,1&gt;+0x4c5
sqllang!CMsqlExecContext::FExecute+0xaae
sqllang!CSQLSource::Execute+0xa2c
sqllang!process_request+0xe52
sqllang!process_commands_internal+0x289
sqllang!process_messages+0x213
sqldk!SOS_Task::Param::Execute+0x231
...
ntdll!RtlUserThreadStart+0x21
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Call Stack Local Context Pull</em></p>

<p>The call stack we see in <em>Code Snippet 4</em> is somewhat edited in that it shows only the important parts. Let us compare it with the call-stack we see when we push the data, (<code>@input_data_1</code>), and receive a division by zero exception as we did in <a href="/2017/11/11/microsoft-sql-server-r-services---internals-xiii/">Microsoft SQL Server R Services - Internals XIII</a>:</p>

<pre><code class="language-cpp">KERNELBASE!RaiseException+0x68
...
sqllang!CXStmtQuery::ErsqExecuteQuery+0x49f
sqllang!CXStmtSelect::XretExecute+0x2f2
sqllang!CMsqlExecContext::ExecuteStmts&lt;1,1&gt;+0x4c5
sqllang!CMsqlExecContext::FExecute+0xaae
sqllang!CSQLSource::Execute+0xa2c
sqllang!SpExecuteExternalScript+0x154b
sqllang!CSpecProc::ExecuteSpecial+0x31e
sqllang!CXProc::Execute+0x139
sqllang!CSQLSource::Execute+0xb5b
sqllang!CStmtExecProc::XretLocalExec+0x2d3
sqllang!CStmtExecProc::XretExecExecute+0x4a1
sqllang!CXStmtExecProc::XretExecute+0x38
sqllang!CMsqlExecContext::ExecuteStmts&lt;1,1&gt;+0x4c5
sqllang!CMsqlExecContext::FExecute+0xaae
sqllang!CSQLSource::Execute+0xa2c
sqllang!process_request+0xe52
sqllang!process_commands_internal+0x289
sqllang!process_messages+0x213
sqldk!SOS_Task::Param::Execute+0x231
...
ntdll!RtlUserThreadStart+0x21
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Call Stack Local Context Push</em></p>

<p>The call-stack in <em>Code Snippet 5</em> is also edited and when we compare the two call-stacks, they look somewhat similar.  We see <code>sqllang!CXStmtQuery::ErsqExecuteQuery</code> in both code snippets, and we know from <a href="/2017/11/11/microsoft-sql-server-r-services---internals-xiii/">Internals - XIII</a> that <code>ErsqExecuteQuery</code> handles execution of SQL statements and also sending data to the SQL Satellite. So that makes sense that we see it in <em>Code Snippet 4</em>. However, what does not make sense is that we do not see <code>SpExecuteExternalScript</code> in the <em>Code Snippet 4</em> call stack (and no, I did not edit it out). What goes on here?</p>

<p>Ok, let us refer back to what we saw in <a href="/2017/10/31/microsoft-sql-server-r-services---internals-xii/">Internals - XII</a> and  <a href="/2017/11/11/microsoft-sql-server-r-services---internals-xiii/">Internals - XIII</a>, how SQL Server sends various data packages to the SqlSatellite and then finally calls <code>sqllang!CSatelliteCargoContext::SendChunkEndMessage</code>. So, add a breakpoint, in addition to the <code>SpExecuteExternalScript</code> breakpoint, at <code>sqllang!CSatelliteCargoContext::SendChunkEndMessage</code> and execute the code again. Oh, we see how we hit the <code>SpExecuteExternalScript</code> breakpoint first, followed by <code>SendChunkEndMessage</code>, and finally, we break at the division by zero exception. That is interesting, SQL Server sends the chunk end message and only after that, the query executes.</p>

<p>So what with <code>ErsqExecuteQuery</code>, what does that do? Let us see if we can find out:</p>

<ul>
<li>Change the code not to cause a divide by zero exception: <code>SELECT TOP(1) y, rand1, ...</code>.</li>
<li>Set a breakpoint at <code>ErsqExecuteQuery</code>.</li>
</ul>

<p>When you execute the code you see something like so:</p>

<pre><code class="language-cpp">0:099&gt; g
Breakpoint 0 hit
sqllang!SpExecuteExternalScript:
0:091&gt; g
Breakpoint 2 hit
sqllang!CXStmtQuery::ErsqExecuteQuery:
0:091&gt; g
Breakpoint 1 hit
sqllang!CSatelliteCargoContext::SendChunkEndMessage:
0:091&gt; g
Breakpoint 2 hit
sqllang!CXStmtQuery::ErsqExecuteQuery:
0:078&gt; g
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Breakpoints Hit</em></p>

<p>In <em>Code Snippet 6</em> we see how we first hit the <code>SpExecuteExternalScript</code> breakpoint, followed by <code>ErsqExecuteQuery</code>, (which is expected), then we hit the <code>SendChunkEndMessage</code> followed by a second <code>ErsqExecuteQuery</code>. That is interesting; two <code>ErsqExecuteQuery</code>, and we know from above that the query executes after the second <code>ErsqExecuteQuery</code>. So the question is now how the data flows from SQL Server to the SqlSatellite. We know from [Internals - XIII<a href="/2017/11/11/microsoft-sql-server-r-services---internals-xiii/">si13</a>, as we mentioned above, that when we push data, SQL Server makes these calls (among many others):</p>

<ul>
<li><code>sqlmin!CQScanUdx::PushNextChildRow</code>.</li>
<li><code>sqllang!CUDXR_ExternalScript::PushRow</code>.</li>
<li><code>sqllang!CSatelliteCargoContext::SendPackets</code>.</li>
</ul>

<p>Let us set a breakpoint at <code>sqllang!CSatelliteCargoContext::SendPackets</code> and see what happens when we execute.</p>

<p>THAT was interesting! We never hit <code>SendPackets</code>. In the recap above we said that when we pull the data we use ODBC and the TDS protocol, and what we see here further ensures that is the case. So what does SQL Server call when it uses TDS to send data? To find out:</p>

<ul>
<li>Disable the breakpoints at <code>SendChunkEndMessage</code> and <code>SendPackets</code>.</li>
<li>Execute the query.</li>
</ul>

<p>When you reach <code>ErsqExecuteQuery</code> the second time, do a <code>wt  -l4</code> (watch and trace 4 levels deep) and continue. Look at the output from the <code>wt</code>:</p>

<pre><code class="language-cpp">sqllang!CXStmtQuery::ErsqExecuteQuery
  ...   
    sqllang!CXStmtQuery::SetupQueryScanAndExpression
      sqlmin!CQuery::CreateExecPlan
        sqldk!CMemProc::CpagesInUse
      ...
      sqlmin!CQueryScan::StartupQuery
  ...      
  sqlmin!CQueryScan::GetRow
    sqlmin!CQScanTopNew::GetRow
      sqlmin!CQScanTableScanNew::GetRow
        sqlmin!CQScanRowsetNew::GetRowWithPrefetch
      sqlmin!CQScanTableScanNew::GetRow
        sqllang!CValOdsRow::SetDataX
        sqllang!CShilohTds::SendRowImpl
      sqlmin!CQScanTableScanNew::GetRow
    sqlmin!CQScanTopNew::GetRow
  sqlmin!CQueryScan::GetRow
...
sqllang!CXStmtQuery::ErsqExecuteQuery
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Output from wt</em></p>

<p>The output shows a lot of data, so in <em>Code Snippet 7</em> I have edited out all but the interesting parts. We see some routines that have to do with setting up and starting the query and retrieving result data:</p>

<ul>
<li><code>sqllang!CXStmtQuery::SetupQueryScanAndExpression</code>.</li>
<li><code>sqlmin!CQueryScan::StartupQuery</code>.</li>
<li><code>sqlmin!CQueryScan::GetRow</code>.</li>
</ul>

<p>That is interesting but what is more interesting is what we see after the <code>GetRow</code> calls: <code>sqllang!CShilohTds::SendRowImpl</code>. SQL Server calls <code>SendRowImpl</code> when it pushes data to the network packet which it then sends to the caller. We can confirm this is the case by using <em>WinDbg</em> and <em>Process Monitor</em>. So, run <em>Process Monitor</em> as admin and set an event filter which captures TCP Receive events for the <code>BxlServer.exe</code> process.</p>

<blockquote>
<p><strong>NOTE:</strong> If you are not sure how to set an event filter in <em>Process Monitor</em>, you can read about it in the <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a> post.</p>
</blockquote>

<p>Now, disable all breakpoints in <em>WinDbg</em> and execute the code in <em>Code Snippet 3</em>, (with <code>SELECT TOP(1) ...</code>) and watch the output from <em>Process Monitor</em>:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_procmon_pull_1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Process Monitor Output</em></p>

<p>What we see in <em>Figure 2</em> is similar to what we saw in <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a>, when we looked at the TCP packets SQL Server sends to the <code>BxlServer.exe</code> process. The outlined row in <em>Figure 2</em> is the packet with the result of the <code>SELECT</code>. I know that because I tried with multiple <code>TOP</code> values and saw which row changed length between the <code>TOP</code> values. In <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> the corresponding row had a length of 1358 as we did a <code>TOP(50)</code>. You may wonder what the three highlighted parts if the figure is and we come back to that in a little bit.</p>

<p>Now let us confirm that the outlined row is the row with the data:</p>

<ul>
<li>Re-enable (set) the breakpoint at <code>sqllang!SpExecuteExternalScript</code>.</li>
<li>Set a breakpoint at <code>sqllang!CShilohTds::SendRowImpl</code>.</li>
</ul>

<p>Execute the code and continue through until you break at <code>SendRowImpl</code>. If you look at the <em>Process Monitor</em> output, the last row you see is a row with a length of 405. That is the fourth row from the end as we see in <em>Figure 2</em>. Now continue the execution, and you see how <em>Process Monitor</em> outputs the three last rows, with a row with a length of 133 as the first one. We have &ldquo;thus&rdquo; proven that <code>SendRowImpl</code> sends the data row to the data packet which then SQL Server sends to the caller. Oh, the reason I use <code>TOP(1)</code> is that there is one call to <code>SendRowImpl</code> for each row, and I do not want to have to press F5 millions of times, or even 50 times.</p>

<h2 id="packets">Packets</h2>

<p>Now, what about the highlighted areas in <em>Figure 2</em>, what are those? First of all, if you look at the length of the packets it seems like each area have packets of the same length: in the yellow area, we see packets with a length of 37, 1245, 67 and 405, which we also see in the green and blue area. In <em>Figure 2</em> we also see that the highlighted areas originate from the same <code>BxlServer.exe</code> process (process id 6572), but different ports: 6799, 6800 and 6801. OK, let us see if we can figure out what this is all about, and to try to get a clearer picture we also want to filter on what <code>BxlServer.exe</code> sends to SQL Server. To see that, we add two new conditions to the existing <em>Process Monitor</em> filter:</p>

<ul>
<li>First condition - <em>Operation</em> (first drop-down) <em>is</em> (second dropdown): &ldquo;TCP Send&rdquo;.</li>
<li>Second condition - <em>Path</em> <em>contains</em>: the definition for the SQL Server port (1443). In my examples it is: <code>win10-dev:ms-sql-s</code>. We set this filter condition, so we only see the ODBC communication.</li>
</ul>

<p>Disable all breakpoints in <em>WinDbg</em> and execute the code and watch the <em>Process Monitor</em> output:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_procmon_pull_2.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Process Monitor Output - II</em></p>

<p>The output we see in <em>Figure 3</em> is quite similar to what we see in <em>Figure 2</em>, apart from that we also see the packets the <code>BxlServer.exe</code> process sends. We see the same structure of three separate sections of packets, and we have packets of the same length in the three sections. Once again we see the data packet being in the last section, once again with a length of 133 (the outlined row). What are these packets and why do we have three sections? Let us see if we can figure out what the packets do with the help of <em>WireShark</em>:</p>

<ul>
<li>Start <em>WireShark</em> as admin.</li>
<li>On the opening screen double click the adapter for <em>Npcap</em> (most likely named &ldquo;Npcap Loopback Adapter&rdquo;).</li>
</ul>

<p>This starts capturing events immediately and to stop the capture you click <strong>Ctrl + E</strong>. What we want to do now is to create a <em>WireShark</em> display filter, so we only see network packets we are interested in. What we are interested in are packets for port 1433, and we set the filter in the text box just underneath the toolbox: <code>tcp.port==1433</code>.  Finally, we click on the right arrow to the right the filter box to apply the filter (outlined in red below):</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_filter.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>WireShark Display Filter</em></p>

<p>TThe assumption at this stage is that the <em>WireShark</em> capture is off. Start the capture (&ldquo;Ctrl+E&rdquo;) and then immediately execute the code. We stop the capture as soon as the code has finished executing:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_output_I.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>WireShark Output</em></p>

<p>In <em>Figure 5</em> we see part of the output from <em>WireShark</em>, and we may wonder why we see more packets than when we looked at the output from <em>Process Monitor</em> and why the packet sixes are not the same. There are a couple of answers to that:</p>

<ul>
<li><em>Process Monitor</em> only show packets that has a body (data part), but more messages are going over the wire (<code>ACK</code> etc.), and <em>WireShark</em> shows all of them.</li>
<li>The packet size question is related to the point above. <em>WireShark</em> shows the bytes going over the wire and that includes headers and so forth, whereas <em>Process Monitor</em> only shows the data size.</li>
<li>The <em>WireShark</em> display filter is set on port 1433 (anything going in and out of SQL Server), whereas in <em>Process Monitor</em> we filtered on traffic to and from the <code>BxlServer.exe</code> process. The section in <em>Figure 5</em> highlighted in blue is an example of this; this is the communication between <em>SQL Server Management Studio</em> and SQL Server.</li>
</ul>

<h4 id="yellow-section">Yellow Section</h4>

<p>Ok, so what does <em>WireShark</em> tell us? In <em>Figure 5</em> I have highlighted in yellow the part that corresponds to the highlighted yellow part in <em>Figure 3</em>. For this exercise, we can ignore the packets whose <em>Protocol</em> is TCP and concentrate on the ones which have TDS as a protocol. Just by looking at the info we see that the packets have to do with login and authentication: <code>TDS7 pre-login message</code> followed by <code>Response</code> followed by more pre-login messages, followed by <code>TLS Exchange</code> and a <code>Response</code>. I am not going into any details about these packets, but if you want all the &ldquo;gory&rdquo; details about TDS you find documentation here: <a href="https://msdn.microsoft.com/en-us/library/dd304523.aspx">[MS-TDS]: Tabular Data Stream Protocol</a>.</p>

<p>We now realise that the packets in the first section in <em>Figure 3</em> has to do with authentication. Seeing that we see packets with the same length in the green and blue sections in <em>Figure 3</em> we can safely assume those packets are also authentication packets, and a quick look at the packets in <em>WireShark</em> confirms that. What about the packets that are different?</p>

<h4 id="green-section">Green Section</h4>

<p>In the green section there are packets with lengths of 386, 138, 23 and 22, and in the blue section, we see packet lengths of 330 and 133. Well, we do know what the 133 packet is - that is the packet with the data SQL Server sends to the <code>BxlServer.exe</code> process. So, let us go back to the <em>WireShark</em> output and see what we can find out:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_output_II.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>WireShark Output II</em></p>

<p>We see in <em>Figure 6</em> the last packets of the green section packets in <em>Figure 3</em>. In <em>Figure 6</em> the first packet corresponds to the 405 packet in the green section in <em>Figure 3</em> which we know is a response package. To see what type of response packet it is, we click on the row in the &ldquo;Packet List&rdquo; pane in <em>WireShark</em> and the &ldquo;Packet Details&rdquo; pane shows the details about the packet:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_env_change.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>WireShark Packet Details</em></p>

<p>Aha, in <em>Figure 7</em> we see that this packet is an <code>EnvChange</code> packet (yellow highlight) which SQL Server use to notify the caller of an environment change (for example, database, language, and so on). In this case, it is a database context change from <code>master</code> (grey highlight) to <code>testParallel</code> (blue highlight). So this packet appears in all three section sin <em>Figure 3</em>. Now let us look at the 386, 138, 23, and 22 packets, which in <em>WireShark</em> have lengths of 856, 360, 130 and 128 and are highlighted in <em>Figure 6</em> with yellow, green, blue and purple respectively.</p>

<p><strong>WireShark Packet 856</strong>:</p>

<p>When we click on the packet in the &ldquo;Packet List&rdquo; pane, the &ldquo;Packet Details&rdquo; looks like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_prepare.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>WireShark Packet Details II</em></p>

<p>The packet details we see in <em>Figure 8</em> tells us that this is a &ldquo;Remote Procedure Call&rdquo; (which we also can see in <em>Figure 6</em>), and the call is to <code>sp_prepare</code> (highlighted in blue). The procedure <code>sp_prepare</code> is generally used for performance reasons and prepares a parameterized Transact-SQL statement and returns a statement handle for execution. The &ldquo;Packet Bytes&rdquo; pane shows us a hex dump the statement:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_packet_bytes.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>WireShark Packet Bytes</em></p>

<p><strong>WireShark Packet 360</strong>:</p>

<p>The 360 packet (green) is a response packet, and when we look at the packet we see:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_col_metadata.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>WireShark Packet Details and Bytes</em></p>

<p>We see in <em>Figure 10</em> that the response to <code>sp_prepare</code> is column metadata for the query.</p>

<p><strong>WireShark Packet 130 and 128</strong>:</p>

<p>Finally, the last two packets in the green section is a call to <code>sp_unprepare</code> (packet 130) and an &ldquo;End of message&rdquo; response (packet 128) to that.</p>

<h4 id="blue-section">Blue Section</h4>

<p>On to the blue section in <em>Figure 3</em>. We already said that there are only two packets that are different to the ones in the yellow and green sections:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_output_III.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>WireShark Output III</em></p>

<p>The packet highlighted in yellow in <em>Figure 11</em> is the packet with a length of 330 in <em>Figure 3</em>, and the 350 packet (highlighted in green) is the <em>Figure 3</em> 133 packet.</p>

<p><strong>WireShark Packet 744</strong>:</p>

<p>The 744 packet looks like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_sql_batch.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>WireShark Output III</em></p>

<p>In <em>Figure 12</em> we see how the packet is a SQL batch (yellow highlight), and it contains the actual query (green and blue highlights). So this is the query that SQL Server executes.</p>

<p><strong>WireShark Packet 350</strong>:</p>

<p>As we have mentioned a few times, the 350 packet contains the result of the query executed from the 744 packet, and the packet details and bytes look like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_query_result.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>WireShark Output Query Result</em></p>

<p>With a cursory glance at <em>Figure 13</em> we may think there is no difference between what we see in <em>Figure 10</em> - which shows the response to the <code>sp_prepare</code> call - and what is in <em>Figure 13</em>. However, looking a bit closer we see that this packet contains the result of the query where we see highlighted in green and blue the two first column values, 2 and 6 respectively. The column values are not present in <em>Figure 10</em>.</p>

<h2 id="sections">Sections</h2>

<p>By now we sort of understand what goes on when we pull the data in the local context. However, why do we have these different sections, and why do we see multiple authentications? I have no answer why we see multiple authentications; maybe I can get an answer from someone &ldquo;in the know&rdquo;. As for the multiple sections; I am not 100% clear about that - but it seems to have something to do with what RevoScaleR function that accesses the data, remember from <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> how loading of the data happens not in the <code>RxSqlServerData</code> call, but in the call which uses the data. So why I say that the sections are related to this is that if we, instead of doing a <code>rxLinMod</code> call, did something like <code>ds &lt;-</code>rxImport(mydata)` then we only see the green and blue sections. Hopefully, I can get some more information about the sections as well which I can update this post with.</p>

<h2 id="summary">Summary</h2>

<p>So this post discusses the internal workings in SQL Server when we pull data in the local context. We talked about how SQL Server first sends the script data to the SQL Satellite through the usual <code>sqllang!CXStmtQuery::ErsqExecuteQuery</code> and <code>sqllang!CSatelliteCargoContext::SendChunkEndMessage</code>. After <code>SendChunkEndMessage</code> <code>ErsqExecuteQuery</code> is called again and at that stage the statement query executes.</p>

<p>The data transfer between the SqlSatellite (hosted by <code>BxlServer.exe</code>) and SQL Server happens through TDS packets, and we saw how authentication packages are sent multiple times, followed by <code>sp_prepare</code>. Finally, SQL Server executes the query and returns the result to the SqlSatellite.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 30]]></title>
    <link href="http://nielsberglund.com/2018/07/29/interesting-stuff---week-30/" rel="alternate" type="text/html"/>
    <updated>2018-07-29T19:11:58+02:00</updated>
    <id>http://nielsberglund.com/2018/07/29/interesting-stuff---week-30/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/chaos-engineering-autonous-vehicles">Properties of Chaos</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation about how and why chaos engineering is being applied to autonomous vehicle safety, and how to advance chaos engineering practices to explore beyond basic properties like system availability and extend into verifying system correctness.</li>
</ul>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="https://blog.pulumi.com/program-the-cloud-with-12-pulumi-pearls">Program the Cloud with 12 Pulumi Pearls</a>. In the <a href="/2018/06/24/interesting-stuff---week-25/">week 25</a> roundup I wrote about <a href="https://twitter.com/funcOfJoe">Joe Duffy</a> and his <a href="http://pulumi.com/">Pulumi</a>. This blog post by <a href="https://twitter.com/funcOfJoe">Joe</a> demonstrates some fun ways you can program the cloud using Pulumi. Very interesting!</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/sqlserverstorageengine/2018/07/23/pearl-abyss-massive-scale-using-azure-sql-database/">Pearl Abyss: Massive Scale using Azure SQL Database</a>. A blog post, showing off how you can achieve a massive scale of SQL Server by using Azure SQL Database. 7,500 cores, 46 instances 500,000 queries per second! Can I have two of those, please!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.youtube.com/watch?v=ExEWJVjj-RA">Demo: Build a Streaming Application with KSQL</a>. A YouTube video, illustrating how to build a streaming application using KSQL.</li>
<li><a href="https://www.confluent.io/blog/ansible-playbooks-for-confluent-platform/">Ansible Playbooks for Confluent Platform</a>. This blog post covers how <a href="https://www.confluent.io/">Confluent</a> wants to make it super easy to set up <a href="https://www.confluent.io/product/confluent-platform/">Confluent Platform</a>. In this post, they introduce their first set of Ansible playbooks. It certainly seems easy!</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="http://blog.revolutionanalytics.com/2018/07/a-quick-tour-of-ai-services-in-azure.html">A quick tour of AI services in Azure</a>. A post by <a href="https://twitter.com/revodavid">David</a> at <a href="http://blog.revolutionanalytics.com">Revolution Analytics</a> talking about a video giving a quick overview of some of the services available in Azure to build AI-enabled applications. Watch the video if you are interested in AI and Azure.</li>
<li><a href="https://blogs.msdn.microsoft.com/mlserver/2018/07/26/dockerizing-r-and-python-web-services/">Dockerizing R and Python Web Services</a>.  A post looking into how to build a Docker image containing <strong>Machine Learning Server 9.3</strong>using Dockerfiles and how-to-perform Both R and Python operations.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I am close to finish the third post in the <a href="/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series. I hope to have it out by the end of this upcoming week.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 29]]></title>
    <link href="http://nielsberglund.com/2018/07/22/interesting-stuff---week-29/" rel="alternate" type="text/html"/>
    <updated>2018-07-22T16:06:58+02:00</updated>
    <id>http://nielsberglund.com/2018/07/22/interesting-stuff---week-29/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending. Unfortunately there was not much that caught my eye this week, so this is a short post.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2018/07/zuul-push-messaging">Scaling Push Messaging for Millions of Devices @Netflix - Susheel Aroskar at QCon NY</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about Zuul Push, which is a high performance asynchronous service based on Apache Netty, a non-blocking I/O (NIO)-based network application framework. The technology handles more than 5.5 million connected clients at peak.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/apache-kafka-vs-enterprise-service-bus-esb-friends-enemies-or-frenemies/">Apache Kafka vs. Enterprise Service Bus (ESB)—Friends, Enemies, or Frenemies?</a>. A blog post showing why so many enterprises leverage the open source ecosystem of Apache Kafka for successful integration of different legacy and modern applications, and how this differs but also complements existing integration solutions like ESB or ETL tools.</li>
<li><a href="https://databricks.com/blog/2018/07/19/simplify-streaming-stock-data-analysis-using-databricks-delta.html">Simplify Streaming Stock Data Analysis Using Databricks Delta</a>. A post about how Databricks Delta helps solving many of the pain points of building a streaming system to analyze stock data in real-time.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.usejournal.com/python-vs-and-r-for-data-science-833b48ccc91d">Python vs (and) R for Data Science</a>. This post acts as a guide for those wishing to choose between Python and R Programming languages for Data Science.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I am <del>busy</del> struggling with writing a couple of posts in the <a href="/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series, as well as the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I do hope to have at least one out in a weeks time (or so).</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 28]]></title>
    <link href="http://nielsberglund.com/2018/07/15/interesting-stuff---week-28/" rel="alternate" type="text/html"/>
    <updated>2018-07-15T06:15:28+02:00</updated>
    <id>http://nielsberglund.com/2018/07/15/interesting-stuff---week-28/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/07/09/system-io-pipelines-high-performance-io-in-net/">System.IO.Pipelines: High performance IO in .NET</a>. A blog post which announces <code>System.IO.Pipelines</code> which is a new library that is designed to make it easier to do high performance IO in .NET. I wish it had been around when we wrote socket code, way back when.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/netflix-techblog/evolution-of-application-data-caching-from-ram-to-ssd-a33d6fa7a690">Evolution of Application Data Caching : From RAM to SSD</a>. A blog post about Netflix and how they move from purely RAM based caches to a hybrid between RAM and SSD. Very, very interesting!</li>
<li><a href="https://medium.com/netflix-techblog/auto-scaling-production-services-on-titus-1f3cd49f5cd7">Auto Scaling Production Services on Titus</a>. This blog post, also from Netflix, discusses auto-scaling on their container management system <a href="https://medium.com/netflix-techblog/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436">Titus</a>. It is interesting to read how the interaction happens between the AWS Auto Scaling engine and Titus.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/kafka-1-0-on-hdinsight-lights-up-real-time-analytics-scenarios/">Kafka 1.0 on HDInsight lights up real-time analytics scenarios</a>. This Microsoft blog post discusses the advantages that Kafka 1.0 on Azure HDInsight provides for data scientists and data analysts.</li>
<li><a href="/2018/07/10/install-confluent-platform-kafka-on-windows/">Install Confluent Platform (Kafka) on Windows</a>. This post from yours truly discusses we can install Kafka, in the guise of Confluent Platform, on Windows Subsystem for Linux (WSL). Useful if you, like me, is a Windows dude (or dudette) and you want to run Kafka locally on your development box.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="http://blog.revolutionanalytics.com/2018/07/r-351-update-now-available-.html">R 3.5.1 update now available</a>. This post by <a href="https://twitter.com/revodavid">David</a> talks about the new version of R: 3.5.1.</li>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/07/09/announcing-ml-net-0-3/">Announcing ML.NET 0.3</a>. At Build 2018 Microsoft released ML.NET 0.1, a cross-platform, open source machine learning framework for .NET developers, and I posted about it in the <a href="/2018/05/13/interesting-stuff---week-19/">week 19 roundup</a>. A month or two later they released ML.NET 0.2 which I covered in the roundup for <a href="/2018/06/10/interesting-stuff---week-23/">week 23</a>. It is now time for ML.NET 0.3 with quite a few new enhancements. What interests me is to see what &ldquo;cool&rdquo; new features application developers dreams up with this.</li>
<li><a href="https://www.infoq.com/presentations/r-framework-ai-apps">R for AI developers</a>. So, <a href="https://twitter.com/revodavid">David</a> is at it again. This time he did a presentation at QCon.ai where he makes a case for R as a platform for developing models for intelligent applications. The presentation is a must-see!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Confluent Platform (Kafka) on Windows]]></title>
    <link href="http://nielsberglund.com/2018/07/10/install-confluent-platform-kafka-on-windows/" rel="alternate" type="text/html"/>
    <updated>2018-07-10T18:43:48+02:00</updated>
    <id>http://nielsberglund.com/2018/07/10/install-confluent-platform-kafka-on-windows/</id>
    <content type="html"><![CDATA[<p>You who follows my blog and have read my weekly roundups you know that I am quite (that is an understatement) interested in Apache Kafka and I am curious to find out what &ldquo;cool&rdquo; things one can do with it. For that, I want to be able to test &ldquo;stuff&rdquo; quickly. When I test and try out new things, I usually do it on my development box which contains everything I need: <strong>SQL Server</strong>, <strong>RabbitMQ</strong>, <strong>RStudio</strong>, <strong>Microsoft Machine Learning Server</strong>, <strong>Visual Studio</strong> and the list goes on.</p>

<p>So seeing that I have most of my &ldquo;tools of the trade&rdquo; on my machine I obviously also would like to have Kafka on the box. Herein lies a problem, I am a Windows dude and Kafka, and Windows do not gel. Yes, some people are running Kafka on Windows, but it is a chore. OK, so what to do? Sure, I could potentially run Kafka on a virtual machine, or in a Docker image, but it is not as transparent as I would like it to be (yeah, I am lazy).</p>

<p>Hmm, Microsoft did introduce the ability to run Linux binary executables (in ELF format) natively on Windows 10 in Windows 10 version 1607. The feature is called <strong>Windows Subsystem for Linux</strong> (WSL), and since I am now running version 1803, maybe I should try and install Kafka in <em>WSL</em>.</p>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>To install and run the <strong>Confluent Platform</strong>, (which contains Kafka), on <em>WSL</em> there are some pre-reqs:</p>

<ul>
<li><em>WSL</em> installed (fairly obvious).</li>
<li>Java 1.7 or later.</li>
</ul>

<h2 id="windows-subsystem-for-linux">Windows Subsystem for Linux</h2>

<p><em>WSL</em> is primarily aimed at developers, and it allows you to run Linux environments directly on Windows in a native format and without the overhead of a virtual machine. Let us retake a look at that statement: <em>run Linux environments directly on Windows in a native format</em>. Yes native format, <em>WSL</em> is not a UNIX-like environment like Cygwin, which wraps non-Windows functionality in Win32 system calls but it serves Linux programs as special, isolated minimal processes (<em>pico-processes</em>) attached to kernel-mode <em>pico-providers</em>. If you want to read all the &ldquo;gory&rdquo; details about <em>WSL</em>: <a href="https://blogs.msdn.microsoft.com/wsl/2016/04/22/windows-subsystem-for-linux-overview/">Windows Subsystem for Linux Overview</a> gives you an excellent introduction.</p>

<p>Installing <em>WSL</em> is very easy; you first enable <em>WSL</em> either via a Powershell command: <code>Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux</code> or by switching it on from &ldquo;Turn Windows features on or off&rdquo; via &ldquo;Control Panel | Programs and Features&rdquo;:</p>

<p><img src="/images/posts/inst_kafka_wsl_enable_wsl.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Enable WSL</em></p>

<p>You should restart the machine after enabling <em>WSL</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> So I have enabled <em>WSL</em> a few times now, and some of the times I have not had to restart.</p>
</blockquote>

<p>When <em>WSL</em> is enabled you need to download and install a distro from the Windows Store. When <em>WSL</em> first was introduced the only distro available was Ubuntu, since then quite a few have been added, and now the distro list looks like so:</p>

<ul>
<li>Ubuntu</li>
<li>OpenSUSE</li>
<li>SLES</li>
<li>Kali Linux</li>
<li>Debian GNU/Linux</li>
</ul>

<p>I started with <em>WSL</em> when Ubuntu was the only distro available, so I have &ldquo;stuck&rdquo; with it, but I do not think the distros are that different. To continue, you choose a distro and let it install. Finally, you start the command shell for the distro from the Windows &ldquo;Start&rdquo; menu:</p>

<p><img src="/images/posts/inst_kafka_wsl_start_distro.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Start the Distro</em></p>

<p>When you start up the distro for the first time the setup finishes, and you are prompted to enter a root password. Now is probably a good time to run <code>sudo apt-get update -y &amp;&amp; sudo apt-get upgrade -y</code> where <code>sudo apt-get update -y</code> updates the list of all current program packages in the repositories to determine which packages are candidates for upgrading. The command <code>sudo apt-get upgrade -y</code> upgrades all current program packages in the operating system.</p>

<blockquote>
<p><strong>NOTE:</strong> The above commands are for Ubuntu, so if you have another distro installed check the commands for that particular distro.</p>
</blockquote>

<h4 id="java">Java</h4>

<p>When your distro is up and running, you can now install Java. When reading the documentation about <a href="https://docs.confluent.io/current/installation/versions-interoperability.html#java">supported Java versions</a>, you see that <strong>Confluent Platform</strong> 4.1 is the last version with support for Java 1.7. The <strong>Confluent Platform</strong> version I use is the latest preview (version 5.x), so I install 1.8. Oh, and do not try with 1.9 -  it does not work.</p>

<p>The <a href="https://docs.confluent.io/current/installation/versions-interoperability.html#java">docs</a> mention the JDK, but I have found that the JRE works as well (since I am not writing any Java code) and I use the open source version of Java - OpenJDK. So to install you run the following from the bash shell:</p>

<pre><code class="language-bash">&gt; sudo apt install openjdk-8-jre-headless 
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Install JRE</em></p>

<p>As <em>WSL</em> has no GUI, I choose to install the headless version of the JRE as we see in <em>Code Snippet 1</em>. Finally, to check that it installed correctly I do <code>&gt; java -version</code> and the result is like so:</p>

<pre><code class="language-bash">openjdk version &quot;1.8.0_171&quot;
OpenJDK Runtime Environment (build 1.8.0_171-8u171-b11-0ubuntu0.18.04.1-b11)
OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode)
</code></pre>

<p><strong>Code Snippet 2:</strong> Java Version Output*.</p>

<p>In <em>Code Snippet 2</em> everything looks OK, so we can now go ahead with the main attraction.</p>

<h2 id="confluent-platform">Confluent Platform</h2>

<p>By now you may ask yourself what is this thing <strong>Confluent Platform</strong>? Well, <a href="https://confluent.io">Confluent</a> is a company founded by the guys (and girls) that originally built Kafka back at LinkedIn. The company is now focusing on building a streaming platform to help other companies get easy access to enterprise data as real-time streams.</p>

<p>The <strong>Confluent Platform</strong> improves Apache Kafka by expanding its integration capabilities, adding tools to optimise and manage Kafka clusters, and methods to ensure the streams are secure. <strong>Confluent Platform</strong> makes Kafka easier to build and easier to operate. The <strong>Confluent Platform</strong> comes in two flavours:</p>

<ul>
<li><a href="https://www.confluent.io/product/confluent-open-source/">Confluent Open Source</a> is freely downloadable.</li>
<li><a href="https://www.confluent.io/product/confluent-enterprise/">Confluent Enterprise</a> is available on a subscription basis.</li>
</ul>

<p>Back in April Confluent started releasing preview versions of the <strong>Confluent Platform</strong> with the latest and the greatest and that is what I am using. At the time I write this the June preview has just been released, and that is what I am installing here.</p>

<h4 id="installation">Installation</h4>

<p>Before we can install, we need to download the install media which you do from <a href="https://www.confluent.io/preview-release">here</a>. When clicking the &ldquo;Download &hellip;&rdquo; button, a form &ldquo;pops up&rdquo; where you choose your download format and enter your details. I usually choose <code>tar.gz</code>, and that is what I base the following instructions on. Download the file to your PC and then in the bash shell:</p>

<ul>
<li>Create a directory where to extract the files to.</li>
<li><code>cd</code> to the download directory:</li>
</ul>

<p><img src="/images/posts/inst_kafka_wsl_mkdir.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Make Kafka Directory</em></p>

<p>In <em>Figure 3</em> we see how I create the <code>/opt/kafka</code> directory, and how I <code>cd</code> to the Windows directory where my downloaded files are. One of the cool things with <em>WSL</em> is that the local Windows drives gets automatically mounted under the <code>/mnt</code> folder. I can now extract the files:</p>

<blockquote>
<p><strong>NOTE:</strong> The only reason I chose to create the <code>kafka</code> directory under <code>/opt</code> is that traditionally <code>/opt</code> is for third-party applications.</p>
</blockquote>

<p><img src="/images/posts/inst_kafka_wsl_untar.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Extract Files</em></p>

<p>So I <code>tar</code> the files, and we see in <em>Figure 4</em> how the files are extracted. To extract the files takes a couple of minutes and when done we can drill down into the extracted directories and files:</p>

<p><img src="/images/posts/inst_kafka_wsl_kafka_dirs_files.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Directory and File Structure</em></p>

<p>In <em>Figure 5</em> we see how directories and files ended up under a <code>confluent-version...</code> directory (outlined in white) and when we <code>ls</code> into that directory we see sub-directories (also outlined in white), and amongst them a <code>bin</code> directory.</p>

<p>When we drill down into the <code>bin</code> directory and list the content we see a file named <code>confluent</code>. This is an executable file, and we use this file to start and stop all the Confluent components. The <code>bin</code> directory also contains executable files to start and stop individual components, such as <code>kafka-server-start</code>, <code>kafka-server-stop</code> and <code>zookeeper-server-start</code>, and so forth.</p>

<p>Right, enough of this - let us see if we can get the show on the road and spin up all components:</p>

<p><img src="/images/posts/inst_kafka_wsl_start_kafka.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Start Confluent</em></p>

<p>To start all the Confluent components, we use the command <code>sudo ./confluent start (from the</code>bin` directory) and in <em>Figure 6</em> we see how the various components startup, awesome!</p>

<h4 id="control-center">Control Center</h4>

<p>Part of the <strong>Confluent Platform</strong> installation (Enterprise version) is the <em>Control Center</em>. The <em>Control Center</em> (I copied the text from the <a href="https://www.confluent.io/confluent-control-center/">Control Center</a> site) &ldquo;gives the administrator monitoring and management capabilities, delivering automated, curated dashboards so that Kafka experts can easily understand what is happening without tweaking dashboards&rdquo;. So let us see if we can connect with the <em>Control Center</em>. If we connect from the same machine as we installed <strong>Confluent Platform</strong> on, the address is <code>http://localhost:9021</code>:</p>

<p><img src="/images/posts/inst_kafka_wsl_control_center.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Confluent Control Center</em></p>

<p>Cool, <em>Control Center</em> seems to be up and running, let us now use it to create a <em>Topic</em> so we can do a final test.</p>

<h4 id="topics">Topics</h4>

<p>When you send messages to a Kafka broker, you typically send it to a &ldquo;Topic&rdquo;, which is like a collection point in the broker for &ldquo;like-minded&rdquo; messages. If you are a database dude like me, you can see it as a table in a database where you keep records of the same type.</p>

<p>Typically you create multiple topics in you Kafka cluster to cater for multiple message types, and <em>Control Center</em> can help you with that. In <em>Figure 7</em> we see at the bottom left corner, outlined in red, &ldquo;Topics&rdquo;. Click on that, and you see existing default topics. Click on, in the far right corner, the &ldquo;Create topic&rdquo; button and you see something like so:</p>

<p><img src="/images/posts/inst_kafka_wsl_create_topic.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Topics</em></p>

<p>In the topic name box enter &ldquo;testing&rdquo; and then click &ldquo;Create with defaults&rdquo; and we are back seeing the existing topics as well as the newly created &ldquo;testing&rdquo; topic:</p>

<p><img src="/images/posts/inst_kafka_wsl_topic_created.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>New Topic</em></p>

<p>When we have a topic, we can now see whether we can publish and consume messages.</p>

<h2 id="test-send-receive">Test Send &amp; Receive</h2>

<p><strong>Confluent Platform</strong> is now up and running, and you can now start doing &ldquo;cool&rdquo; stuff. However, to make sure everything works let us use the built-in command line clients to send and receive some test messages.</p>

<p>What we do is that in the open bash shell we <code>cd</code> to the <code>/opt/kafka/confluent-xxx/bin/</code> directory. We use the command line producer <code>kafka-console-producer</code> to send messages:</p>

<pre><code class="language-bash">sudo ./kafka-console-producer --broker-list localhost:9092 --topic testing
&gt;Hello World!
&gt;Life Is Awesome!
&gt;We Have Installed Kafka on Windows!
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Publishing Messages</em></p>

<p>We see in <em>Code Snippet 3</em> how we target the local broker on port 9092, and the topic we send to is the &ldquo;testing&rdquo; topic we created above. After hitting enter, we create one message after each other (hit enter in between).</p>

<p>To consume messages we open a second bash shell and <code>cd</code> into the <code>/bin</code> directory as before, and to receive messages we use the <code>kafka-console-consumer</code> command line client:</p>

<pre><code class="language-bash">sudo ./kafka-console-consumer --bootstrap-server localhost:9092 --topic testing --from-beginning
Hello World!
Life Is Awesome!
We Have Installed Kafka on Windows!
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Consume Messages</em></p>

<p>When running the consumer we see in <em>Code Snippet 4</em> how we receive the messages we just sent. If we were to go back to the publisher and create some more messages we immediately see them in the consumer bash shell. It works! So now we can start creating streaming applications using proper <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">clients</a>. If you write .NET applications, I suggest you look at the <a href="https://github.com/confluentinc/confluent-kafka-dotnet">Confluent client</a> which is very feature rich.</p>

<p>When we are done with the <strong>Confluent Platform</strong>, we stop it from the <code>/bin</code> directory:</p>

<p><img src="/images/posts/inst_kafka_wsl_stop_kafka.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Stopping Kafka</em></p>

<p>We stop Kafka by calling <code>sudo ./confluent stop</code> and then as <em>Figure 10</em> shows, all components shut down in an orderly fashion.</p>

<p>We have installed <strong>Confluent Platform</strong> on <em>WSL</em>, started it, published and consumed some messages and stopped it. All is good! Or is it?</p>

<h2 id="issue">ISSUE</h2>

<p>So what happens when you try to start the platform again:</p>

<p><img src="/images/posts/inst_kafka_wsl_start_error.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Error at Start Up</em></p>

<p>That is not good! We get an error when we try to start the platform after a shutdown. What we see here is a &ldquo;half&rdquo; known issue which is most prevalent on Windows machines, and it has to do with Kafka trying to clean up old log files. If you drill down in the Kafka log files you see an error looking something like this: <code>FATAL Shutdown broker because all log dirs in &lt;path_to_logs&gt; have failed (kafka.log.LogManager)</code>.</p>

<p>At the moment I do not have a solution for it other than that before each startup run something like so: <code>sudo rm -fr /tmp/confl*</code> which removes all Kafka related log directories. This is obviously not a solution in a production environment or a &ldquo;proper&rdquo; test/dev environment but for us just wanting to do some &ldquo;quick and dirty&rdquo; testing on <em>WSL</em> it is sufficient.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we discussed a little bit what <em>WSL</em> is and how we can install <strong>Confluent Platform</strong> on <em>WSL</em>. We looked at we can test the installation by creating a topic and then publish and consume messages using the command line publish and consume clients.</p>

<p>Having <strong>Confluent Platform</strong> installed we can now use a <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">client of choice</a> to start doing &ldquo;cool&rdquo; stuff. Keep an eye on my blog for future <strong>Confluent Platform</strong> and Kafka posts!</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 27]]></title>
    <link href="http://nielsberglund.com/2018/07/08/interesting-stuff---week-27/" rel="alternate" type="text/html"/>
    <updated>2018-07-08T12:11:28+02:00</updated>
    <id>http://nielsberglund.com/2018/07/08/interesting-stuff---week-27/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/ip-filtering-for-event-hubs-and-service-bus/">IP filtering for Event Hubs and Service Bus</a>. A frequently asked for feature in Azure Event Hubs is the ability to restrict access to the Event Hubs to certain well-known sites, alternatively rejecting traffic from specific IP addresses. The Azure team has now announced a public preview of IP filtering.</li>
<li><a href="https://databricks.com/blog/2018/07/02/build-a-mobile-gaming-events-data-pipeline-with-databricks-delta.html">Build a Mobile Gaming Events Data Pipeline with Databricks Delta</a>. This blog post shows how to build a data pipeline in AWS using the <a href="https://databricks.com/product/unified-analytics-platform">Databricks Unified Analytics Platform</a>. Even though they in the blog post use AWS it should be possible to do the same on Azure since Databricks is now available there as well.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="http://mattwarren.org/2018/07/05/.NET-JIT-and-CLR-Joined-at-the-Hip/">.NET JIT and CLR - Joined at the Hip</a>. This is another excellent blog post by <a href="https://twitter.com/matthewwarren">Matthew</a>, looking at how the .NET JIT compiler works together with CLR.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2018/07/boner-events-first-microservices">QCon NY: Jonas Bonér on Designing Events-First Microservices</a>. An <a href="https://www.infoq.com/">InfoQ</a> article summarising a QCon talk by <a href="http://jonasboner.com/">Jonas Bonér</a> of <a href="https://akka.io/">Akka</a> fame. The main point Jonas makes is that events-first domain-driven design (DDD) and event streaming are critical in a microservices based architecture. Oh and by th way, if you are interested in event-driven systems and microservices go and download Jonas mini-book <a href="https://www.lightbend.com/blog/reactive-microsystems-the-evolution-of-microservices-at-scale-free-oreilly-report-by-jonas-boner">Reactive Microsystems - The Evolution Of Microservices At Scale</a>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/june-preview-release-confluent-plaform/">June Preview Release: Packing Confluent Platform with the Features You Requested!</a>. A blog post by the Confluent team announcing the latest preview release of Confluent Platform. This release packs quite a few new features, and I am especially interested in the KSQL support for nested data as well as the ability to join two streams together.</li>
<li><a href="https://www.infoq.com/news/2018/07/event-sourcing-kafka-streams">Experiences from Building an Event-Sourced System with Kafka Streams</a>. An article from <a href="https://www.infoq.com/">InfoQ</a> about how engineers at <a href="https://www.wix.com/">Wix</a> built an event sourced system using <a href="https://kafka.apache.org/documentation/streams/">Kafka Streams</a>.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0">A Feature Selection Tool for Machine Learning in Python</a>. This blog post is about a Python tool that helps with feature selection in a dataset.</li>
<li><a href="https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420">A Complete Machine Learning Project Walk-Through in Python: Part One</a>. First post in a series walking through a complete Python machine learning solution.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<ul>
<li><a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a>. I finally published part 2 of the <a href="/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series. In the post, I tried to figure out why the performance is so much better when executing in a <strong>SQL Server Compute Context</strong> in <strong>SQL Server ML Services</strong> compared to executing in the local context (it is SQL Server after all). Even though I &ldquo;sort of&rdquo; figured it out, a few questions arose and hopefully I can answer those questions in a future post.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/spees_and_sql_compute_context) -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

