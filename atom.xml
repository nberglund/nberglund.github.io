<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="http://nielsberglund.com/atom.xml" rel="self"/>
  <link href="http://nielsberglund.com"/>
  <updated>2018-12-19T17:12:39+02:00</updated>
  <id>http://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Extensibility Framework &amp; Java - Null Values]]></title>
    <link href="http://nielsberglund.com/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/" rel="alternate" type="text/html"/>
    <updated>2018-12-19T17:12:39+02:00</updated>
    <id>http://nielsberglund.com/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/</id>
    <content type="html"><![CDATA[<p>This post is the third post where I look at the Java extension in SQL Server, i.e. the ability to execute Java code from inside SQL Server. The previous two posts are:</p>

<ul>
<li><a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>: We looked at installing and enabling the Java extension, as well as some very basic Java code.</li>
<li><a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">SQL Server 2019 Extensibility Framework &amp; Java - Passing Data</a>: In this post we discussed what is required to pass data back and forth between SQL Server and Java.</li>
</ul>

<p>In this post we look at something related to the <a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">data passing post</a>; how to handle null values.</p>

<p></p>

<blockquote>
<p><strong>DISCLAIMER:</strong> <em>This post contains Java code. I am not a Java guy, in fact, the only Java I have ever written is the code in this post and the previous SQL Server 2019 Java posts. So, the code is not elegant in any shape or form, and I am absolutely certain it can be done in a much better way. However, this is not about Java as such, but how you call Java code from SQL Server, and what you need to implement on the Java side.</em></p>
</blockquote>

<p>Before we dive into this post&rsquo;s topic, let us do a recap.</p>

<h2 id="recap">Recap</h2>

<p>In the <a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">SQL Server 2019 Extensibility Framework &amp; Java - Passing Data</a> post, we looked at how we pass data back and forth between SQL Server and Java. In the Java extensions we do not have the <code>InputDataSet</code>, and <code>OutputDataSet</code> variables, so we need to define class member arrays for the columns we send in and pass back out, as well as a variable indicating the number of columns we return:</p>

<ul>
<li><strong><code>inputDataCol</code></strong><strong><em><code>N</code></em></strong>: array variable representing the input columns, where <em>N</em> is the column number (1 based).</li>
<li><strong><code>outputDataCol</code></strong><strong><em><code>N</code></em></strong>: array variable representing the output columns, where <em>N</em> is the column number (1 based).</li>
<li><strong><code>numberofOutputCols</code></strong>: it represents the number of columns returned from Java, and it is always required - regardless if you return columns or not.</li>
</ul>

<p>In addition to these variables we need two variables mapping null values:</p>

<ul>
<li><strong><code>inputNullMap</code></strong>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
<li><strong><code>outputNullMap</code></strong>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
</ul>

<p>All the <code>input</code><em><code>xxx</code></em> variables get populated automatically, whereas you need to populate the <code>output</code><em><code>xxx</code></em> variables in the code.</p>

<p>In the <a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">post</a> we had some example code looking like so:</p>

<pre><code class="language-java">public class DataPassing {
  //input data variables 
  static public int[] inputDataCol1 = new int[1];
  static public int[] inputDataCol2 = new int[1];
  static public int[] inputDataCol3 = new int[1];
  static public boolean[][] inputNullMap = new boolean[1][1];

  //output variables
  static public int[] outputDataCol1;
  static public int[] outputDataCol2;
  static public int[] outputDataCol3;
  static public boolean[][] outputNullMap;

  static public short numberOfOutputCols;

  public static void bar() {
    
    int numRows = inputDataCol1.length;
    numberOfOutputCols = 3;

    outputDataCol1 = new int[numRows];
    outputDataCol2 = new int[numRows];
    outputDataCol3 = new int[numRows];

    for(int x = 0; x &lt; numRows; x++) {
      outputDataCol1[x] = inputDataCol1[x];
      outputDataCol2[x] = inputDataCol2[x];
      outputDataCol3[x] = inputDataCol3[x];
    }

    outputNullMap = new boolean[numberOfOutputCols][numRows];
  }

}
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Example Code</em></p>

<p>The code we see in <em>Code Snippet 1</em> represents a class to which we pass in a data set consisting of three columns. The class passes back a three column data set. What the code does should be pretty self-explanatory, but there are two array variables that we are not doing much with: <code>inputNullMap</code> and <code>outputNullMap</code>, and today we look at them.</p>

<h2 id="demo-data">Demo Data</h2>

<p>In today&rsquo;s post, we use some data from the database, so let us set up the necessary database, a table, and load data into the table:</p>

<pre><code class="language-sql">USE master;
GO
SET NOCOUNT ON;
GO
DROP DATABASE IF EXISTS JavaNullDB;
GO
CREATE DATABASE JavaNullDB;
GO
USE JavaNullDB;
GO
DROP TABLE IF EXISTS dbo.tb_NullRand10
CREATE TABLE dbo.tb_NullRand10(RowID int identity primary key, 
                          x int, y int, col1 nvarchar(50));
GO
INSERT INTO dbo.tb_NullRand10(x, y, col1)
SELECT TOP(10) CAST(ABS(CHECKSUM(NEWID())) % 14 AS int) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS int)
  , N'Hello ' + CAST(CAST(ABS(CHECKSUM(NEWID())) % 25 AS int) AS nvarchar(50))
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO
UPDATE dbo.tb_NullRand10
  SET y = NULL
WHERE RowId = 3
UPDATE dbo.tb_NullRand10
  SET Col1 = NULL
WHERE RowId = 5    
UPDATE dbo.tb_NullRand10
  SET x = NULL
WHERE RowId = 6 
UPDATE dbo.tb_NullRand10
  SET y = NULL
WHERE RowId = 8  
UPDATE dbo.tb_NullRand10
  SET col1 = NULL
WHERE RowId = 9 
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Database Objects</em></p>

<p>We see from <em>Code Snippet 2</em> how we:</p>

<ul>
<li>Create a database: <code>JavaNullDB</code>.</li>
<li>Create a table: <code>dbo.tb_NullRand10</code>.</li>
<li>Insert some data into the table.</li>
</ul>

<p>The data we insert is entirely random, but it gives us something to &ldquo;play around&rdquo; with. Now, when we have a database and some data let us get started.</p>

<h2 id="null-values">Null Values</h2>

<p>So why do we care about null values? Well, the reason is that in SQL Server all data types are nullable, whereas in Java that is not the case. In Java, like in .NET, primitive types (<code>int</code>, etc.) cannot be null, so code like this:</p>

<pre><code class="language-java">public static void foo() {
    int x;
    x = null;
  }
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Null Value</em></p>

<p>When we try and compile the code in <em>Code Snippet 3</em> we get a compile-time error: <code>error: incompatible types: &lt;null&gt; cannot be converted to int</code>. In this case, the compiler saves us, but if we now think about passing in data from SQL Server, we can get into trouble as columns in the dataset can be null. So what do we do?</p>

<p>In the <strong>SQL Server 2019 Java Extension</strong> there are a couple of things helping with null values:</p>

<ul>
<li><strong>Extension components</strong>.</li>
<li><strong>Null maps</strong>.</li>
</ul>

<h4 id="extension-components">Extension Components</h4>

<p>In previous posts, I have briefly mentioned the Java extension components. They are similar to the launchers, and the &ldquo;link&rdquo; dll&rsquo;s for R/Python, and they are involved when passing data to Java as well as receiving data from Java.</p>

<blockquote>
<p><strong>NOTE:</strong> I cover these components in future posts.</p>
</blockquote>

<p>Let us try to get an understanding of what the components do when passing data to Java. We start with doing a simple <code>SELECT RowID, x, y FROM dbo.tb_NullRand10</code>, where <code>x</code> and <code>y</code> are integer columns:</p>

<p><img src="/images/posts/sql_2k19_java_null_null_result1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Select with NULL</em></p>

<p>In <em>Figure 1</em>, (highlighted in red), we see, which we also mentioned above, how primitive types are nullable in SQL. However, let us say we have some Java code looking like so:</p>

<pre><code class="language-java">public class NullValues {
  static public int[] inputDataCol1 = new int[1];
  static public int[] inputDataCol2 = new int[1];
  static public int[] inputDataCol3 = new int[1];
  static public boolean[][] inputNullMap = new boolean[1][1];

  static public short numberOfOutputCols;

  public static void foo() {
    
    for(int x = 0; x &lt; inputDataCol1.length; x++) {
    System.out.printf(&quot;%d\t\t%d\t\t%d\n&quot;, 
                        inputDataCol1[x],
                        inputDataCol2[x], 
                        inputDataCol3[x]);
    }
  }
}
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Java Code</em></p>

<p>In the code in <em>Code Snippet 4</em> we see how we expect a three-column dataset passed in, which we then print out in the <code>foo</code> method. The immediate problem is that, as we see in <em>Figure 1</em>, the dataset consists of null values, so what happens if we execute some SQL code looking like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'NullValues.foo'
, @input_data_1 = N'SELECT RowID, x, y FROM dbo.tb_NullRand10';
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>SQL Pushing in Null Values</em></p>

<p>When we run the code in <em>Code Snippet 5</em> the result looks as follows:</p>

<p><img src="/images/posts/sql_2k19_java_null_null_result2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Dataset with Null Values in Java</em></p>

<p>Ok, we did not crash and burn, that is good! However, what about the rows with the null values, <code>RowId</code>&rsquo;s 3, 6, and 8? Notice in <em>Figure 2</em> how the values printed out are set to 0 where we in <em>Figure 1</em> saw nulls. So <em>something</em> has &ldquo;automagically&rdquo; converted the nulls to 0&rsquo;s. That <em>something</em> is one of the Java extension components, and from a high level it works something like so (this is somewhat of guesswork from me):</p>

<ol>
<li>SQL Server pushes the data into the component.</li>
<li>The component loops through the data and replaces null values with the data type&rsquo;s default value.</li>
<li>The component calls into the relevant class and method and copies the data into the column variables.<br /></li>
</ol>

<p>That is nice, but now we receive 0&rsquo;s instead for nulls, what about if we want to handle null differently than 0. Think about how SQL Server handles null in some of its functions; columns with null values are ignored. With nulls replaced with 0&rsquo;s, what do we do?</p>

<h4 id="null-maps">Null Maps</h4>

<p>Null maps solve the problem of null values coming back with the default value for the type. Remember what we said in the previous post and in the <em>Recap</em> above that null maps are two-dimensional arrays indicating whether a value is null or not, and that we have two null maps:</p>

<ul>
<li><strong><code>inputNullMap</code></strong>: for data passed in.</li>
<li><strong><code>outputNullMap</code></strong>: for data returned.</li>
</ul>

<p><strong>inputNullMap</strong></p>

<p>For input data we use the <code>inputNullMap</code>, or rather one of the Java extension components populates the map, and we use it in our code like so:</p>

<pre><code class="language-java">public static void bar() {
    
  int numRows = inputDataCol1.length;
  int numCols = inputNullMap.length;
  for(int x = 0; x &lt; numRows; x++) {
    for(int y = 0; y &lt; numCols; y++) {
      System.out.printf(&quot;Null map value at row: %d, column: %d, value: %b\n&quot;, 
                      x, y, inputNullMap[y][x]);  
    }
  }
}
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Input Data Null Map</em></p>

<p>We see in <em>Code Snippet 6</em> how we have a new method: <code>bar</code>, where we loop through the <code>inputNullMap</code> array. For each row, we loop the columns and print out the boolean value indicating if a column value is null or not.</p>

<p>Let us change the code in <em>Code Snippet 5</em> to call into the <code>bar</code> method:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'NullValues.bar'
, @input_data_1 = N'SELECT RowID, x, y 
                    FROM dbo.tb_NullRand10
                    WHERE RowID IN(3, 6, 8)';
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>SQL Pushing in Null Values - II</em></p>

<p>Apart from calling into the <code>bar</code> method in <em>Code Snippet 7</em> the other difference from <em>Code Snippet 5</em> is that we only push in rows which have null values, (to keep it short). The result we see after we execute looks like this:</p>

<p><img src="/images/posts/sql_2k19_java_null_null_map1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Null Map Output</em></p>

<p>We see, highlighted in yellow in <em>Figure 3</em>, how the null map indicates which columns have null values.</p>

<p>So how do we use this? Let us assume we have code which takes a dataset and multiplies two columns together (<code>x</code> and <code>y</code>) and returns the <code>RowID</code> and the result back to the caller:</p>

<pre><code class="language-java">public class NullValues {
  static public int[] inputDataCol1 = new int[1];
  static public int[] inputDataCol2 = new int[1];
  static public int[] inputDataCol3 = new int[1];
  static public boolean[][] inputNullMap = new boolean[1][1];

  static public int[] outputDataCol1;
  static public int[] outputDataCol2;
  static public boolean[][] outputNullMap;

  static public short numberOfOutputCols;

  public static void multiplier() {
    
    int numRows = inputDataCol1.length;
    
    outputDataCol1 = new int[numRows];
    outputDataCol2 = new int[numRows];
    
    for(int i = 0; i &lt; numRows; i++) {
       outputDataCol1[i] = inputDataCol1[i];
       outputDataCol2[i] = inputDataCol2[i] * inputDataCol3[i];

    }
    
    numberOfOutputCols = 2;
    outputNullMap = new boolean[numberOfOutputCols][numRows];

  }
  ...
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Multiplier</em></p>

<p>We see in <em>Code Snippet 8</em> how we:</p>

<ul>
<li>Declare two output column arrays for the two column output dataset.</li>
<li>Declare an output null map.</li>
<li>In the <code>multiplier</code> method initialize the output column arrays.</li>
</ul>

<p>We then loop through the input data and assigns the <code>RowID</code> (<code>inputDataCol1</code>) to <code>outputDataCol1</code>, and sets the value of <code>outputDataCol2</code> to be <code>inputDataCol2 * inputDataCol3</code>. If we at this stage compiled, moved the <code>.class</code> file to the <code>CLASSPATH</code> location, and executed as per <em>Code Snippet 5</em> (obviously edit the <code>@script</code> variable to be: <code>@script = N'NullValues.multiplier'</code>), this is the result:</p>

<p><img src="/images/posts/sql_2k19_java_null_multiplier1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Multiplier Output</em></p>

<p>What we see in <em>Figure 4</em> is how the rows with original null values came back with a result of 0 as, as we know from above, the Java components replace null with 0. However, this may not be what we want; instead we want the original null value columns not to be part of the result at all. So, to not include the original null value columns we use the input null map:</p>

<pre><code class="language-java">import java.util.ArrayList;
import java.util.stream.*;
import java.util.*;

public class NullValues {
  //static variables as in Code Snippet 8 ...

  public static void multiplier2() {
    
    int numRows = inputDataCol1.length;
    int numCols = inputNullMap.length;

    List&lt;Integer&gt; rowId = new ArrayList&lt;Integer&gt;();
    List&lt;Integer&gt; result = new ArrayList&lt;Integer&gt;();

    Boolean colIsNull;
    
    for(int i = 0; i &lt; numRows; i++) {
      colIsNull = false;
      for(int y = 0; y &lt; numCols; y++) {
        if(inputNullMap[y][i] == true) {
          colIsNull = true;
          break;
        }
      }
      if(!colIsNull) {
        rowId.add(inputDataCol1[i]);
        result.add(inputDataCol2[i] * inputDataCol3[i]);
      }
    }

    int numOutRows = rowId.size();
    outputDataCol1 = new int[numOutRows];
    outputDataCol2 = new int[numOutRows];

    for(int i = 0; i &lt; numOutRows; i++) {
      outputDataCol1[i] = rowId.get(i);
      outputDataCol2[i] = result.get(i);
    }
    numberOfOutputCols = 2;
    outputNullMap = new boolean[numberOfOutputCols][numOutRows];
  }
 
  // the other methods ...
} 
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Using Input Null Map for Output Data</em></p>

<p>So in <em>Code Snippet 9</em> we now see a couple of different things compared with <em>Code Snippet 8</em>:</p>

<ul>
<li>As we do not know upfront how many rows the method returns, we use Java <code>List</code>&rsquo;s to load output data into. The two lists are for the <code>RowID</code> and the result output columns.</li>
<li>We use the <code>inputNullMap</code> to check if any column in the row we process is null. If so we break immediately out, and we go to next row.</li>
<li>If the columns for the row is not null, we add the <code>RowID</code> column (<code>inputDataCol1</code>) to the row id list: <code>rowId</code>.</li>
<li>If none of the columns in the row are null we do the multiplication of <code>x</code> and <code>y</code>, (<code>inputDataCol2</code>, <code>inputDataCol3</code>), and add the result to the <code>result</code> list.</li>
<li>We initialize the two output data column arrays with the size of the <code>rowId</code> list.</li>
<li>Finally we loop over the data in the two lists and add it to the two output data column arrays.</li>
</ul>

<p>When we now execute the code in <em>Code Snippet 5</em>, (after compiling etc.), and having changed the @script parameter to: <code>@script = N'NullValues.multiplier2'</code>, the result is as so:</p>

<p><img src="/images/posts/sql_2k19_java_null_multiplier2.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Null Rows Not Included</em></p>

<p>We see in <em>Figure 5</em> how the rows with null values are excluded from the dataset, how cool is that?! However, what about if we want null values back, like the result of this code:</p>

<pre><code class="language-sql">DECLARE @x int = NULL;
DECLARE @y int = 21;

SELECT @x * @y;
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Multiplication with NULL</em></p>

<p>When we run the code in <em>Code Snippet 10</em> we get a result of <code>NULL</code>, and that is expected in SQL Server: any operations against a <code>NULL</code> value, yields <code>NULL</code>. To &ldquo;mimic&rdquo; that behaviour we use <code>outputNullMap</code>.</p>

<p><strong>outputNullMap</strong></p>

<p>The <code>outputNullMap</code> variable is like <code>inputNullMap</code> but the opposite. What do I mean with that: remember how the <code>inputNullMap</code> variable gets populated by the Java extension components, and read by the code we write. For the <code>outputNullMap,</code> our code populates the variable and the Java extension components read it.</p>

<p>To see how this works we incorporate some of the code in <em>Code Snippet 9</em>, with the code in <em>Code Snippet 8</em>, and we create a new method <code>multiplier3</code>:</p>

<pre><code class="language-java">public static void multiplier3() {
    
  int numRows = inputDataCol1.length;
  int numCols = inputNullMap.length;

  outputDataCol1 = new int[numRows];
  outputDataCol2 = new int[numRows];
  
  Boolean colIsNull;

  numberOfOutputCols = 2;
  outputNullMap = new boolean[numberOfOutputCols][numRows];

  for(int i = 0; i &lt; numRows; i++) {
    colIsNull = false;
    for(int y = 0; y &lt; numCols; y++) {
      if(inputNullMap[y][i] == true) {
        colIsNull = true;
        break;
      }
    }
    outputDataCol1[i] = inputDataCol1[i];
    outputDataCol2[i] = inputDataCol2[i] * inputDataCol3[i];
    outputNullMap[0][i] = false;
    outputNullMap[1][i] = colIsNull;
  }
}
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Output Null Values</em></p>

<p>Some things to notice in <em>Code Snippet 11</em>:</p>

<ul>
<li>We initialise the output column arrays to the same size as the input data. We no longer need to dynamically size the output arrays, as they will contain all data passed in.</li>
<li>When we loop the input rows, we also loop the <code>inputNullMap</code> and set the <code>colIsNull</code> variable to whatever the <code>inputNullMap</code> value is.</li>
<li>Regardless if <code>colIsNull</code> is <code>true</code> or <code>false</code> we do the multiplication. I know, we could skip this if we have a null, but I am lazy.</li>
<li>We set the value for the first column in <code>outputNullMap</code> to always be false as it represents the primary key of the data.</li>
<li>The <code>outputNullMap</code> value for the second column (the result) gets the value of <code>colIsNull</code>. In other words, if a column in the row is null, we set the result column in the <code>outputNullMap</code> to be null.</li>
</ul>

<p>After compiling etc., we execute the code in <em>Code Snippet 5</em>, pointing in the <code>@script</code> parameter to our new method. The result looks like so:</p>

<p><img src="/images/posts/sql_2k19_java_null_multiplier3.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Null Passed Back</em></p>

<p>In <em>Figure 6</em> we see null values in the result set, as we would expect in an SQL Server environment. The reason we see null values (instead of 0&rsquo;s) is that the Java extension components, as I mentioned before, reads the data returned together with the null map, and creates a SQL Server result set, including null values.</p>

<h4 id="string-and-null-maps">String and Null Maps</h4>

<p>Ok, we have now seen how we use the <code>inputNullMap</code> and <code>outputNullMap</code> to handle null values, and we know we need to use the null maps for primitive types. However, what about strings, as a string is not a primitive type, and is allowed to be null in Java. Let us have a look.</p>

<blockquote>
<p><strong>NOTICE:</strong> When we push string data from SQL Server, the SQL data type has to be either <code>nchar</code> or <code>nvarchar</code>.</p>
</blockquote>

<p>We create a new class, (in its own source file), in which we expect a two column data set passed in, and a two column data set returned. We also create a method which looks more or less the same as the <code>bar</code> method in <em>Code Snippet 6</em>:</p>

<pre><code class="language-java">public class NullStringValues {
  static public int[] inputDataCol1 = new int[1];
  static public String[] inputDataCol2 = new String[1];
  static public boolean[][] inputNullMap = new boolean[1][1];

  static public int[] outputDataCol1;
  static public String[] outputDataCol2;
  static public boolean[][] outputNullMap;
  
  static public short numberOfOutputCols;

  public static void bar() {
    
    
    int numRows = inputDataCol1.length;
    int numCols = inputNullMap.length;
    for(int x = 0; x &lt; numRows; x++) {
      System.out.printf(&quot;RowID: %d\t, StringValue: %s\t, 
                         NullMapValueStringCol: %b\n&quot;, 
                        inputDataCol1[x],
                        inputDataCol2[x], 
                        inputNullMap[1][x]);

   }
  }
}
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>String and Input Null Map</em></p>

<p>The code in <em>Code Snippet 12</em> should not come as a surprise. We see how we:</p>

<ul>
<li>Have two input column arrays of which one is a <code>String</code> array.</li>
<li>Also have two output column arrays of which one is a <code>String</code> array.</li>
<li>In the <code>bar</code> method loops the rows, and prints out the column values as well as the value of the <code>inputNullMap</code> for the string column.</li>
</ul>

<p>We compile the code in <em>Code Snippet 12</em> and move the <code>.class</code> file to the <code>CLASSPATH</code> location. We then use SQL code like this:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'NullStringValues.bar'
, @input_data_1 = N'SELECT RowID, Col1 
                    FROM dbo.tb_NullRand10 
                    WHERE RowID IN(4, 5, 8, 9, 10)';
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>SQL Code for String Input</em></p>

<p>The code in <em>Code Snippet 13</em> points to the <code>bar</code> method in the <code>NullStringValues</code> class, and it pushes in five rows of which two have a null value in the string column (<code>Col1</code>). The result when we execute the code in <em>Code Snippet 13</em> is:</p>

<p><img src="/images/posts/sql_2k19_java_null_string_null_input.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>String Nulls Passed In</em></p>

<p>We see in <em>Figure 7</em> how we receive null values for the rows where the column was null. However, we also see that the null map value for those columns is set to true. So even for strings, it looks like the null map is used, at least for input data.</p>

<p>So, what about output data. Let us create a new method which populates the output column arrays with some random data:</p>

<pre><code class="language-java"> public static void foo() {
  
    int numOutRows = 3;
    outputDataCol1 = new int[numOutRows];
    outputDataCol2 = new String[numOutRows];

    numberOfOutputCols = 2;
    
    outputNullMap = new boolean[numberOfOutputCols][numOutRows];

    for(int n = 0; n &lt; numOutRows; n++) {
      outputDataCol1[n] = n+1;
      outputNullMap[0][n] = false;
      if(n == 1) {
        outputDataCol2[n] = null;
        outputNullMap[1][n] = true;
      }
      else {
        outputDataCol2[n] = &quot;Hello&quot;;
        outputNullMap[1][n] = false;
      }
      
    }
  }
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>Java Code for String Null Output</em></p>

<p>We see in <em>Code Snippet 14</em> how we output three rows with two columns, where the second column is a string. In the second row, the string column is set to <code>null</code>, and the <code>outputNullMap</code> is also set to <code>null</code> for that column. After compilation, etc. we use following SQL code:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'NullStringValues.foo';
</code></pre>

<p><strong>Code Snippet 15:</strong> <em>SQL Code for String Output</em></p>

<p>The code runs fine when we execute, and the result looks like so:</p>

<p><img src="/images/posts/sql_2k19_java_null_string_null_output.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>String Nulls Passed Out</em></p>

<p>As expected we get a null value back in the outlined row in <em>Figure 8</em>. This does not tell us though whether the <code>outputNullMap</code> is required for strings. Let us comment out all code inside the <code>for</code> loop that references the <code>outputNullMap</code>. Keep the initialization of the null map as is (<code>outputNullMap = new boolean[numberOfOutputCols][numOutRows];</code>), compile, etc. Remember that for output data sets with no nulls we only need to initialise the <code>outputNullMap</code>. The theory is that for non-primitive data types we do not need to set the null map.</p>

<p>When we now execute the code in <em>Code Snippet 15</em> we get an error:</p>

<p><img src="/images/posts/sql_2k19_java_null_string_null_output_error1.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>String Nulls without Output Null Map</em></p>

<p>Ok, so the theory above does not seem correct based on the exception we see in <em>Figure 9</em>. So what about if the column is null, but the <code>outputNullMap</code> says <code>false</code> (not null):</p>

<pre><code class="language-java">if(n == 1) {
  outputDataCol2[n] = null;
  outputNullMap[1][n] = false;
}
</code></pre>

<p><strong>Code Snippet 16:</strong> <em>String Value Null Null Map False</em></p>

<p>When we execute after having compiled the code after the change in <em>Code Snippet 16</em>, we get the same exception as we see in <em>Figure 9</em>. Also, if we assign a value to the string column, but set the <code>outputNullMap</code> to be <code>true</code>:</p>

<pre><code class="language-java">if(n == 1) {
  outputDataCol2[n] = &quot;xxx&quot;;
  outputNullMap[1][n] = true;
}
</code></pre>

<p><strong>Code Snippet 17:</strong> <em>String Value Null Null Map False</em></p>

<p>When we execute the code in <em>Code Snippet 17</em> we get:</p>

<p><img src="/images/posts/sql_2k19_java_null_string_null_output_error2.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>String not Null with Output Null Map True</em></p>

<p>Hmm, that is interesting! We see in <em>Figure 10</em> how we do receive data back, but also an exception, and notice that the exception has a different <code>HRESULT</code> than the exception in <em>Figure 9</em>.</p>

<blockquote>
<p><strong>NOTICE:</strong> I do believe that Microsoft can do a better job with the errors returned from the Java extensions, as they at the moment are not very descriptive.</p>
</blockquote>

<p>We do not receive all rows, the row where the actual string value is not null but the null map says it is, is the last row. Oh, and the value for the column is what the null map say - <code>null</code>.</p>

<p>We now know that the null maps are needed for strings as well as primitive types. It also seems like the Java extensions are doing some validation when data returns from Java.</p>

<h2 id="summary">Summary</h2>

<p>Wow, this turned out to be quite a long blog post! So what have we found out:</p>

<p>When we deal with data passing in Java code we need two null maps:</p>

<ul>
<li><code>static public boolean[][] inputNullMap</code>: for input data.</li>
<li><code>static public boolean[][] outputNullMap</code>: for output data.</li>
</ul>

<p>You initialize the <code>inputNullMap</code>, and the Java extensions components populate the map, based on the data pushed in from SQL Server. You initialise and populate the <code>outputNullMap</code>, and the Java extensions components read the map and create the resultset returned to SQL Server. Even for string null values you need to populate the <code>outputNullMap</code>.</p>

<p>In your code, you can read the <code>inputNullMap</code> to decide how to process the data pushed in.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 50 &amp; Year End!]]></title>
    <link href="http://nielsberglund.com/2018/12/16/interesting-stuff---week-50--year-end/" rel="alternate" type="text/html"/>
    <updated>2018-12-16T18:45:34+02:00</updated>
    <id>http://nielsberglund.com/2018/12/16/interesting-stuff---week-50--year-end/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<blockquote>
<p><strong>NOTE:</strong> <em>I started with these roundup posts in the beginning of 2017 as a way to force myself to write, and - so far, so good. It is now coming up on Christmas and New Year, and I will take a break with these posts and come back in the beginning of next year.</em></p>
</blockquote>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/netflix-play-api">Netflix Play API - An Evolutionary Architecture</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation which deeps dive into how Netflix used a set of three core foundational principles to iteratively develop their architecture. This led to a list of practices to create an Evolutionary Architecture.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/net-mocking-framework">Mocking .NET without Hurting Its Feelings</a>. An [InfoQ] presentation, discussing two main types of mocking frameworks: constrained frameworks (like RhinoMocks and Moq) and unconstrained frameworks (such as Typemock Isolator and Telerik JustMock).</li>
<li><a href="https://mattwarren.org/2018/12/13/Exploring-the-.NET-Core-Runtime/">Exploring the .NET Core Runtime (in which I set myself a challenge)</a>. This is another awesome blog post by <a href="https://twitter.com/matthewwarren">Matthew</a>. In this post, he looks at the inner workings of <strong>.NET Core</strong>. I have said it before, and I say it again; if you have the vaguest interest in .NET/CLR Matthews [blog] is a must!</li>
<li><a href="https://natemcmaster.com/blog/2017/12/21/netcore-primitives/">Deep-dive into .NET Core primitives: deps.json, runtimeconfig.json, and dll&rsquo;s</a>. The post by Matthew above lead me to this post by <a href="https://twitter.com/natemcmaster">Nate</a>. The post is a deep dive into the &ldquo;plumbing&rdquo; of <strong>.NET Core</strong>. A must read!</li>
<li><a href="https://www.infoq.com/articles/netcore-devops">.NET Core and DevOps</a>. An <a href="https://www.infoq.com/">InfoQ</a> article which discusses how .NET Core was designed with DevOps in mind, and how .NET Core projects can benefit from the build automation and application monitoring intrinsic to the platform. The article also shows how the command-line accessibility of .NET Core makes DevOps easier to implement.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/">Azure Functions now supported as a step in Azure Data Factory pipelines</a>. A blog post which shows how easy it is to set up Azure Functions to run as steps in an Azure Data Factory pipeline.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.infoq.com/articles/machine-learning-learn-devops">What Machine Learning Can Learn From DevOps</a>. This <a href="https://www.infoq.com/">InfoQ</a> article makes a strong case that Machine Learning can gain many benefits from DevOps: culture change to support experimentation, continuous evaluation, sharing, abstraction layers, observability, and working in products and services.</li>
<li><a href="https://blog.revolutionanalytics.com/2018/12/azurecontainers.html">How to deploy a predictive service to Kubernetes with R and the AzureContainers package</a>. This blog post from the crew at <a href="http://blog.revolutionanalytics.com">Revolution Analytics</a> shows how you can deploy an R fitted model as a Plumber web service in Kubernetes, using Azure Container Registry (ACR) and Azure Kubernetes Service (AKS).</li>
<li><a href="https://blog.revolutionanalytics.com/2018/12/ten-years-of-revolutions.html">Reflections on the 10th anniversary of the Revolutions blog</a>. Oh, and while we are on the subject <a href="http://blog.revolutionanalytics.com">Revolution Analytics</a>, a huge Happy 10:th Anniversary to you!</li>
<li><a href="https://eng.uber.com/billion-data-point-challenge">The Billion Data Point Challenge: Building a Query Engine for High Cardinality Time Series Data</a>. This blog post discusses the challenges Uber had when building their in-house metrics solution, and the query engine for that solution.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/news/2018/12/pinterest-kafka-scaling">Scaling Apache Kafka at Pinterest</a>. Apache Kafka is used at Pinterest for transporting data for real-time streaming applications, logging and visibility metrics for monitoring. The installation runs on over 2000 brokers and handles more than 800 million messages and 1.2 Petabytes per day. This <a href="https://www.infoq.com/">InfoQ</a> article talks about how  Pinterest scales Kafka.</li>
<li><a href="https://www.confluent.io/blog/deep-dive-ksql-deployment-options">Deep Dive into KSQL Deployment Options</a>. This blog post provides first a brief overview of Kafka Streams and its execution model, and then it discusses KSQL deployment options.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/12/10/announcing-sql-server-2019-community-technology-preview-2-2/">Announcing SQL Server 2019 community technology preview 2.2</a>. The title of this blog post says it all. CTP 2.2 of SQL Server 2019 is released. Go and get it!</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/12/10/availability-groups-on-kubernetes-in-sql-server-2019-preview/">Availability Groups on Kubernetes in SQL Server 2019 preview</a>. This post discusses the ability to deploy a SQL Server 2019 container with Availability Groups on Kubernetes.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>I am continuing my Java and SQL Server 2019 journey. I am very close to finishing a post about how to deal with null values in the SQL Server 2019 Java Extension. It is a continuation of these posts:</p>

<ul>
<li><a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>. In this post, I take a look at how to install and enable the SQL Server 2019 Java extensions. The post finishes with some very simple Java code which I execute from SQL Server.</li>
<li><a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">SQL Server 2019 Extensibility Framework &amp; Java - Passing Data</a>. In this post, I look at how we pass data back and forth between SQL Server and Java.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me. Seeing that holidays are coming up - have a Great Holiday Season!!</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 49]]></title>
    <link href="http://nielsberglund.com/2018/12/09/interesting-stuff---week-49/" rel="alternate" type="text/html"/>
    <updated>2018-12-09T07:18:55+02:00</updated>
    <id>http://nielsberglund.com/2018/12/09/interesting-stuff---week-49/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/s/story/lets-take-a-crack-at-understanding-distributed-consensus-dad23d0dc95">How Does Distributed Consensus Work?</a>. This is an excellent blog post discussing distributed computing and distributed consensus. I learned a lot for the post!</li>
<li><a href="https://cloudblogs.microsoft.com/opensource/2018/12/04/announcing-service-fabric-provider-bosh">Announcing the Service Fabric provider for Bosh</a>. This is an announcement from Microsoft how Service Fabric can now me deployed an managed by <a href="https://bosh.io/docs/">Bosh</a>. BOSH is a project that unifies release engineering, deployment, and lifecycle management of small and large-scale cloud software.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/12/04/announcing-net-core-3-preview-1-and-open-sourcing-windows-desktop-frameworks/">Announcing .NET Core 3 Preview 1 and Open Sourcing Windows Desktop Frameworks</a>. A blog post from Microsoft, announcing a preview of .NET Core 3. In .NET Core 3 Microsoft includes Windows Forms, WPF and WinUI, and they open-source them! Who&rsquo;d have thunk!</li>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/12/04/announcing-ml-net-0-8-machine-learning-for-net/">Announcing ML.NET 0.8 – Machine Learning for .NET</a>. Microsoft has just released version .8 of Machine Learning for .NET (ML.NET), and this blog post provides details about some of the new/improved functionality.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://data-artisans.com/blog/5-steps-flink-application-development">5 “baby” steps to develop a Flink application</a>. A short an sweet introduction how to get up and running with Apache Flink.</li>
<li><a href="https://www.confluent.io/blog/kafka-streams-ksql-minimum-privileges">Kafka Streams and KSQL with Minimum Privileges</a>. You should read this blog post if you are working with Kafka. It discusses security, and how to apply security with minimum privileges.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="http://sql-sasquatch.blogspot.com/2018/12/fun-with-sql-server-query-store-query.html">Fun with SQL Server Query Store, Query Plan &lsquo;StatisticsInfo&rsquo; XML nodes, and STATS_STREAM</a>. A very cool blog post by <a href="https://twitter.com/sqL_handLe">Lonny</a> where he has fun with SQL Server query related &ldquo;stuff&rdquo;.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/11/06/the-november-release-of-azure-data-studio-is-now-available/">The November release of Azure Data Studio is now available</a>. The title says it all. The blog post discusses new/improved functionality in the November release of Azure Data Studio. I have found myself quite like ADS, and are doing most of my &ldquo;stuff&rdquo; in ADS instead of in SSMS; they said you can&rsquo;t learn old dogs new tricks - hah!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/2018/12/03/learning-areas-for-sql-server-big-data-clusters/">Learning Areas for SQL Server Big Data Clusters</a>. A teaser from <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a>, where he talks about upcoming training and workshops related to <strong>SQL Server 2019 Big Data Clusters</strong>. He also lists some resources to start to get to understand all the new technologies in <strong>SQL Server 2019 Big Data Clusters</strong>.</li>
<li><a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">SQL Server 2019 Extensibility Framework &amp; Java - Passing Data</a>. I continue my journey with SQL Server 2019 Java extensions. In this post, I look at how we pass data back and forth between SQL Server and Java.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Extensibility Framework &amp; Java - Passing Data]]></title>
    <link href="http://nielsberglund.com/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/" rel="alternate" type="text/html"/>
    <updated>2018-12-08T19:52:11+02:00</updated>
    <id>http://nielsberglund.com/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/</id>
    <content type="html"><![CDATA[<p>This post is the second post about SQL Server 2019 Extensibility Framework and the Java language extensions. In the first post, <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>, we looked at how to install and enable the Java language extensions, and we also wrote some pretty basic Java code to ensure it all worked.</p>

<p>In this post we look at how we can pass data back and forth between SQL Server and Java.</p>

<p></p>

<h2 id="recap">Recap</h2>

<p>Let us just look back and refresh our memories about what we discussed in the previous <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">post</a>.</p>

<ul>
<li>On Windows you install and enable the Java language extensions when enabling the <em>Machine Learning Services</em> feature during setup or when adding features.</li>
<li>For CTP 2.1 the supported Java version on both Linux and Windows is 1.8.x.</li>
<li>Java does not get automatically installed on Windows when you install and enable the Java language extensions (on Linux it does). So unless 1.8.x is already installed you need to install it.</li>
<li>On Windows the <code>Path</code> environment variable needs to extend to the directory where the <code>jvm.dll</code> is located.</li>
<li>It is good practice to create an environment variable called <code>CLASSPATH</code> which indicates where your compiled Java code exists.</li>
<li>On Windows you need to give <code>Read</code> permissions on the <code>CLASSPATH</code> directory to <code>ALL APPLICATION PACKAGES</code>.</li>
<li>On Linux you need to give <code>Read</code> and <code>Execute</code> permissions on the on the <code>CLASSPATH</code> directory to the <code>mssql_satellite</code> user.</li>
</ul>

<p>So, when the Java extensions are enabled we can write and execute Java code:</p>

<ul>
<li>We execute Java code from SQL Server using <code>sp_execute_external_script</code>.</li>
<li>We define the class and method we call in the <code>@script</code> parameter.</li>
<li>All methods called from SQL Server need to be <code>public static</code>.</li>
<li>The static methods can not have a return type, they need to be <code>public static void</code>.</li>
<li>The methods must be parameterless.</li>
<li>No support for output parameters.</li>
<li>The code needs to contain &ldquo;magic&rdquo; <code>public static</code> class members.</li>
<li>One such member is <code>numberOfOutputCols</code> which is always required. It has to be declared as: <code>public static short numberOfOutputCols;</code>.</li>
<li>Even though the methods must be parameterless we can pass in parameters as class members.</li>
<li>We refer to the class member parameters in SQL with the same name as the Java names but appended with <code>@</code>.</li>
<li>We define the SQL parameters in <code>sp_execute_external_script</code>&rsquo;s <code>@params</code> parameter.</li>
<li>We add the SQL parameters as named parameters in <code>sp_execute_external_script</code>.</li>
</ul>

<p>Based on the points above we saw some Java code looking like so:</p>

<pre><code class="language-java">public class JavaTest1 {
  public static short numberOfOutputCols;
  public static int x;
  public static int y;

  public static void adder() {
    System.out.printf(&quot;The result of adding %d and %d = %d&quot;, x, y, x + y);   
  }

  public static void helloWorld() {
    System.out.print(&quot;Hello World from SQL Java&quot;);
  }

}
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Basic Java Code</em></p>

<p>In <em>Code Snippet 1</em> we see the required <code>numberOfOutputCols</code> class member, together with class members for the two parameters used in the <code>adder</code> method. We also see that we do not pass back the result of the <code>adder</code> method as there is no support for output parameters (in this post we cover how we can pass data back to SQL).</p>

<p>We call the code in <em>Code Snippet 1</em> from SQL like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'JavaTest1.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Execute Java from SQL</em></p>

<p>To call the code in <em>Code Snippet 1</em> I execute what we see in <em>Code Snippet 2</em>:</p>

<ul>
<li>I indicate that I want to call into Java: <code>@language = N'Java'</code>.</li>
<li>The method I want to call is set in the <code>@script</code> parameter.</li>
<li>The Java class members <code>x</code> and <code>y</code> is defined in the <code>@params</code> parameter as SQL parameters <code>@x</code>, and <code>@y</code>.</li>
<li>I have added the <code>@x</code>, and <code>@y</code> parameters as named parameters and assigned them some values.</li>
</ul>

<p>So, that was what we discussed in the previous post. Towards the end of this post, we revisit the code above.</p>

<h2 id="demo-data">Demo Data</h2>

<p>In today&rsquo;s post, we use some data from the database, so let us set up the necessary database, tables, and load data into the tables:</p>

<pre><code class="language-sql">USE master;
GO
SET NOCOUNT ON;
GO
DROP DATABASE IF EXISTS JavaTest;
GO
CREATE DATABASE JavaTest;
GO
USE JavaTest;
GO

DROP TABLE IF EXISTS dbo.tb_Rand100
CREATE TABLE dbo.tb_Rand100(RowID int identity primary key, y int, 
                          rand1 int, rand2 int);

INSERT INTO dbo.tb_Rand100(y, rand1, rand2)
SELECT TOP(100) CAST(ABS(CHECKSUM(NEWID())) % 14 AS int) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS int)
  , CAST(ABS(CHECKSUM(NEWID())) % 25 AS int)
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create Database Objects</em></p>

<p>We see from <em>Code Snippet 3</em> how we:</p>

<ul>
<li>Create a database: <code>JavaTest</code>.</li>
<li>Create a table: <code>dbo.tb_Rand100</code>.</li>
<li>Insert some data into the table.</li>
</ul>

<p>The data we insert is entirely random, but it gives us something to &ldquo;play around&rdquo; with. Now, when we have a database and some data let us get started.</p>

<h2 id="data-passing-in-r-python">Data Passing in R/Python</h2>

<p>We start by looking at how we pass data when we use R/Python in <strong>SQL Server Machine Learning Services</strong>. In <a href="/2018/03/07/microsoft-sql-server-r-services---sp_execute_external_script---i/">Microsoft SQL Server R Services: sp_execute_external_script - I</a> I discussed, among other things, how we use named parameters to refer to data pushed into the external engine from SQL Server as well as data returned to SQL Server.</p>

<p>The parameter&rsquo;s default names are <code>InputDataSet</code> and <code>OutputDataSet</code>, and a simple example looks like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
      @language = N'Python'
    , @script = N'OutputDataSet = InputDataSet'
    , @input_data_1 = N'SELECT RowID, y, rand1 
                        FROM dbo.tb_Rand100';  
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Passing Data In and Out</em></p>

<p>The Python code in <em>Code Snippet 4</em> pushes data into the Python engine via <code>InputDataSet</code>, and <code>OutputDataSet</code> echoes the data back to SQL Server. Why this works is because R/Python are aware of these parameters, thanks to &ldquo;helper&rdquo; components, (<code>SqlSatellite.dll</code> and friends), shipped together and &ldquo;baked&rdquo; into R/Python. In essence, there is a very tight integration with the external runtime which makes this possible.</p>

<h2 id="data-passing-in-java-extensions">Data Passing in Java Extensions</h2>

<p>In Java, there are also helper components, (a topic for future posts), but the integration is not as tight, so when we want to pass data into and out of Java we need to code somewhat more explicit to make data passing possible.</p>

<p>In our Java code, we need to represent the data passed in and out as class member column arrays. You define in your classes, one array per column passed in, and one array per column returned. These column arrays are some of the &ldquo;magic&rdquo; members I mentioned above, and they are the equivalent to <code>InputDataSet</code>, and <code>OutputDataSet</code>.</p>

<p>The components that are part of the Java extension need to know about these members as the components either populate them when pushing data into Java or read from them when returning data from Java. The way that the components know about the members is based on a naming standard.</p>

<h4 id="pushing-data-into-java">Pushing Data into Java</h4>

<p>When pushing data into Java and using that data, we need to define two <code>public static</code> class members:</p>

<ul>
<li><strong><code>inputDataColN</code></strong>: array variable representing the input columns, where <em>N</em> is the column number (1 based).</li>
<li><strong><code>inputNullMap</code></strong>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
</ul>

<p>The calling components populate the variables for input data automatically, and you just need to initialize the arrays with a size greater than 0. Let us look at an example based on the <code>SELECT</code> statement in <em>Code Snippet 4</em>, where we want to push data into Java:</p>

<pre><code class="language-java">public class DataPassing {
  static public int[] inputDataCol1 = new int[1];
  static public int[] inputDataCol2 = new int[1];
  static public int[] inputDataCol3 = new int[1];

  static public boolean[][] inputNullMap = new boolean[1][1];

  static public short numberOfOutputCols;

  public static void foo() {
    
    for(int x = 0; x &lt; inputDataCol1.length; x++) {
      System.out.printf(&quot;Row %d:\t\t%d\t %d\t %d\n&quot;, x+1, 
                          inputDataCol1[x], 
                          inputDataCol2[x], 
                          inputDataCol3[x]);
    }
  }
}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Input Data</em></p>

<p>In <em>Code Snippet 5</em> we see how we:</p>

<ul>
<li>Declare and initialize three <code>int</code> arrays representing the three columns we expect from the <code>SELECT</code> statement.</li>
<li>Declare and initialize a <code>boolean</code> array for null mapping.</li>
<li>Declare the <code>numberOfOutputCols</code> variable which always is required.</li>
</ul>

<p>In the <code>foo</code> method we loop the three input arrays and print them out. We see that we do not have to populate the arrays as something else, (some component), does that for us. To make the code callable from SQL Server we:</p>

<ul>
<li>Copy the code in <em>Code Snippet 5</em> to a file: <code>DataPassing.java</code>.</li>
<li>We compile it like so: <code>javac DataPassing.java</code>, which results in a <code>.class</code> file: <code>DataPassing.class</code></li>
<li>We copy the <code>.class</code> file to the location of <code>CLASSPATH</code>.</li>
</ul>

<p>With the <code>.class</code> file in the <code>CLASSPATH</code> location we call it from SQL Server:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'DataPassing.foo'
, @input_data_1 = N'SELECT TOP(10) RowID, y, rand1 
                    FROM dbo.tb_Rand100';
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Execute &amp; Push Data to Java</em></p>

<p>The code in <em>Code Snippet 6</em> follows what we discussed in <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>, and the summary section above. When we execute it we see this:</p>

<p><img src="/images/posts/sql_2k19_java_intro_exec_input.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Result of Exec</em></p>

<p>In <em>Figure 1</em> we see the result of the <code>printf</code> statement in <em>Code Snippet 5</em>. Printing data to the console is probably not what we want to do in a &ldquo;proper&rdquo; application, so let us see how we can return data from Java back to SQL.</p>

<h4 id="returning-data-from-java">Returning Data from Java</h4>

<p>When returning data from our Java method(s), we use column array class members, similar to the ones we use when passing data into Java:</p>

<ul>
<li><strong><code>outputDataColN</code></strong>: array variable representing the output columns, where <em>N</em> is the column number (1 based).</li>
<li><strong><code>numberofOutputCols</code></strong>: we discussed this variable in the summary section above. It represents the number of columns returned from Java, and it is always required - regardless if you return columns or not.</li>
<li><strong><code>outputNullMap</code></strong>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
</ul>

<p>There are a couple of differences between the variables used for input data and the above output variables:</p>

<ul>
<li>You need to populate the variables yourself.</li>
<li>You initialize the variables right before you use them.</li>
</ul>

<p>So, if we wanted to have a method, <code>bar</code>, &ldquo;echoing&rdquo; back the input dataset, similar to what we see in *Code Snippet, 4 the code looks like so:</p>

<pre><code class="language-java">public class DataPassing {
  //input data variables as per above
  ...

  //output variables
  static public int[] outputDataCol1;
  static public int[] outputDataCol2;
  static public int[] outputDataCol3;
  static public boolean[][] outputNullMap;

  static public short numberOfOutputCols;

  public static void bar() {
    
    int numRows = inputDataCol1.length;
    numberOfOutputCols = 3;

    outputDataCol1 = new int[numRows];
    outputDataCol2 = new int[numRows];
    outputDataCol3 = new int[numRows];

    for(int x = 0; x &lt; numRows; x++) {
      outputDataCol1[x] = inputDataCol1[x];
      outputDataCol2[x] = inputDataCol2[x];
      outputDataCol3[x] = inputDataCol3[x];
    }

    outputNullMap = new boolean[numberOfOutputCols][numRows];
  }

 public static void foo() {...}

</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Output Data</em></p>

<p>In <em>Code Snippet 7</em> we see how I:</p>

<ul>
<li>Declare the output variables as class members.</li>
<li>Set the <code>numberOfOutputCols</code> variable. I do not get any data back without setting this variable,</li>
<li>Instantiate output variables in the method, based on the number of columns and number of rows.</li>
<li>Copy the input dataset into the output variables.</li>
<li>Instantiate the null map variable.</li>
</ul>

<p>After I have compiled the code and moved the <code>.class</code> file to the <code>CLASSPATH</code> location, I execute as in <em>Code Snippet 6</em>, but with one difference: the <code>@script</code> parameter now points to the <code>bar</code> method: <code>@script = N'DataPassing.bar'</code>:</p>

<p><img src="/images/posts/sql_2k19_java_intro_exec_output.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Output Dataset</em></p>

<p>We see in <em>Figure 2</em> how the data is returned to SQL Server and presented as a result set. If you compare what you see in <em>Figure 2</em> with what you see in <em>Figure 1</em>, you see the data is identical.</p>

<p>Cool, we have now seen how we pass datasets in and out of Java and how we use the different class arrays. Actually, that is not entirely true; we have not discussed the null map variables at all, other than saying what they are and that they need to be instantiated. We answer the questions about how and why to use them in the next post.</p>

<h2 id="input-output-parameters">Input &amp; Output Parameters</h2>

<p>In the previous post, (and in the recap above), we said that Java methods we use from SQL Server cannot have parameters and they must be <code>void</code>. In the recap section, we saw how we tried to work around those restrictions by having class member variables, etc.</p>

<p>With what we now know, we can re-write the Java code to something like so:</p>

<pre><code class="language-java">public class Calculator {
  static public int[] inputDataCol1 = new int[1];
  static public int[] inputDataCol2 = new int[1];
  static public boolean[][] inputNullMap = new boolean[1][1];

  static public int[] outputDataCol1;
  static public boolean[][] outputNullMap;

  static public short numberOfOutputCols;

  public static void adder() {
    
    int x = inputDataCol1[0];
    int y = inputDataCol2[0];

    numberOfOutputCols = 1;

    outputDataCol1 = new int[1];
    outputDataCol1[0] = x + y;

    outputNullMap = new boolean[1][1];
  }
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>New Adder</em></p>

<p>In this new code we see in <em>Code Snippet 8</em> we:</p>

<ul>
<li>Receive a one row, two column data set in the class.</li>
<li>Parse out the two columns, as <code>x</code>, and <code>y</code>, from the data coming in.</li>
<li>Return a one row, one column dataset back to SQL Server with the result of <code>x+y</code>.</li>
</ul>

<p>The SQL code to call into this Java code looks like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'Calculator.adder'
, @input_data_1 = N'SELECT 21, 21'; 
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Execute the Adder Method</em></p>

<p>The result returned when we execute the code in <em>Code Snippet 9</em> is as follows:</p>

<p><img src="/images/posts/ql_2k19_java_intro_exec_output_param.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Output Parameter</em></p>

<p>In this section, we saw how we can work around, (somewhat), the restrictions that exist at the moment when calling Java code from SQL Server.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we looked at how we pass data back and forth between SQL Server and Java. In the Java extensions we do not have the <code>InputDataSet</code>, and <code>OutputDataSet</code> variables, so we need to define class member arrays for the columns we send in and pass back out, as well as a variable indicating the number of columns we return:</p>

<ul>
<li>`<strong>inputDataCol*N</strong>*: array variable representing the input columns, where <em>N</em> is the column number (1 based).</li>
<li><code>**outputDataCol*N***</code>: array variable representing the output columns, where <em>N</em> is the column number (1 based).</li>
<li><code>**numberofOutputCols**</code>: it represents the number of columns returned from Java, and it is always required - regardless if you return columns or not.</li>
</ul>

<p>In addition to these variables we need two variables mapping null values:</p>

<ul>
<li><code>**inputNullMap**</code>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
<li><code>**outputNullMap**</code>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
</ul>

<p>All the <code>input*xxx*</code> variables get populated automatically, whereas you need to populate the <code>output*xxx*</code> variables in the code.</p>

<p>In the next post we discuss more about the null map variables.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 48]]></title>
    <link href="http://nielsberglund.com/2018/12/02/interesting-stuff---week-48/" rel="alternate" type="text/html"/>
    <updated>2018-12-02T14:43:12+02:00</updated>
    <id>http://nielsberglund.com/2018/12/02/interesting-stuff---week-48/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/considering-azure-functions-for-a-serverless-data-streaming-scenario/">Considering Azure Functions for a serverless data streaming scenario</a>. A blog post which describes a solution of using Azure server-less to create a data pipeline to detect fraudulent transactions. Quite a lot of exciting technologies/solutions!</li>
<li><a href="https://data-artisans.com/blog/getting-started-data-artisans-platform-azure-kubernetes-service">Getting Started with data Artisans Platform on Azure Kubernetes Service</a>. The <a href="https://data-artisans.com/download">dA Platform</a> is a production-ready stream processing infrastructure that includes open-source Apache Flink, and it is purpose-built for the stateful stream processing architecture. This blog post discusses how to get the dA Platform up and running on Azure Kubernetes Service.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://data-artisans.com/blog/apache-flink-1-7-0-release">What’s new in the latest Apache Flink 1.7.0 release</a>. As per the title. This blog post mentions some new functionality in the 1.7.0 release of Apache Flink. The new <code>MATCH_RECOGNIZE</code> functionality looks very juicy!</li>
<li><a href="https://databricks.com/blog/2018/11/30/apache-avro-as-a-built-in-data-source-in-apache-spark-2-4.html">Apache Avro as a Built-in Data Source in Apache Spark 2.4</a>. Starting from the Apache Spark 2.4 release, Spark provides built-in support for reading and writing Avro data. This blog post examines the functionality in detail.</li>
<li><a href="https://www.confluent.io/blog/3-ways-prepare-disaster-recovery-multi-datacenter-apache-kafka-deployments">3 Ways to Prepare for Disaster Recovery in Multi-Datacenter Apache Kafka Deployments</a>. What it says in the title - how we can prepare for disaster recovery in Kafka environments.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>. In the roundup for last <a href="/2018/11/25/interesting-stuff---week-47/">week</a> I wrote how I was working on a blog post about the new Java functionality in SQL Server 2019. This is it. In this post I take a look at how to install and enable the Java extensions. The post finishes with some very simple Java code which I execute from SQL Server. Do not be surprised if more posts comes shortly about Java.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Extensibility Framework &amp; Java - Hello World]]></title>
    <link href="http://nielsberglund.com/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/" rel="alternate" type="text/html"/>
    <updated>2018-12-02T09:36:34+02:00</updated>
    <id>http://nielsberglund.com/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/</id>
    <content type="html"><![CDATA[<p>The SQL Server 2016 release introduced the <strong>SQL Server Extensibility Framework</strong> (EF), which gives us the ability to, from inside SQL Server, execute code in an external language runtime environment. SQL Server 2016 supports R as external runtime, and Microsoft added Python to supported runtimes in the SQL Server 2017 release. The important part about the EF is that the runtime is outside of the core database engine, but we call it from inside SQL Server via the stored procedure <code>sp_execute_external_script</code>. We can push data from SQL Server queries to the external runtime, and consume data, (resultsets, output parameters) from the external runtime back in SQL Server.</p>

<blockquote>
<p><strong>NOTE:</strong> You can read more about the actual implementation of the external runtimes and <code>sp_execute_external_script</code> in my <a href="/sql_server_2k16_r_services/">SQL Server R Services Series</a> posts.</p>
</blockquote>

<p>In SQL Server 2019 Microsoft added the ability to execute custom Java code along the same lines we execute R and Python, and this blog post intends to give an introduction of how to install and enable the Java extension, as well as execute some very basic Java code. In future posts, I drill down how to pass data back and forth between SQL Server and Java.</p>

<p></p>

<p>There may very well be future posts discussing how the internals differ between Java and R/Python, but I want to talk about that a little bit in this post as well, as it has an impact on how we write and call Java code.</p>

<h2 id="r-python-basics">R/Python Basics</h2>

<p>In my <a href="/sql_server_2k16_r_services/">SQL Server R Services</a> series  talked about the components which make up <strong>SQL Server Machine Learning Services</strong>, and we saw how the flow when we execute an external script, looks something like so:</p>

<p><img src="/images/posts/sql_2k19_java_intro_flow1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Components &amp; Flow</em></p>

<p>We see in <em>Figure 1</em> what happens when we execute an R or Python script:</p>

<ul>
<li>Inside SQL Server we execute <code>sp_execute_external_script</code> and we send in the language as a parameter.</li>
<li>A connection happens to the <em>Launchpad</em> service and the service calls into an R or Python launcher (based on the <code>@language</code> parameter).</li>
<li>The launcher launches either the R or Python process.</li>
<li>Through a proprietary link dll, the <em>BxlServer</em> process gets launched.</li>
<li>In the process is the <code>SqlSatellite.dll</code> which handles data transfers.</li>
</ul>

<p>In the case of either R or Python, the execution of the script happens in the <em>BxlServer</em> process. A couple of things to keep in mind for later when we look at what we do when executing Java code:</p>

<ul>
<li>R/Python gets installed when we install SQL Server and choose In-Database machine learning. As R/Python is installed together with SQL Server, SQL gets implicit permissions to the R/Python exe&rsquo;s.</li>
<li>Even though we install open source versions of R/Python, they come together with proprietary dll&rsquo;s and exe&rsquo;s.</li>
<li>When we execute external scripts, we execute just that - scripts, and we pass in the code in the <code>sp_execute_external_script</code> call.</li>
</ul>

<p>Before we look at a how Java compares to the above, let us look at how we install and enable the Java extension.</p>

<h2 id="installing-java-extensions">Installing Java Extensions</h2>

<p>The way we install and enable Java on SQL Server differs depending on if you run SQL Server on Windows or Linux.</p>

<h4 id="windows">Windows</h4>

<p>You enable the ability to execute Java code, the same way as you do with R/Python. At install time, or when you add features, you check the <em>Machine Learning Services (In-Database)</em> checkbox:</p>

<p><img src="/images/posts/sql_2k19_java_intro_install1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Enable In-Database Machine Learning</em></p>

<p>In <em>Figure 2</em> we see how I have in the <em>Feature Selection</em> dialog checked the Machine Learning Services checkbox only. To enable Java, I do not need to choose either R or Python, the &ldquo;top-level&rdquo; <em>Machine Learning Services</em> is enough. Checking that, ensures that the required components, (<em>Launchpad</em>, and so on), gets installed.</p>

<blockquote>
<p><strong>NOTE:</strong> A future post covers the various components.</p>
</blockquote>

<p>When you install/add <em>Machine Learning Services</em> on Windows, you need to be aware that Java not gets installed. See below for more about that.</p>

<h4 id="linux">Linux</h4>

<p>When we install SQL Server on Linux, we do not have a &ldquo;nice&rdquo; UI, but we install via the shell. Similarly, when you want to install <em>Machine Learning Services</em> you do it via the shell. So to install only the Java extension you:</p>

<pre><code class="language-bash">sudo apt-get install mssql-server-extensibility-java
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Install Java Extension on Linux</em></p>

<p>In <em>Code Snippet 1</em> we see how I use the Ubuntu package manager to install the Java extension on an already installed <em>SQL Server on Linux</em> instance. Of course, you can install the instance as well as the Java extension in one go:</p>

<pre><code class="language-bash">sudo apt-get install mssql-server mssql-server-extensibility-java
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Install SQL Server and Java Extension Together</em></p>

<p>A difference between the install on Linux vs the Windows install is that on Linux Java 1.8.x gets installed if it is not already on the box.</p>

<h4 id="enabling-external-scripts">Enabling External Scripts</h4>

<p>On both Windows and Java we need to enable the execution of external scripts after installation:</p>

<pre><code class="language-sql">EXEC sp_configure 'external scripts enabled', 1
RECONFIGURE WITH OVERRIDE
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Enable External Scripts</em></p>

<p>You may remember that when we executed the code in <em>Code Snippet 3</em> in SQL Server 2016, and 2017 we had to restart the SQL Server instance afterwards to make the change &ldquo;stick&rdquo;. In SQL Server 2019 that is no longer necessary, (at least from CTP 2.1 and later), you just run the code and the setting changes!</p>

<h2 id="windows-and-java">Windows and Java</h2>

<p>When we run <code>mssql-server-extensibility-java</code> as in <em>Code Snippet 1</em> (or <em>Code Snippet 2</em>), I mentioned that Java 1.8.x gets installed if it is not already on the box. The installation also takes care of permissions as well as paths to the JVM. On Windows, this is not the case, so in this section, I talk a little bit about what you need to do if you are on Windows. More specifically we discuss Java versions, environment variables, and permissions.</p>

<blockquote>
<p><strong>NOTE:</strong> If you are on Linux have a look below where I mention <code>CLASSPATH.</code></p>
</blockquote>

<h4 id="java-version">Java Version</h4>

<p>As enabling <em>Machine Learning Services</em> does not install Java (on Windows), you need to ensure you already have a compatible version of Java on the box, or that you install one. On Windows, the suggestion for CTP 2.0 was version 1.10.x. However, Oracle no longer supports that Java version, so for CTP 2.1 the recommended version is 1.8.x, which is the same version recommended for SQL Server 2019 on Linux.</p>

<h4 id="environment-variables">Environment Variables</h4>

<p>When you do a typical installation of Java on Windows, you (or the installation program) also set some environment variables, so that applications can find the various Java dependencies:</p>

<ul>
<li>Set the <code>Path</code> environment variable to point to the <code>bin</code> directory of the JDK or JRE installation.</li>
<li>Create a <code>JAVA_HOME</code> environment variable pointing to the JDK/JRE top-level directory.</li>
</ul>

<p>When installing Java 1.8.x to use by <em>Machine Learning Services</em> you do not need the <code>JAVA_HOME</code> variable. However, other applications may need it, so I suggest you set it unless it is already set.</p>

<p>One crucial thing to be aware of is that the <code>Path</code> variable needs to be extended to include the directory where the <code>jvm.dll</code> exists. So, even if the <code>Path</code> includes the JDK/JRE <code>bin</code> directory, you need to edit the path to include the directory where the <code>jvm.dll</code> exists. The easiest way to do it (once again, this is Windows) is to use the UI: <strong>Control Panel-&gt;All Control Panel Items-&gt;System-&gt;Advanced System Setttings-&gt;Environment Variables-&gt;System Variables-&gt;Path-&gt;Edit</strong>:</p>

<p><img src="/images/posts/sql_2k19_java_intro_path.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Set Path</em></p>

<p>We see in <em>Figure 3</em> how I in the <em>Edit environment variable</em> dialog:</p>

<ul>
<li>Click <em>New</em> (outlined in blue).</li>
<li>Add the path in the text box (highlighted in red).</li>
</ul>

<p>I then click <em>Ok</em> in the various screens to exit out of there.</p>

<p>So far in this section, the discussion has been around things related to Java on Windows. The next thing I want to mention is as equally relevant for Windows as for Linux, and it is the Java <code>CLASSPATH</code> environment variable.</p>

<p>Remember what I mentioned above; when we use R or Python in <em>Machine Learning Services</em> we send in the script to execute as a parameter in the call. For Java, we execute against compiled code, and somehow we need to let the external engine know where the code is. That is where the <code>CLASSPATH</code> variable comes in. The <code>CLASSPATH</code> variable is there so that the Java Compiler and Java Runtime can find Java classes referenced in your program. It maintains a list of directories (containing many Java class files) and JAR file (a single-file archive of Java classes).</p>

<p>To create the <code>CLASSPATH</code> variable on Windows I use the UI: <strong>Control Panel-&gt;All Control Panel Items-&gt;System-&gt;Advanced System Setttings-&gt;Environment Variables-&gt;System Variables</strong>, and click <em>New</em> in the <em>Environment Variables</em> dialog:</p>

<p><img src="/images/posts/sql_2k19_java_intro_classpathpath.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Create CLASSPATH Windows</em></p>

<p>In <em>Figure 4</em>, after clicking <em>New</em>:</p>

<ul>
<li>I entered <code>CLASSPATH</code> in the <em>Variable name</em> text box (outlined in blue).</li>
<li>In the <em>Variable value</em> text box, (outlined in red), I entered the actual path where my compiled java code is located.</li>
</ul>

<p>If the <code>CLASSPATH</code> variable already exists, but you want your code in another location than where the variable points to, you can add a new path to the existing. In Windows, you delimit the paths with a semi-colon, and in Linux with a colon.</p>

<p>For completeness let us see how you create/set the <code>CLASSPATH</code> in Linux:</p>

<pre><code class="language-bash">export CLASSPATH=&quot;/path/to/directory:/path/to/directory2:$CLASSPATH&quot;
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Setting CLASSPATH in Linux</em></p>

<p>The code we see in <em>Code Snippet 4</em> adds two new directories to the existing <code>CLASSPATH</code> variable, delimited with colons.</p>

<blockquote>
<p><strong>NOTE:</strong> It is not absolutely necessary to have the <code>CLASSPATH</code> variable set. In a future post we see how we can do without it.</p>
</blockquote>

<h4 id="permissions">Permissions</h4>

<p>The last thing we need to do is to set permissions on the <code>CLASSPATH</code> directory.</p>

<blockquote>
<p><strong>NOTE:</strong> The following is based on what I found using CTP 2.1. This may change in future releases.</p>
</blockquote>

<p>On Windows, right click on the directory(s) in the <code>CLASSPATH</code>, followed by <strong>Properties-&gt;Security</strong>, and you see something like so:</p>

<p><img src="/images/posts/sql_2k19_java_intro_classpathpath_sec1.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Security Tab for CLASSPATH</em></p>

<p>In <em>Figure 5</em> we see how we are in the <em>Security</em> tab, and from here we can <strong>Edit-&gt;Add</strong> permissions, which gives us the ability to add permissions for users/groups:</p>

<p><img src="/images/posts/sql_2k19_java_intro_classpathpath_sec2.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Security Tab for CLASSPATH</em></p>

<p>The group we want to add permissions for is <code>ALL APPLICATION PACKAGES</code>, as we see in <em>Figure 6</em>. Clicking <em>OK</em> allows us to assign the permissions needed:</p>

<p><img src="/images/posts/sql_2k19_java_intro_classpathpath_sec3.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Security Tab for CLASSPATH</em></p>

<p>The required permission needed for <code>ALL APPLICATION PACKAGES</code> is <code>Read</code> as in <em>Figure 7</em>. After we click <em>OK</em> out of the dialogs, we are finally ready to write and execute code on Windows.</p>

<p>On Linux you need to give read and execute permissions on the <code>CLASSPATH</code> directory to the <code>mssql_satellite</code> user.</p>

<h2 id="java-code">Java Code</h2>

<p>There are certain things to think about when we write Java code which we want to call from SQL Server:</p>

<ul>
<li>We have no way, from calling from SQL Server to <code>new</code> &ldquo;up&rdquo; a class to get a reference to the class and call methods on that reference.</li>
<li>All methods called from SQL Server need to be <code>public static</code>. This is similar to SQLCLR. However, static methods can <code>new</code> &ldquo;up&rdquo; a class and call methods on the reference.</li>
<li>The static methods can not have a return type, they need to be <code>public static void</code>.</li>
<li>The methods must be parameterless!</li>
<li>No support for output parameters.</li>
</ul>

<p>The two first points above are probably not that too much of a hindrance, after all, this is the same as with SQLCLR. The last three, however, is more of a pain, and shortly we see how we work around the parameterless restriction.</p>

<p>So, let us write some code, and we start with &ldquo;Hello World&rdquo;:</p>

<pre><code class="language-java">public class JavaTest1{
  
  public static void helloWorld() {
    System.out.print(&quot;Hello World from SQL Java&quot;);
  }
}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Java Hello World - I</em></p>

<p>In <em>Code Snippet 5</em> we see your typical &ldquo;Hello World&rdquo; Java application. Copy the code into a file and name the file <code>JavaTest1.java</code>. Then compile it into a <code>class</code> file:</p>

<pre><code class="language-bash">javac JavaTest1.java
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Compile Java Code</em></p>

<p>After executing the code in <em>Code Snippet 6</em>, copy the <code>class</code> file to the location of your <code>CLASSPATH</code> variable. When the <code>class</code> file is in the <code>CLASSPATH</code> directory, we try and call it from SQL:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
                  @language = N'Java'
                , @script = N'JavaTest1.helloWorld'
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Execute Hello World</em></p>

<p>The only real difference we see in <em>Code Snippet 7</em> when executing Java code, compared to R/Python is that we call into the method we want to execute as opposed to send in the script.</p>

<blockquote>
<p><strong>NOTE:</strong> If you want a refresher about <code>sp_execute_external_script</code>, have a look at these three blog posts:</p>

<ul>
<li><a href="/2018/03/07/microsoft-sql-server-r-services---sp_execute_external_script---i/">Microsoft SQL Server R Services: sp_execute_external_script - I</a></li>
<li><a href="/2018/03/11/microsoft-sql-server-r-services---sp_execute_external_script---ii/">Microsoft SQL Server R Services - sp_execute_external_script - II</a></li>
<li><a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services: sp_execute_external_script - III</a></li>
</ul>
</blockquote>

<p>So, the code in <em>Code Snippet 7</em> looks very straightforward, but when we execute, the result is this:</p>

<p><img src="/images/posts/sql_2k19_java_intro_error1.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Execution Exception</em></p>

<p>Hmm, in <em>Figure 8</em> we see an exception about how we miss a member <code>numberOfOutputCols</code> in the class. What is this? So, in R/Python, there is a much &ldquo;tighter&rdquo; integration between SQL Server and the external runtime than between SQL Server and Java. Also, once again, with Java, we execute already compiled code whereas we in R/Python execute scripts. This all means that we don&rsquo;t have the same control over the runtime in Java, and we need to use &ldquo;magic&rdquo; variables to indicate to Java what we want to do. The <code>numberOfOutputCols</code> variable is such a variable, and the components that call into Java expects to find this variable. In this case, the variable is not there, so, therefore, the exception. However, take a look at the exception. Before the <code>Error: ...</code>, we do see the output from <code>System.out.print</code>, so all is not lost :).</p>

<blockquote>
<p><strong>NOTE:</strong> As I mentioned above, in future blog posts I cover the various components that make the Java integration work.</p>
</blockquote>

<h4 id="numberofoutputcols">numberOfOutputCols</h4>

<p>Above I said that <code>numberOfOutputCols</code> is a &ldquo;magic&rdquo; variable and that there are others as well. In future posts we cover the others, here we look at what <code>numberOfOutputCols</code> does.</p>

<p>First, all the &ldquo;magic&rdquo; variables have to do with passing of data between SQL Server and Java, and <code>numberOfOutputCols</code> indicates the number of columns you return to the calling code. In our code in <em>Code Snippet 5</em> we do not return anything, but the variable is still expected to be there. So, let us change the code slightly:</p>

<pre><code class="language-java">public class JavaTest1{
  public static short numberOfOutputCols;
  public static void helloWorld() {
    System.out.print(&quot;Hello World from SQL Java&quot;);
  }
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Java Hello World - II</em></p>

<p>In <em>Code Snippet 8</em> I added <code>numberOfOutputCols</code> as a class member. I recompile the code and copy the <code>class</code> file to <code>CLASSPATH</code>, and when I execute as in <em>Code Snippet 7</em>, all works! Yes, I have successfully executed my first Java application from inside SQL Server.</p>

<blockquote>
<p><strong>NOTE:</strong> The <code>numberOfOutputCols</code> variables has to be declared as <code>public static</code> and <code>short</code> as data type. Any other data type, and you get an exception.</p>
</blockquote>

<p>That was easy, what about something slightly more complicated, like a method expecting parameters?</p>

<h4 id="method-parameters">Method Parameters</h4>

<p>We want to add some basic calculator capabilities to our <code>JavaTest1</code> class because it is almost a law that after the obligatory &ldquo;Hello World&rdquo; application we need to write an adder method :). A typical Java <code>adder</code> method might look like so:</p>

<pre><code class="language-java">public static int adder(int x, int y) {
  return x + y;
}
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Java Adder</em></p>

<p>The problem with the code above, (if we want to use it from SQL Server), is twofold:</p>

<ul>
<li>It expects parameters to be passed in to the <code>adder</code> method.</li>
<li>It has a return type (<code>int</code>).</li>
</ul>

<p>So how do we turn the code in <em>Code Snippet 9</em> into something we can execute from SQL Server?</p>

<p>We need to figure out what to do with the parameters (<code>x</code> and <code>y</code>), and the return type. The parameters cannot be parameters in the method, but potentially variables in the class. As for the return type, for now, we just print the result:</p>

<pre><code class="language-java">public class JavaTest1 {
  public static short numberOfOutputCols;
  public static int x;
  public static int y;

  public static void adder() {
    System.out.printf(&quot;The result of adding %d and %d = %d&quot;, x, y, x + y);   
  }

  public static void helloWorld() {
    System.out.print(&quot;Hello World from SQL Java&quot;);
  }

}
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Java Adder Using Class Variables</em></p>

<p>The code in <em>Code Snippet 10</em>, looks like it should work, so recompile and place the compiled code in <code>CLASSPATH</code>. We can now execute, but the question is how we pass in the parameters, and how the Java code knows what they are?</p>

<p>In <a href="/2018/03/11/microsoft-sql-server-r-services---sp_execute_external_script---ii/">Microsoft SQL Server R Services - sp_execute_external_script - II</a> I wrote about parameters for scripts when we execute <code>sp_execute_external_script</code>:</p>

<ul>
<li>For parameters in R/Python scripts we declare them in SQL appending <code>@</code> to the R/Python name.</li>
<li>We define the SQL parameters in <code>sp_execute_external_script</code>&rsquo;s <code>@params</code> parameter.</li>
<li>We add the SQL parameters as named parameters in <code>sp_execute_external_script</code>.</li>
</ul>

<p>For Java code we do the same, so the SQL code looks something like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'JavaTest1.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Execute SQL Code with Parameters</em></p>

<p>As we see in <em>Code Snippet 11</em>, I have added the Java <code>x</code> and <code>y</code> class members to the <code>@params</code> parameter as SQL parameters: <code>@x</code> and <code>@y</code>. I then add the <code>@x</code> and <code>@y</code> parameters as named parameters, and assign them values from the declared parameters <code>@p1</code> and <code>@p2</code>. When I execute the code in <em>Code Snippet 11</em>, I see this:</p>

<p><img src="/images/posts/sql_2k19_java_intro_exec_params.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Execution with Parameters</em></p>

<p>As we see in <em>Figure 9</em> it works and, outlined in red, we get the result printed out. Happy Times!</p>

<h2 id="summary">Summary</h2>

<p>In this post we saw how we install and enable the Java extensions for SQL Server 2019:</p>

<ul>
<li>On Windows you install them via the <em>Machine Learning Services</em> feature during setup or when adding features.</li>
<li>In Linux you install by installing the package: <code>mssql-server-extensibility-java</code>.</li>
<li>Unless the <code>external scripts enabled</code> configuration option is already enabled you need to enable it as in <em>Code Snippet 3</em>.</li>
<li>The Java supported version on both Windows and Linux is version 1.8.x (at least for CTP 2.1).</li>
<li>On Linux, when you install the <code>mssql-server-extensibility-java</code> package, the correct Java version gets installed if it is not on the box already.</li>
<li>On Windows, you have to install the correct version manually if it is not installed.</li>
<li>On Windows, the <code>Path</code> system environment variable needs to be extended to include the directory where <code>jvm.dll</code> exists.</li>
<li>It is good practice to create an environment variable called <code>CLASSPATH</code> which indicates where your compiled Java code exists.</li>
<li>On Windows you need to give <code>Read</code> permissions on the <code>CLASSPATH</code> directory to <code>ALL APPLICATION PACKAGES</code>.</li>
<li>On Linux you need to give <code>Read</code> and <code>Execute</code> permissions on the on the <code>CLASSPATH</code> directory to the <code>mssql_satellite</code> user.</li>
</ul>

<p>So, when the Java extensions are enabled we can write and execute Java code:</p>

<ul>
<li>We execute Java code from SQL Server using <code>sp_execute_external_script</code>.</li>
<li>We define the class and method we call in the <code>@script</code> parameter.</li>
<li>All methods called from SQL Server need to be <code>public static</code>.</li>
<li>The static methods can not have a return type, they need to be <code>public static void</code>.</li>
<li>The methods must be parameterless.</li>
<li>No support for output parameters.</li>
<li>The code needs to contain &ldquo;magic&rdquo; <code>public static</code> class members.</li>
<li>One such member is <code>numberOfOutputCols</code> which is always required. It has to be declared as: <code>public static short numberOfOutputCols;</code>.</li>
<li>Even though the methods must be parameterless we can pass in parameters as class members.</li>
<li>We refer to the class member parameters in SQL with the same name as the Java names but appended with <code>@</code>.</li>
<li>We define the SQL parameters in <code>sp_execute_external_script</code>&rsquo;s <code>@params</code> parameter.</li>
<li>We add the SQL parameters as named parameters in <code>sp_execute_external_script</code>.</li>
</ul>

<p>So far, the code we have used is very simple. In the next post we look at how we can return data to the caller, and how we pass data sets around.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 47]]></title>
    <link href="http://nielsberglund.com/2018/11/25/interesting-stuff---week-47/" rel="alternate" type="text/html"/>
    <updated>2018-11-25T13:21:17+02:00</updated>
    <id>http://nielsberglund.com/2018/11/25/interesting-stuff---week-47/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.tbray.org/ongoing/When/201x/2018/11/18/Post-REST">Post-REST</a>. A very interesting post by [Tim Bray] where he looks at <em>What Comes after REST</em>/how REST will evolve. Very interesting!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/using-apache-kafka-drive-cutting-edge-machine-learning">Using Apache Kafka to Drive Cutting-Edge Machine Learning</a>. A very, very &ldquo;cool&rdquo; post, discussing how Kafka and Machine Learning fits together.</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/dean-of-big-data-driving-the-snakes-of-data-science-out-of">Dean of Big Data Driving the Snakes of Data Science Out of Ireland</a>. Well, I don&rsquo;t know about &ldquo;St. Paddy&rdquo; and <a href="https://news.nationalgeographic.com/news/2014/03/140315-saint-patricks-day-2014-snakes-ireland-nation/">snakes in Ireland</a>, but this post tries to do away with some Data Science myths (get rid of the snakes). Really worth reading!</li>
<li><a href="https://databricks.com/blog/2018/11/21/mlflow-v0-8-0-features-improved-experiment-ui-and-deployment-tools.html">MLflow v0.8.0 Features Improved Experiment UI and Deployment Tools</a>. A week or so ago, <a href="https://twitter.com/databricks">databricks</a> released <a href="https://www.mlflow.org/">MLFlow 0.8.0</a>. This blog post describes a couple of new features in that release.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>After I published the <a href="/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/">SQL Server 2019 Big Data Cluster on Azure Kubernetes Service</a> post, I went back and started to look at the new Java functionality in SQL Server 2019. There is a post forthcoming shortly (the famous last words).</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 46]]></title>
    <link href="http://nielsberglund.com/2018/11/18/interesting-stuff---week-46/" rel="alternate" type="text/html"/>
    <updated>2018-11-18T08:22:25+02:00</updated>
    <id>http://nielsberglund.com/2018/11/18/interesting-stuff---week-46/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/11/12/building-c-8-0/">Building C# 8.0</a>. A post about new functionality in the upcoming C# 8 release. Some very interesting new features, and it looks like C# gets more and more features that you normally find in F#.</li>
<li><a href="https://adamsitnik.com/Sample-Perf-Investigation/">Sample performance investigation using BenchmarkDotNet and PerfView</a>. In this blog post <a href="https://twitter.com/SitnikAdam">Adam</a> describes how he approaches sample performance problem using available free .NET tools and best practices for performance engineering.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://muratbuffalo.blogspot.com/2018/11/my-emacs-journey.html">My Emacs journey</a>. This blog post by <a href="https://twitter.com/muratdemirbas">Murat</a>, where he talks about Emacs, brings me back to the early days of .NET, (pre 1.0), where I created an Emacs .NET intellisense extension. Ah, those were the days!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/11/16/overload-control-for-scaling-wechat-microservices/">Overload control for scaling WeChat microservices</a>. In this blog post <a href="https://twitter.com/adriancolyer">Adrian</a> looks at a white paper discussing how <a href="https://www.wechat.com/en/">WeChat</a> handles overload control. Some very impressive numbers and quotes: <em>&ldquo;WeChat’s microservice system accommodates more than 3000 services running on over 20,000 machines in the WeChat business system, and these numbers keep increasing as WeChat is becoming immensely popular… As WeChat is ever actively evolving, its microservice system has been undergoing fast iteration of service updates. For instance, from March to May in 2018, WeChat’s microservice system experienced almost a thousand changes per day on average.&rdquo;</em>. Think about that; 20,000 machines, thousand code changes per day, wow!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://data-artisans.com/blog/stream-processing-introduction-event-time-apache-flink">Stream processing: An Introduction to Event Time in Apache Flink</a>. Apache Flink supports multiple notions of time for stateful stream processing. This post focuses on event time support in Apache Flink.</li>
<li><a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained">Kafka Connect Deep Dive – Converters and Serialization Explained</a>. In this article <a href="https://twitter.com/rmoff">Robin Moffat</a> takes us through how serialization works in Kafka Connectors.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/11/13/creating-a-data-hub-for-your-analytics-with-polybase/">Creating a data hub for your analytics with PolyBase</a>. SQL Server 2019 CTP 2.0 introduces new connectors for PolyBase, and this blog post discusses how the new connectors enable customers to leverage PolyBase for creating a virtual data hub for a wide variety of data sources within the enterprise. Very interesting!<br /></li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/the-ultimate-r-cheatsheet">The Ultimate R Cheatsheet</a>. As the title says, this post points to an impressive cheatsheet for R.</li>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/data-science-for-managers-mindmap">Intro to Data Science for Managers [Mindmap]</a>. Data science is incredibly broad and complex discipline and can be daunting trying to get your head around. So this blog post tries to mind-map the various key data science concepts and techniques in order to make data science easier to grasp.</li>
<li><a href="https://powerbi.microsoft.com/en-us/blog/power-bi-announces-new-ai-capabilities/">Announcing new AI Capabilities for Power BI to make AI Accessible for Everyone</a>. I guess the title of this blog posts says it all. It announces new AI capabilities for Power BI. This is huge, and I can not wait to test it out!</li>
<li><a href="https://www.infoq.com/presentations/uber-big-data-dl-ml">Big Data and Deep Learning: A Tale of Two Systems</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation about how Uber tackles data caching in large-scale deep learning, its ML architecture and discusses how Uber uses Big Data. The presentation concludes by sharing AI use cases.</li>
<li><a href="https://towardsdatascience.com/implementing-facebook-prophet-efficiently-c241305405a3">Implementing Facebook Prophet efficiently</a>. I wrote back in 2017 about Facebook Prophet and how to run it in <a href="/2017/05/20/facebook-prophet-and-microsoft-r-server/">Microsoft R Server</a> as well as <a href="/2017/05/20/facebook-prophet-and-microsoft-r-server/">SQL Server Machine Learning Services</a>. The blog post I link to <a href="https://towardsdatascience.com/implementing-facebook-prophet-efficiently-c241305405a3">here</a> discusses how to optimize Prophet. Very interesting!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 45]]></title>
    <link href="http://nielsberglund.com/2018/11/11/interesting-stuff---week-45/" rel="alternate" type="text/html"/>
    <updated>2018-11-11T08:05:58+02:00</updated>
    <id>http://nielsberglund.com/2018/11/11/interesting-stuff---week-45/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<p>##.NET</p>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/11/07/understanding-the-whys-whats-and-whens-of-valuetask/">Understanding the Whys, Whats, and Whens of ValueTask</a>. The .NET Framework 4 saw the introduction of the <code>System.Threading.Tasks</code> namespace, and with it the <code>Task</code> class. In this post, <a href="https://github.com/stephentoub">Stephen Toub</a>, covers the newer <code>ValueTask</code> and <code>ValueTask&lt;TResult&gt;</code> types.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://data-artisans.com/blog/flink-sql-powerful-querying-of-data-streams">Flink SQL for powerful querying of data streams and data at rest</a>. This post covers Flink SQL, and discusses how we can quickly explore data in streams or data at rest. Very interesting!<br /></li>
<li><a href="https://azure.microsoft.com/en-us/blog/announcing-the-general-availability-of-azure-event-hubs-for-apache-kafka/">Announcing the general availability of Azure Event Hubs for Apache Kafka</a>. This post announces the general availability (GA) of Azure Event Hubs for Apache Kafka. So what does it mean? Well, it means that you can now stream events from applications using the Kafka protocol directly into Azure Event Hubs. Awesome!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/container-runtimes-kubernetes">Who Is Running My Kubernetes Pod? The Past, Present, and Future of Container Runtimes</a>. Container runtime choices are nowadays not only Docker as the &ldquo;Open Container Initiative&rdquo; (OCI) has successfully standardized the concept of a container and container image to guarantee interoperability between runtimes. This <a href="https://www.infoq.com/">InfoQ</a> article looks at the past, present, and future of container engine implementations.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/sqlserverstorageengine/2018/11/07/introducing-scalar-udf-inlining/">Introducing Scalar UDF Inlining</a>. SQL Server 2017 introduced <a href="https://docs.microsoft.com/en-us/sql/relational-databases/performance/intelligent-query-processing?view=sql-server-2017">Intelligent Query Processing</a> which is meant to improve the performance of existing workloads with minimal implementation effort. SQL Server 2019 further expands query processing capabilities, and this blog post discusses Scalar T-SQL UDF Inlining. The inlining of scalar UDF&rsquo;s is a feature to improve the performance of queries that invoke scalar UDFs, where UDF execution is the main bottleneck.</li>
<li><a href="/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/">SQL Server 2019 Big Data Cluster on Azure Kubernetes Service</a>. This is a blog post by &ldquo;yours truly&rdquo;. As the title implies, it discusses how to deploy <strong>SQL Server 2019 Big Data Cluster</strong> to <strong>Azure Kubernetes Service</strong> (AKS).</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Big Data Cluster on Azure Kubernetes Service]]></title>
    <link href="http://nielsberglund.com/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/" rel="alternate" type="text/html"/>
    <updated>2018-11-10T07:58:09+02:00</updated>
    <id>http://nielsberglund.com/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/</id>
    <content type="html"><![CDATA[<p>At the <a href="https://www.microsoft.com/en-us/ignite/agenda"><strong>Microsoft Ignite 2018</strong></a> conference back in September Microsoft released <strong>SQL Server 2019</strong> for public preview, and I wrote two short blog posts about it:</p>

<ul>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a>.</li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a>.</li>
</ul>

<p>What Microsoft also announced was <strong>SQL Server 2019 Big Data Clusters</strong>, which combines the SQL Server database engine, Spark, and HDFS into a unified data platform! Yes, you read that right: SQL Server, Spark, and Hadoop right out of the box. Seeing that both Spark and Hadoop are mainly Linux based, what makes the Big Data Cluster possible is <strong>SQL Server on Linux</strong>. When you deploy a <strong>SQL Server 2019 Big Data Cluster</strong>, you deploy it as containers on <strong>Kubernetes</strong>, where the Kubernetes cluster can be in the cloud, such as <a href="https://azure.microsoft.com/en-us/services/kubernetes-service/"><strong>Azure Kubernetes Service</strong></a>, or on-prem like <a href="https://www.openshift.com/learn/topics/kubernetes/"><strong>Red Hat OpenShift</strong></a> or even on a local dev-box/laptop using <a href="https://kubernetes.io/docs/setup/minikube/"><strong>Minikube</strong></a>.</p>

<p>Initially, this post was about <strong>SQL Server 2019 Big Data Clusters</strong> on Minikube, but after quite a few failed installation attempts I realised I did not have enough memory on my development box, so I decided to try it on <strong>Azure Kubernetes Service</strong> (AKS) instead.</p>

<blockquote>
<p><strong>NOTE:</strong> If you want to run <strong>SQL Server 2019 Big Data Clusters</strong> on Minikube it is suggested that your host machine (Minikube is essentially a VM) has at least 32Gb of memory, and you allocate at least 22Gb to the Minikube VM.</p>
</blockquote>

<p>Since I am a complete novice when it comes to Kubernetes, this post covers both how I set up AKS as well as the deployment of <strong>SQL Server 2019 Big Data Clusters</strong> to AKS, and the post is somewhat a summary of the official <a href="https://docs.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?view=sqlallproducts-allversions">documentation</a>.</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> SQL Server 2019 is in public preview, but the preview does not contain the Big Data Cluster parts. To deploy <strong>SQL Server 2019 Big Data Clusters</strong> you need to be part of the SQL Server 2019 Early Adoption Program, for which you can sign up for <a href="https://sqlservervnexteap.azurewebsites.net/">here</a>.</p>
</blockquote>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>Apart from having an Azure subscription and being enrolled in the SQL Server 2019 EAP, there are a couple of other pre-reqs needed.</p>

<h4 id="python">Python</h4>

<p>If you, like me, are a SQL Server guy, you are probably quite familiar with installing SQL Server instances by mounting an ISO file, and running setup. Well, you can forget all that when you deploy a <strong>SQL Server 2019 Big Data Cluster</strong>. The setup is all done via Python utilities, and various Docker images pulled from a private repository. So, you need Python3. On my box I have Python 3.5, and - according to Microsoft - version 3.7 also works. Make you that you have your Python installation on the path.</p>

<p>When you deploy you use a Python utility: <code>mssqlctl</code>. To download <code>mssqlctl</code>, you need Python&rsquo;s package management system <code>pip</code> installed. During installation you also need a Python HTTP library: <em>Requests</em>. If you do not have it you need to install it:</p>

<pre><code class="language-python">python -m pip install requests
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Installing Python Requests</em></p>

<p>Down below we talk some more about how to download the <code>mssqlctl</code> utility.</p>

<h4 id="azure-cli">Azure CLI</h4>

<p>When working with Azure, you can do it in three ways:</p>

<ul>
<li>Azure Portal</li>
<li>Cloud Shell from within the portal.</li>
<li>Azure CLI.</li>
</ul>

<p>The Azure CLI is Microsoft&rsquo;s cross-platform command-line experience for managing Azure resources, and you install it on your local machine. In this post I mainly use the Azure CLI, so if you want to follow along download it from <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest">here</a>.</p>

<h4 id="kubectl">Kubectl</h4>

<p>The <code>kubernetes-cli</code> (Kubernetes command line tool), gives you an executable <code>kubectl.exe</code> which you use to manage your Kubernetes cluster. Using <code>kubectl</code>, you can inspect cluster resources; create, delete, and update components; etc.</p>

<p>You can install <code>kubectl</code> in different ways, and I installed it from <a href="https://chocolatey.org/packages/kubernetes-cli">Chocolatey</a>: <code>choco install kubernetes-cli</code>.</p>

<h2 id="azure-kubernetes-cluster">Azure Kubernetes Cluster</h2>

<p>Ok, so having &ldquo;sorted&rdquo; out the pre-reqs, let us start with creating an Azure Kubernetes cluster through the Azure CLI.</p>

<h4 id="login">Login</h4>

<p>I start with open Powershell as administrator and from Powershell I log in to Azure:</p>

<pre><code class="language-bash">az login
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Login to Azure</em></p>

<p>When I execute the code in <em>Code Snippet 2</em> a tab opens in my browser, and I see a dialog that asks me to pick an account to log in to Azure with:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Azure Login</em></p>

<p>I choose the account from what I see in <em>Figure 1</em>, and after a little while I see in the browser a success message:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login_success.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Azure Login Success</em></p>

<p>At the same time as the success message in <em>Figure 2</em>, the code in <em>Code Snippet 1</em> returns with information what subscriptions I have access to in Azure:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login_return.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Azure Login Return</em></p>

<p>As we see in <em>Figure 3</em>, I have access to multiple subscriptions, and before we go any further, I set the subscription I target. I look at the <code>id</code> for the subscription I want and:</p>

<pre><code class="language-bash">az account set -s &lt;my_subscription_id&gt;
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Set Azure Context</em></p>

<p>I have now logged in and indicated, as in <em>Code Snippet 3</em>, what subscription to use.</p>

<h4 id="resource-groups">Resource Groups</h4>

<p>Everything we do in Azure is in the context of a resource group. A resource group is a logical group in which Azure resources are deployed and managed, and it exists in a physical location (Azure data center). So when I create a Kubernetes cluster, I need to define what resource group the cluster should belong to. So let us create a resource group:</p>

<pre><code class="language-bash">az group create --name kubernetes-rg --location westeurope
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Create Resource Group</em></p>

<p>In <em>Code Snippet 4</em> we see how I create a group named <code>kubernetes-rg</code>, and I want it in the West Europe region. After I run the code in <em>Code Snippet 4</em>, I get back a JSON blob:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_create_rg.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Azure Create Group Return</em></p>

<p>The JSON blob, as in <em>Figure 4</em>, contains details about my newly created resource group. If I log in to the Azure Portal:</p>

<p><img src="/images/posts/sql_2k19_bdc_portal_resource_group.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Azure Resource Group</em></p>

<p>In the portal, when I click the <em>Resource groups</em> link (outlined in red in <em>Figure 5</em>), I see my newly created resource group outlined in blue.</p>

<h2 id="create-kubernetes-cluster">Create Kubernetes Cluster</h2>

<p>I now have a resource group, and I go on to create the Kubernetes cluster.</p>

<blockquote>
<p><strong>NOTE:</strong> You do not need to create a new resource group as such. When you create the Kubernetes cluster, you can create it in an existing group.</p>
</blockquote>

<p>To create the Kubernetes cluster I continue to use the Azure CLI, and I use the <code>az aks create</code> command. The command has quite a few parameters, which you can read about <a href="https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create">here</a>, but I use only a few of them:</p>

<ul>
<li><code>--name</code>: name of the cluster.</li>
<li><code>--resource-group</code>: name of the resource group.</li>
<li><code>--disable-rbac</code>: disables Kubernetes Role-Based Access Control.</li>
<li><code>--generate-ssh-keys</code>: if no SSH keys exist, generate both public and private key files.</li>
<li><code>--node-vm-size</code>: the size of the VM&rsquo;s used for the nodes in the Kubernetes cluster. For a <strong>SQL Server 2019 Big Data Cluster</strong> you need a VM with at least 32Gb of memory. You can see a list of VM sizes and their features in the portal <a href="https://portal.azure.com/#create/microsoft.aks">here</a>. I use &ldquo;Standard E4s_v3&rdquo;.</li>
<li><code>--node-count</code>: number of nodes in the Kubernetes node pool. I use 3.</li>
<li><code>--kubernetes-version</code>: the version of Kubernetes to use for creating the cluster. The <strong>SQL Server 2019 Big Data Cluster</strong> requires at minimum the Kubernetes v1.10 version.</li>
</ul>

<p>Before I create the cluster, let us talk a little bit about <code>--node-vm-size</code> and <code>--node-count</code> as they are somewhat related to each other. In addition to defining how much memory a VM has the <code>--node-vm-size</code> also defines the number of virtual CPU&rsquo;s (VCPUS) for the VM. The number of VCPUS controls how many data disks the VM has, (normally it is 2 disks per VCPU). The number of disks per VM is important as the <strong>SQL Server 2019 Big Data Cluster</strong> mounts quite a lot of storage on individual disks and with too few disks the mount operations fail. To get more disks you either increase the VM size or the node count, and that is the relation between <code>--node-vm-size</code> and <code>--node-count</code>.</p>

<p>For a &ldquo;default&rdquo; <strong>SQL Server 2019 Big Data Cluster</strong> deployment around 20 disks are required. So if I choose the &ldquo;Standard E4s_v3&rdquo; VM as vm size, I want at least 3 nodes. With this in mind the code to create a Kubernetes cluster looks like so:</p>

<pre><code class="language-bash">az aks create --name sqlkubecluster \
--resource-group kubernetes-rg \
--disable-rbac \
--generate-ssh-keys \
--node-vm-size Standard_E4s_v3 \
--node-count 3 \
--kubernetes-version 1.10.8
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Kubernetes Cluster</em></p>

<p>In <em>Code Snippet 5</em> we see how:</p>

<ul>
<li>I want to create a cluster with the name <code>sqlkubecluster</code>.</li>
<li>I want the cluster in the <code>kubernetes-rg</code> resource group.</li>
<li>I do not want to use Kubernetes Role-Based Access Control.</li>
<li>I want SSH keys created.</li>
<li>I want the VM&rsquo;s to be &ldquo;Standard E4s_v3&rdquo;,</li>
<li>I want 3 nodes.</li>
<li>Finally I want the Kubernetes version to be 1.10.8.</li>
</ul>

<p>When I execute the code I see something like so:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_create_cluster.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Run Create Kubernetes Cluster</em></p>

<p>What we see in <em>Figure 6</em> runs for several minutes and when it completes I receive a JSON blob with information about the cluster.</p>

<p>I mentioned above how <code>kubectl</code> is used to manage your Kubernetes cluster, and now when the cluster is created, we need to configure <code>kubectl</code> to connect to the cluster. To do this, you use the <code>az aks get-credentials</code> command like so:</p>

<pre><code class="language-bash">az aks get-credentials --resource-group=kubernetes-rg --name sqlkubecluster
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Get Credentials</em></p>

<p>We see in <em>Code Snippet 6</em> how I pass in the name of the resource group and cluster as parameters and when I execute:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_credentials.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Get Credentials</em></p>

<p>The <code>config</code> file we see outlined in red in <em>Figure 7</em> holds, among other things, the keys for the Kubernetes cluster. To ensure that I can connect to the cluster I call <code>kubectl get nodes</code>, and I see some information about the cluster nodes.</p>

<h4 id="dashboard-namespaces">Dashboard &amp; Namespaces</h4>

<p>To monitor and manage a Kubernetes cluster you do not have to rely on <code>kubectl</code> solely, as you can also use the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Kubernetes Dashboard</a>. In Azure you access the Dashboard by the <code>az aks browse</code> command, and - as with <code>get-credentials</code> - you pass in the names of the resource group and cluster: <code>az aks browse --resource-group kubernetes-rg --name sqlkubecluster</code>, and a new tab opens in your browser:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_dashboard.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Kubernetes Dashboard</em></p>

<p>In <em>Figure 8</em> we see the dashboard right after I created the VM and the Kubernetes cluster. Notice <em>namespaces</em>, outlined in red. <em>Namespcaces</em> help different projects, teams, or customers to share a Kubernetes cluster, and when you deploy to Kubernetes, you deploy into a namespace. To see what <em>namespaces</em> exist in a cluster you execute: <code>kubectl get namespaces</code>. When I do it at this stage I see:</p>

<p><img src="/images/posts/sql_2k19_bdc_kubectl_namespaces.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Kubernetes Namespaces</em></p>

<p>When we look at <em>Figure 9</em> we see following <em>namespaces</em>:</p>

<ul>
<li><code>default</code>: A default, (duh), namespace to hold the default set of Pods, Services, and Deployments used by the cluster.</li>
<li><code>kube-public</code>: A namespace readable by everyone for public <a href="https://unofficial-kubernetes.readthedocs.io/en/latest/tasks/configure-pod-container/configmap/">ConfigMap&rsquo;s</a>.</li>
<li><code>kube-system</code>: A namespace for objects created by the Kubernetes system.</li>
</ul>

<p>So, coming back to Dashboard: when we want to monitor a deployment with Dashboard, we monitor a specific namespace. Enough of this, let us deploy!</p>

<h2 id="deploy-sql-server-2019-big-data-cluster">Deploy SQL Server 2019 Big Data Cluster</h2>

<p>I mentioned above that when we deploy a <strong>SQL Server 2019 Big Data Cluster</strong> we deploy using a Python utility: <code>mssqlctl</code>. So what we need to do is to download and install the utility:</p>

<pre><code class="language-bash">pip3 install \
   --index-url https://private-repo.microsoft.com/python/ctp-2.0 \
     mssqlctl
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Download and Install mssqlctl</em></p>

<p>We download and install <code>mssqlctl</code> from a Microsoft repository as we see in <em>Code Snippet 7</em>. After download the source is located in <code>&lt;python_path&gt;\lib\site-packages</code> and the executable - <code>mssqlctl.exe</code> - is at: <code>&lt;python_path&gt;\Scripts</code>.</p>

<h4 id="environment-variables">Environment Variables</h4>

<p>When you deploy the <strong>SQL Server 2019 Big Data Cluster</strong> using <code>mssqlctl</code> you customise the cluster configuration via environment variables read by <code>mssqlctl</code>. To see all available environment variables you go <a href="https://docs.microsoft.com/en-us/sql/big-data-cluster/deployment-guidance?view=sqlallproducts-allversions">here</a>. Below I list the ones I use:</p>

<ul>
<li>SET ACCEPT_EULA=Y - to accept the SQL Server license agreement.</li>
<li>SET CLUSTER_PLATFORM=aks - the Kubernetes platform you deploy to: Azure - <code>aks</code>, Kubernetes - <code>kubernetes</code>, Minikube - <code>minikube</code>.</li>
<li>SET CONTROLLER_USERNAME=admin - the user name for the cluster administrator. You can set this to anything.</li>
<li>SET CONTROLLER_PASSWORD=<some_secret_password> - the password for the cluster administrator.</li>
<li>SET KNOX_PASSWORD=<some_secret_password> - the password for the Knox user. <a href="https://knox.apache.org/">Knox</a> is an application gateway for interacting with the REST API&rsquo;s and UI&rsquo;s of Apache Hadoop deployments.</li>
<li>SET MSSQL_SA_PASSWORD=<some_secret_password> - the <code>sa</code> password for the master SQL instance. It needs to meet password complexity requirements.</li>
<li>SET DOCKER_REGISTRY=private-repo.microsoft.com - the registry for the images being pulled.</li>
<li>SET DOCKER_REPOSITORY=mssql-private-preview - the repository within the registry.</li>
<li>SET DOCKER_USERNAME=<docker_username> - user name to access the images. You get this when you sign up for the <a href="https://sqlservervnexteap.azurewebsites.net/">EAP</a>.</li>
<li>SET DOCKER_PASSWORD=<some_secret_password> - the password for the above user. You get this when you sign up for the <a href="https://sqlservervnexteap.azurewebsites.net/">EAP</a>.</li>
<li>SET DOCKER_EMAIL=<email_for_the_docker_user> - the email associated with the registry. You get this when you sign up for the <a href="https://sqlservervnexteap.azurewebsites.net/">EAP</a>.</li>
<li>SET DOCKER_PRIVATE_REGISTRY=1 - this has to be set to 1.</li>
</ul>

<p>Before you deploy the environment variables, need to be set, and if you are on Windows, you need to do it from a command prompt (not Powershell). Instead of having to enter these variables individually, I have a <code>bat</code> file I run before deploying: <code>set_env_variables_aks.bat</code>.</p>

<h4 id="create-cluster">Create Cluster</h4>

<p>After I have set the variables I create the cluster with the <code>mssqlctl</code> command, and I do it from the command prompt (not Powershell):</p>

<pre><code class="language-bash">mssqlctl create cluster sqlbigdata1 -v
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Create Big Data Cluster</em></p>

<p>Looking at <em>Code Snippet 8</em> we see how I call <code>mssqlctl</code> to create a <strong>SQL Server 2019 Big Data Cluster</strong>, and I want to create it in a namespace called <code>sqlbigdata1</code>. I use the <code>-v</code> flag (as in verbose) to get debug output. When I execute the code I see something like so:</p>

<p><img src="/images/posts/sql_2k19_bdc_create_cluster.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Create Big Data Cluster</em></p>

<p>What we see in <em>Figure 10</em> is how we have started to create the main controller and its pod. We also see a note (outlined in red) saying that it can take quite a while to create the cluster. To monitor the process you can use Dashboard:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_dashboard2.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Kubernetes Dashboard</em></p>

<p>In <em>Figure 11</em> we see an overview of the <code>sqlbigdata1</code> namespace. You may see errors in the dashboard, but you can ignore them initially. In addition to Dashboard to monitor progress you can use <code>kubectl</code> commands, for example: <code>kubectl get pods -n sqlbigdata1</code>:</p>

<p><img src="/images/posts/sql_2k19_bdc_kubectl1.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Pods being Created</em></p>

<p>The <strong>SQL Server 2019 Big Data Cluster</strong> exposes its own dashboard; the <em>Cluster Administration Portal</em>, which we can use to monitor the deployment as well. The portal becomes available as soon as the controller is up, and in a running state. The portal is exposed at the endpoint for the <code>service-proxy-lb</code> (proxy load balancer) service. To find the IP address, you call: <code>kubectl get svc service-proxy-lb -n &lt;name of your cluster&gt;</code>:</p>

<p><img src="/images/posts/sql_2k19_bdc_cluster_admin_ip.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Endpoint for Cluster Admin</em></p>

<p>In <em>Figure 13</em> we see how <code>svc service-proxy-lb</code> has an external IP of <code>13.94.174.28</code>, and it exposes two ports: <code>30777</code> and <code>31826</code>. The port for the portal is <code>30777</code>, and when I browse there, I first need to log in with the <code>CONTROLLER_USERNAME</code> (admin in my case) and <code>CONTROLLER_PASSWORD</code>. After login, I come to the <em>Overview</em> page. I then click on the <em>Deployment</em> link outlined in red:</p>

<p><img src="/images/posts/sql_2k19_bdc_cluster_admin2.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Deployment Progress</em></p>

<p>What we see in <em>Figure 14</em> is the progress of the <strong>SQL Server 2019 Big Data Cluster</strong> deployment, and we see that it is still in progress: yellow triangle by the namespace, (outlined in blue).</p>

<p>Eventually, the deployment finishes, and we know that either: by seeing that the triangle in <em>Figure 14</em> is now a green circle, or by the output from command line:</p>

<pre><code class="language-bash">2018-11-07 09:04:52.0147 UTC | INFO | Data pool is ready...
2018-11-07 09:04:52.0148 UTC | INFO | Storage pool is ready...
...
2018-11-07 09:06:55.0073 UTC | INFO | Compute pool is ready...
...
2018-11-07 09:07:36.0155 UTC | INFO | Cluster state: Ready
2018-11-07 09:07:36.0155 UTC | INFO | Cluster deployed successfully.
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Cluster Deployed Successfully</em></p>

<p>We see in <em>Code Snippet 9</em> how <code>mssqlctl</code> reports that the various pools are ready, followed by successful cluster deployment.</p>

<h4 id="connection-endpoints">Connection Endpoints</h4>

<p>So far, so good - but what do we do now? We know that a <strong>SQL Server 2019 Big Data Cluster</strong> consists both of a SQL Server master instance, as well as Hadoop/Spark, but where do we find them?</p>

<p>As with the portal, the endpoints are service load balancers endpoints. The service load balancer for the SQL Server master instance is: <code>service-master-pool-lb</code> and for Hadoop/Spark it is: <code>service-security-lb</code>. To retrieve the endpoints I call:</p>

<pre><code class="language-bash"># SQL Server master instance
kubectl get service service-master-pool-lb \
                 -o=custom-columns=&quot;IP:.status.loadBalancer.\
                 ingress[0].ip,PORT:.spec.ports[0].port&quot; \
                 -n sqlbigdata1

# Hadoop/Spark
kubectl get service service-security-lb \
                 -o=custom-columns=&quot;IP:.status.loadBalancer.\
                 ingress[0].ip,PORT:.spec.ports[0].port&quot; \
                 -n sqlbigdata1
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Get Endpoints</em></p>

<p>In <em>Code Snippet 10</em> we see how I customize what I want to be returned from the <code>get service</code> calls to only to return IP addresses and ports. With these endpoints, I can now connect to my <strong>SQL Server 2019 Big Data Cluster</strong>. Once again, to connect to the SQL Server master instance (databases), you use the <code>service-master-pool-lb</code> endpoint, and to connect to Hadoop/Spark, the <code>service-security-lb</code> endpoint is what you use.</p>

<p>The user names and passwords are:</p>

<ul>
<li>SQL Server master instance: <code>sa</code> as user name, and <code>MSSQL_SA_PASSWORD</code> as password.</li>
<li>Hadoop / Spark: <code>root</code> as user name, and <code>KNOX_PASSWORD</code> as password.</li>
</ul>

<p>In a future post I look at what we can do with <strong>SQL Server 2019 Big Data Cluster</strong>.</p>

<h2 id="summary">Summary</h2>

<p>In this post we looked at how to install <strong>SQL Server 2019 Big Data Cluster</strong> on <em>Azure Kubernetes Service</em> (AKS). We saw how to:</p>

<ul>
<li>Create a new Azure resource group using Azure CLI.</li>
<li>Create a Kubernetes cluster in that resource group.</li>
</ul>

<p>We discussed the size requirements for the VM&rsquo;s in the cluster, and mentioned they needed at least 32Gb of RAM. We also need quite a few disks to mount storage on, so the node count is important.</p>

<p>The actual deployment of a <strong>SQL Server 2019 Big Data Cluster</strong> is done using a Python utility <code>mssqlctl</code>. During the deployment process we can monitor the progress via:</p>

<ul>
<li><code>kubectl</code> commands.</li>
<li>the Kubernetes dashboard.</li>
<li>the <strong>SQL Server 2019 Big Data Cluster</strong>&rsquo;s <em>Cluster Administration Portal</em>.</li>
</ul>

<p>Access to the various services in a <strong>SQL Server 2019 Big Data Cluster</strong> is through service load balancers and their external IP addresses and ports:</p>

<ul>
<li>Cluster Administration Portal: <code>service-proxy-lb</code>.</li>
<li>SQL Server master instance: <code>service-master-pool-lb</code>.</li>
<li>Hadoop/Spark: <code>service-security-lb</code>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 44]]></title>
    <link href="http://nielsberglund.com/2018/11/04/interesting-stuff---week-44/" rel="alternate" type="text/html"/>
    <updated>2018-11-04T18:54:29+02:00</updated>
    <id>http://nielsberglund.com/2018/11/04/interesting-stuff---week-44/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/local-testing-with-live-data-means-faster-development-with-azure-stream-analytics-2/">Local testing with live data means faster development with Azure Stream Analytics</a>. An Azure Stream Analytics (ASA) announcement of a new feature which lets you test your ASA queries locally while using live data streams from cloud sources such as Azure Event Hubs, IoT Hub or Blob storage!</li>
<li><a href="https://www.confluent.io/blog/atm-fraud-detection-apache-kafka-ksql">ATM Fraud Detection with Apache Kafka and KSQL</a>. A blog post by <a href="https://twitter.com/rmoff">Robin Moffat</a> discussing how, by using Kafka and KSQL, you can take a stream of inbound ATM transactions and easily set up an application to detect transactions that look fraudulent. Awesome!</li>
<li><a href="https://blog.acolyer.org/2018/10/29/noria-dynamic-partially-stateful-data-flow-for-high-performance-web-applications/">Noria: dynamic, partially-stateful data-flow for high-performance web applications</a>. <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white-paper about Noria: a new streaming data-flow system designed to act as a fast storage backend for read-heavy web applications. It acts like a database but precomputes and caches relational query results so that reads are blazingly fast.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/11/02/the-fuzzylog-a-partially-ordered-shared-log/">The FuzzyLog: a partially ordered shared log</a>. Another white-paper dissected by <a href="https://twitter.com/adriancolyer">Adrian</a>. This time it is about FuzzyLog and the FuzzyLog abstraction which extends the shared log approach to partial orders, allowing applications to scale linearly without sacrificing transactional guarantees, and switch seamlessly between these guarantees when the network partitions and heals.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>I am still trying to get to grips with <strong>SQL Server 2019 Big Data Cluster</strong>, and now I better sort it out as I on Wednesday (November 7), is supposed to give a presentation about <a href="https://www.meetup.com/Azure-Transformation-Labs/events/255690875/?rv=ea1_v2&amp;_xtd=gatlbWFpbF9jbGlja9oAJGRjZmM2OTBjLWEwNTgtNDRjZi1iOWQyLWI5YzMyZTM1YmIzMg">Building a SQL 2019 Big Data Cluster on Azure Kubernetes Service</a>. If you are in Durban, please come by and say Hi!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 43]]></title>
    <link href="http://nielsberglund.com/2018/10/28/interesting-stuff---week-43/" rel="alternate" type="text/html"/>
    <updated>2018-10-28T07:44:55+02:00</updated>
    <id>http://nielsberglund.com/2018/10/28/interesting-stuff---week-43/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/announcing-automated-ml-capability-in-azure-machine-learning/">Announcing automated ML capability in Azure Machine Learning</a>. Somehow I must have missed this post announcing <strong>Azure Automated Machine Learning</strong>. What is it? Well, it is a way for the <a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">Azure Machine Learning Service</a> to automatically pick an algorithm for you, and generate a model from it. It sounds really interesting, and this is something I need to take a look at.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><p><a href="https://www.confluent.io/blog/kafka-summit-san-francisco-2018-roundup">That’s a Wrap! Kafka Summit San Francisco 2018 Roundup</a>. The San Francisco <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2018/">Kafka Summit</a> ran October 16 - 17, and this blog post is a summary of the conference. It also has links to some interesting sessions, and out of those, these are my three favorites:</p>

<ul>
<li><a href="https://www.confluent.io/kafka-summit-sf18/zen-and-the-art-of-streaming-joins">Zen and the Art of Streaming Joins—The What, When and Why</a>.</li>
<li><a href="https://www.confluent.io/kafka-summit-sf18/kafka-security-101-and-real-world-tips">Kafka Security 101 and Real-World Tips</a>.</li>
<li><a href="https://www.confluent.io/kafka-summit-sf18/breaking-down-a-sql-monolith">Breaking Down a SQL Monolith with Change Tracking, Kafka and KStreams/KSQL</a>.</li>
</ul></li>

<li><p><a href="https://data-artisans.com/blog/stateful-stream-processing-apache-flink-state-backends">Stateful Stream Processing: Apache Flink State Backends</a>. This post explores stateful stream processing and more precisely the different state backends available in Apache Flink. It presents the 3 state backends of Apache Flink, their limitations and when to use each of them depending on case-specific requirements.</p></li>

<li><p><a href="https://www.confluent.io/blog/apache-kafka-kubernetes-could-you-should-you">Apache Kafka on Kubernetes – Could you? Should you?</a>. This is a post discussing whether you should run Kafka on Kubernetes or not. At <a href="/derivco">work</a>, we are in the process of rolling out our first Kafka deployments, (not on Kubernetes), and this post definitely gives us &ldquo;food for thought&rdquo;.</p></li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Yeah, I kind of ask myself that question as well (what am I doing): at the moment I have a hard time getting any blog posts out, as I am busy at work as well as trying to get to grips with <strong>SQL Server 2019 Big Data Clusters</strong>. I hope to be able to publish something in a week (or twos) time.</p>

<p>In the meantime, if you are interested in <strong>SQL Server 2019</strong>, go and have a read at these two posts:</p>

<ul>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a>.</li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 42]]></title>
    <link href="http://nielsberglund.com/2018/10/21/interesting-stuff---week-42/" rel="alternate" type="text/html"/>
    <updated>2018-10-21T19:19:43+02:00</updated>
    <id>http://nielsberglund.com/2018/10/21/interesting-stuff---week-42/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/15/guidance-for-library-authors/">Guidance for library authors</a>. This blog post announces the publication of the <strong>.NET Library Guidance</strong>. It’s brand new set of articles for .NET developers who want to create high-quality libraries for .NET.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/16/automating-release-notes-with-azure-functions/">Automating Release Notes with Azure Functions</a>. This post is a walk through how Azure Functions and Azure Blob Storage can help generate release notes.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://eng.uber.com/uber-big-data-platform/">Uber’s Big Data Platform: 100+ Petabytes with Minute Latency</a>. This article looks in-depth at Uber&rsquo;s Hadoop platform and how the platform allows for analysis of over 100 petabytes of data with minimal latency.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/how-to-choose-a-machine-learning-model-some-guidelines">How to Choose a Machine Learning Model – Some Guidelines</a>. This blog post explores some broad guidelines for selecting machine learning models.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/event-driven-2-0">Event Driven 2.0</a>. In this article <a href="https://twitter.com/benstopford">Ben Stopford</a> discusses the next generation of event-driven architecture.</li>
<li><a href="https://www.youtube.com/watch?v=HeNegOzjnJY&amp;feature=em-uploademail">Jay Kreps | Kafka Summit SF 2018 Keynote (Kafka and Event-Oriented Architecture)</a>. The San Francisco <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2018/">Kafka Summit 2018</a> was held October 16 - 17, and this video is Jay Kreps keynote.</li>
<li><a href="https://www.youtube.com/watch?v=v2RJQELoM6Y&amp;feature=em-uploademail">Martin Kleppmann | Kafka Summit SF 2018 Keynote (Is Kafka a Database?)</a>. Another <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2018/">Kafka Summit 2018</a> keynote video. This is <a href="https://twitter.com/martinkl">Martin Kleppmann</a> comparing Kafka to databases.<br /></li>
<li><a href="https://www.infoq.com/articles/apache-kafka-best-practices-to-optimize-your-deployment">Apache Kafka: Ten Best Practices to Optimize Your Deployment</a>. An <a href="https://www.infoq.com/">InfoQ</a> article discussing the latest Kafka best practices for developers to manage the data streaming platform more effectively. Best practices include log configuration, proper hardware usage, Zookeeper configuration, replication factor, and partition count.</li>
<li><a href="https://data-artisans.com/blog/watermarks-in-apache-flink-made-easy">Watermarks in Apache Flink Made Easy</a>. This blog post looks at how watermarks work in Flink.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 41]]></title>
    <link href="http://nielsberglund.com/2018/10/14/interesting-stuff---week-41/" rel="alternate" type="text/html"/>
    <updated>2018-10-14T08:03:37+02:00</updated>
    <id>http://nielsberglund.com/2018/10/14/interesting-stuff---week-41/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/a-fast-serverless-big-data-pipeline-powered-by-a-single-azure-function/">A fast, serverless, big data pipeline powered by a single Azure Function</a>. This is a blog post about how to use Azure Serverless functions to build highly performant data pipelines. At work, we are looking at implementing something similar.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><p><a href="https://muratbuffalo.blogspot.com/2018/10/everything-is-broken.html">Everything is broken</a>. This is a very cool post by <a href="https://twitter.com/muratdemirbas">Murat</a> where he lists some very relevant quotes/statements from a recent <a href="https://www.meetup.com/Everything-Is-Broken/events/251899676/">Everything Is Broken</a> meetup he attended. Some of the quotes I particularly liked:</p>

<ul>
<li><em>Without observability you don&rsquo;t have chaos engineering, you have a chaos.</em></li>
<li><em>You don&rsquo;t know what you don&rsquo;t know, so dashboards are very limited utility. Dashboards are only for anticipated cases: every dashboard is an artifact of past failures. There are too many dashboards, and they are too slow.</em></li>

<li><p><em>prerequisites for chaos engineering:</em></p>

<ol>
<li><em>monitoring &amp; observability</em></li>
<li><em>on-call &amp; incident management</em></li>
<li><em>know the cost of your downtime per hour (British Airlines&rsquo;s 1 day outage costed $150 millon)</em></li>
</ol></li>

<li><p><em>How to choose a chaos experiment?</em></p>

<ul>
<li><em>identify top 5 critical systems</em></li>
<li><em>choose 1 system</em></li>
<li><em>whiteboard the system</em></li>
<li><em>select attack: resource/state/network</em></li>
<li><em>determine scope</em></li>
</ul></li>
</ul></li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/08/announcing-ml-net-0-6-machine-learning-net/">Announcing ML.NET 0.6 (Machine Learning .NET)</a>. Microsoft just released ML.NET 0.6, and this post highlights some of the new enhancements.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/@simon.aubury/machine-learning-kafka-ksql-stream-processing-bug-me-when-ive-left-the-heater-on-bd47540cd1e8">Machine learning &amp; Kafka KSQL stream processing — bug me when I’ve left the heater on</a>. I like this post as it combines two of my favorite topics: Streaming and Machine Learning. So anyway, the post is about how you can, by using Kafka and Machine Learning, monitor household power usage and alert when something out of the ordinary occurs.</li>
<li><a href="https://data-artisans.com/blog/an-introduction-to-acid-guarantees-and-transaction-processing">An introduction to ACID guarantees and transaction processing</a>. A while ago dataArtisans introduced serializable, distributed ACID transactions directly on data streams in Flink. This post here talks about the foundations of the capability.</li>
<li><a href="https://www.confluent.io/blog/ksql-recipes-available-now-stream-processing-cookbook">KSQL Recipes Available Now in the Stream Processing Cookbook</a>. A post which introduces the &ldquo;KSQL Cookbook&rdquo;: a collection of &ldquo;recipes&rdquo; designed to help people build event-driven, real-time systems.</li>
<li><a href="https://data-artisans.com/blog/how-apache-flink-manages-kafka-consumer-offsets">How Apache Flink manages Kafka consumer offsets</a>. This post explains with a step-by-step example of how Apache Flink works with Apache Kafka to ensure that records from Kafka topics are processed with exactly-once guarantees.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 40]]></title>
    <link href="http://nielsberglund.com/2018/10/07/interesting-stuff---week-40/" rel="alternate" type="text/html"/>
    <updated>2018-10-07T09:34:44+02:00</updated>
    <id>http://nielsberglund.com/2018/10/07/interesting-stuff---week-40/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/04/update-on-net-core-3-0-and-net-framework-4-8/">Update on .NET Core 3.0 and .NET Framework 4.8</a>. A blog post from the .NET engineering team, where they talk about the future of the .NET Framework and .NET Core. I wonder if this post was prompted by speculations recently about the future of the .NET Framework, where there were questions whether the .NET Framework 4.8 would be the last version, and all development would be concentrated on .NET Core.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/enabling-real-time-data-warehousing-with-azure-sql-data-warehouse/">Enabling real-time data warehousing with Azure SQL Data Warehouse</a>. This post is an announcement how <a href="https://www.striim.com/">Striim</a> now fully supports SQL Data Warehouse as a target for Striim for Azure. Striim is a system which enables continuous non-intrusive performant ingestion of enterprise data from a variety of sources in real time.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/event-streaming-new-big-thing-finance">Is Event Streaming the New Big Thing for Finance?</a>. An excellent blog post by <a href="https://twitter.com/benstopford">Ben Stopford</a> where he discusses the use of event streaming in the financial sector.</li>
<li><a href="https://www.confluent.io/blog/troubleshooting-ksql-part-2">Troubleshooting KSQL – Part 2: What’s Happening Under the Covers?</a>. The second post by <a href="https://twitter.com/rmoff">Robin Moffat</a> about debugging of KSQL. In this post - Robin, as the title says, goes under the covers to figure out what happens with KSQL queries.</li>
<li><a href="https://data-artisans.com/blog/6-things-to-consider-when-defining-your-apache-flink-cluster-size">6 things to consider when defining your Apache Flink cluster size</a>.  This post discusses how to plan and calculate a Flink cluster size. In other words; how to define the number of resources you need to run a specific Flink job.</li>
</ul>

<h2 id="ms-ignite">MS Ignite</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/2018/10/02/syllabuck-ignite-2018-conference/">Syllabuck: Ignite 2018 Conference</a>. A great list of MS Ignite sessions that <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a> found interesting! Now I know what to do in my spare time!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/10/03/customized-regression-model-for-airbnb-dynamic-pricing/">Customized regression model for Airbnb dynamic pricing</a>. This post by <a href="https://twitter.com/adriancolyer">Adrian</a> is about a white-paper which details the methods that Airbnb use to suggest prices to listing hosts.</li>
<li><a href="https://towardsdatascience.com/cleaning-and-preparing-data-in-python-494a9d51a878">Cleaning and Preparing Data in Python</a>. A post which lists Python methods and functions that helps to clean and prepare data.</li>
<li><a href="https://www.microsoft.com/en-us/research/blog/the-microsoft-infer-net-machine-learning-framework-goes-open-source/">The Microsoft Infer.NET machine learning framework goes open source</a>. A blog post from Microsoft Research, in which they announce the open-sourcing of <a href="https://dotnet.github.io/infer/">Infer.NET</a>. Is anyone else but me somewhat confused about the various data science frameworks that Microsoft has?</li>
<li><a href="https://towardsdatascience.com/how-to-build-a-simple-recommender-system-in-python-375093c3fb7d">How to build a Simple Recommender System in Python</a>. A blog post which discusses what a recommender system is and how you can use Python to build one.</li>
</ul>

<h2 id="what-is-niels-doing-wind">What Is Niels Doing (WIND)</h2>

<p>That is a good question! As you know, I wrote two blog posts about SQL Server 2019:</p>

<ul>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a></li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a></li>
</ul>

<p>My plan was to relatively quickly follow up those two posts with a third post how to run <strong>SQL Server Machine Learning Services</strong> on <strong>SQL Server 2019 on Linux</strong>, and do it inside a Docker container. After having spent some time trying to get it to work, (with no luck), I gave up and contacted a couple of persons in MS asking for help. The response was that, right now in <strong>SQL Server 2019 on Linux CTP 2.0</strong>, you cannot do it - bummer! The functionality will be in a future release.</p>

<p>I am now reworking the post I had started on to cover <strong>SQL Server Machine Learning Services</strong> in an <strong>Ubuntu</strong> based <strong>SQL Server 2019 on Linux</strong>. I should be able to publish something within a week or two.</p>

<p>I am also working on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series (still). Right now I have no idea when I can publish it - Sorry!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 39]]></title>
    <link href="http://nielsberglund.com/2018/09/30/interesting-stuff---week-39/" rel="alternate" type="text/html"/>
    <updated>2018-09-30T13:13:43+02:00</updated>
    <id>http://nielsberglund.com/2018/09/30/interesting-stuff---week-39/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://adamsitnik.com/ETW-Profiler/">Profiling .NET Code with BenchmarkDotNet</a>. If you want to benchmark your .NET code, you probably use <a href="https://benchmarkdotnet.org/">BenchMarkDotNet</a> (if you do not, you should). The man behind BenchMarkDotNet is <a href="https://twitter.com/SitnikAdam">Adam Sitnik</a>, and in the linked blog post he announces how you, soon, can use the EtwProfiler to profile benchmarked code! Very cool!</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/09/26/the-design-and-implementation-of-modern-column-oriented-database-systems/">The design and implementation of modern column-oriented database systems</a>. In this post, <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white paper about column-oriented databases. Having worked a little bit with SQL Server&rsquo;s column store indexes, it is very cool to get the &ldquo;lowdown&rdquo; on the design behind it.</li>
</ul>

<h2 id="azure-cloud">Azure Cloud</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/azure-databricks-delta-in-preview-9-regions-added-and-other-exciting-announcements/">Azure Databricks – Delta in preview, 9 regions added, and other exciting announcements</a>. A blog post announcing that Azure Databricks Delta is available in preview. This is very interesting since I have been &ldquo;chomping at the bits&rdquo;, to do some tests with Databricks Delta.</li>
<li><a href="https://azure.microsoft.com/en-us/blog/spark-debugging-and-diagnosis-toolset-for-azure-hdinsight/">Spark Debugging and Diagnosis Toolset for Azure HDInsight</a>. This post is another announcement from Microsoft. This time it is how <strong>Spark Diagnosis Toolset for HDInsight</strong> is now available in preview. The toolset allows you to identify low parallelization, to detect data skew and run data skew analysis, and quite a lot more.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-presence-detection-apache-kafka-aws">Real-Time Presence Detection at Scale with Apache Kafka on AWS</a>. This post discusses how <a href="https://www.zenreach.com/">Zenreach</a> has implemented a framework for real-time presence detection, using Kafka Streams.</li>
<li><a href="https://data-artisans.com/blog/state-ttl-for-apache-flink-how-to-limit-the-lifetime-of-state">State TTL for Apache Flink: How to Limit the Lifetime of State</a>. Instead of me summarising the post, I shamelessly copy the opening paragraph: <em>A common requirement for many stateful streaming applications is the ability to control how long application state can be accessed (e.g., due to legal regulations like GDPR) and when to discard it. This blog post introduces the state time-to-live (TTL) feature that was added to Apache Flink with the 1.6.0 release</em>. It is very, very interesting. I need to start to play around with Flink!</li>
<li><a href="https://www.confluent.io/blog/troubleshooting-ksql-part-1">Troubleshooting KSQL – Part 1: Why Isn’t My KSQL Query Returning Data?</a>. The obligatory Kafka link. The post is the first in a series how to troubleshoot KSQL. This and future posts in the series is, and, will be required reading for our Kafka team!</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/azure-data-studio-for-sql-server/">Azure Data Studio for SQL Server</a>. A post by <a href="https://twitter.com/vickyharp">Vicky Harp</a>. Vicky is Principal Program Manager Lead at Microsoft for SQL Server tooling, and in the post, she introduces <strong>Azure Data Studio</strong> (the artist formerly known as SQL Operations Studio). Azure Data Studio is a new cross-platform desktop environment for both on-premises and cloud data platforms on Windows, MacOS, and Linux.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/24/sql-server-2019-preview-combines-sql-server-and-apache-spark-to-create-a-unified-data-platform/">SQL Server 2019 preview combines SQL Server and Apache Spark to create a unified data platform</a>. An announcement by Microsoft how SQL Server 2019 comes with support for both Spark as well as Hadoop File System (HDFS). We do live in exciting times!</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/introducing-microsoft-sql-server-2019-big-data-clusters/">Introducing Microsoft SQL Server 2019 Big Data Clusters</a>. This post builds on top of the post above. It discusses how we can create big data clusters utilising the support in SQL Server 2019 of Spark and HDFS.</li>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a>. A post by yours truly. I do a quick look at what is new in SQL Server 2019, and I especially look at the Java language extension.</li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a>. Another post my myself. Since SQL Server 2019 for Linux now have support for SQL Server Machine Learning Services, I want to have a look at how it works. For that I obviously need it installed and I decided to install it as a Docker for Windows container. The post walks through what I did to get it installed. The post also discusses Azure Data Studio briefly.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 for Linux in Docker on Windows]]></title>
    <link href="http://nielsberglund.com/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/" rel="alternate" type="text/html"/>
    <updated>2018-09-29T12:06:09+02:00</updated>
    <id>http://nielsberglund.com/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/</id>
    <content type="html"><![CDATA[<p>By the time I publish this blog post <a href="https://www.microsoft.com/en-us/ignite">MS Ignite</a> is over. During Ignite, Microsoft announced quite a few new things, amongst them <strong>SQL Server 2019</strong> with a whole lot of new features and functionality.</p>

<p>I touched briefly on some of them in my <a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a> post. A couple of things that caught my eye were that <strong>SQL Server 2019 for Linux</strong> now supports In-Database analytics, what we know as <strong>SQL Server Machine Learning Services</strong> (R and Python), as well as the Java language extension.</p>

<blockquote>
<p><strong>NOTE:</strong> You may ask yourself what the Java language extension is; well, that is the ability to access Java code from T-SQL. It is a little bit like SQLCLR, but it executes outside of the SQL Server memory and process space.</p>
</blockquote>

<p>Seeing that I never really have played around with <em>SQL Server for Linux</em>, mostly due to that in previous versions (2017) it did not have support for In-Database analytics, I thought that now would be a good time to have a look.</p>

<p></p>

<p>Cool, so install <em>SQL Server 2019 for Linux</em> and start to play around! Hmm, what do I install it on - I am a Windows guy, this whole Linux thing is &ldquo;scary&rdquo;. Ok, I guess I could spin up a virtual machine and install it there, but I am lazy. Create a VM, and then install SQL Server seemed like too much work.</p>

<p>Then I thought about my mate and colleague <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>, a.k.a <a href="https://charlla.com/kafka-donuts/">The Donut Maker</a>, and how he raves about Docker. So maybe I should run <em>SQL Server 2019 for Linux</em> in a container, problem solved. Uh, maybe not; you see - I have never used Docker. I am an old guy (some would even call me a &ldquo;Grumpy Old Man&rdquo;, a <em>GOM</em>), and you know the saying about old dogs and new tricks.</p>

<p>So anyway, I decided to give it a go; how hard can it be (it turns out not hard at all), and this post is about the steps I took to get <em>SQL Server 2019 for Linux</em> running in Docker on Windows.</p>

<h2 id="docker-for-windows">Docker for Windows</h2>

<p>This post does not cover how to download and install Docker for Windows, as there are lots of posts out there about it. If you want somewhere to start; <a href="https://docs.docker.com/v17.09/docker-for-windows/">Get started with Docker for Windows</a> is an excellent starting point.</p>

<p>I do, however, want to point out a couple of things, that caught me out:</p>

<ul>
<li>Hyper-V needs to be enabled on your host computer. This means you cannot run Virtual Box VM&rsquo;s at the same time.</li>
<li>When you install Docker, you decide whether you want to run Linux or Windows containers. So, if you install Docker for Windows intending to run <em>SQL Server 2019 for Linux</em>, you choose Linux containers.</li>
</ul>

<p>You can change the choice between Linux and Windows containers from the Docker icon in the system tray (right click on the icon):</p>

<p><img src="/images/posts/sql_2k19Docker_container_type.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Change Container Type</em></p>

<p>In <em>Figure 1</em> we see the menu entry to change the Docker container type to Windows. To change container type works the other way around as well; changing from Windows to Linux.</p>

<h2 id="docker-basics">Docker Basics</h2>

<p>Before we look at how to get and install the SQL Server &ldquo;stuff&rdquo; let us discuss some basics, and let us start with some vocabulary:</p>

<ul>
<li>Layer: a set of read-only files or commands that describe how to set up the underlying system beneath the container</li>
<li>Image: this is the piece of &ldquo;something&rdquo;, in our case <em>SQL Server 2019 for Linux</em>, that you want to install. The image consists of one or more layers.</li>
<li>Container: you download an image and create a container, and this is what you interact with.</li>
<li>Registry: where images are stored and delivered from.</li>
</ul>

<p>In our case we:</p>

<ul>
<li>Connect to a registry which contains a <em>SQL Server 2019 for Linux</em> image.</li>
<li>We download the image and create a container.</li>
<li>We &ldquo;run&rdquo; the container and interact with SQL Server.</li>
</ul>

<p>The interaction with Docker (download image, create a container, etc.) is via CLI (Command Line Interface), using the <code>docker</code> base command followed by child commands and options/parameters (<code>docker childcommand</code>). Examples of child commands:</p>

<ul>
<li><code>login</code>: logs in to a Docker registry.</li>
<li><code>pull</code>: retrieve an image from a registry.</li>
<li><code>images</code>: returns a list of images on the machine.</li>
<li><code>run</code>: creates a new container from an image and starts it. If the image has not been <code>pull</code>:ed yet, it also pulls the image.</li>
<li><code>ps</code>: Lists containers.</li>
<li><code>exec</code>: executes a command in a container. For example, you want to run a command shell in the container.</li>
<li><code>stop</code>: stops a running container.</li>
<li><code>start</code>: starts up an existing stopped container.</li>
<li><code>rm</code>: removes a container.</li>
</ul>

<p>To see a full list of commands you can go <a href="https://docs.docker.com/engine/reference/commandline/docker/">here</a>.</p>

<p>As I mentioned above, we interact with Docker via the command line, and when you are on Windows, you most likely use <em>Powershell</em>. In this post I do it somewhat differently in that I do not use the actual <em>Powershell</em> shell, but instead <strong>Azure Data Studio</strong>.</p>

<h2 id="azure-data-studio">Azure Data Studio</h2>

<p>What is Azure Data Studio then? Well, it is the evolution of SQL Operations Studio. The blog post <a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/azure-data-studio-for-sql-server/">Azure Data Studio for SQL Server</a>, introduces it like so:</p>

<p>*Azure Data Studio is a new cross-platform desktop environment for data professionals using the family of on-premises and cloud data platforms on Windows, MacOS, and Linux. Previously released under the preview name SQL Operations Studio, Azure Data Studio offers a modern editor experience with lightning fast IntelliSense, code snippets, source control integration, and an <strong>integrated terminal</strong>. It is engineered with the data platform user in mind, with built-in charting of query resultsets and customizable dashboards.*</p>

<p>We can think what we want about the &ldquo;blurb&rdquo; above, but <em>ADS</em> does have some interesting features, and for the Docker CLI work we use the integrated terminal:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Azure Data Studio and Integrated Terminal</em></p>

<p>What we see in <em>Figure 2</em> is <em>ADS</em> with visualised resultsets, some dashboards and - outlined in red - the integrated terminal. Now, let us get down to business.</p>

<h2 id="getting-the-sql-server-2019-for-linux-image">Getting the SQL Server 2019 for Linux Image</h2>

<p>We get the <em>SQL Server 2019 for Linux</em> Docker image from the <a href="https://azure.microsoft.com/en-us/blog/microsoft-syndicates-container-catalog/">Microsoft Container Registry</a> (MCR). MCR acts as a single download source for Microsoft’s container images. Regardless of where customers discover Microsoft images, the pull source is <a href="https://azure.microsoft.com/en-us/services/container-registry/">mcr.microsoft.com</a>.</p>

<p>To get the image I open <em>Azure Data Studio</em>:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio2.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Helper Files</em></p>

<p>We see in <em>Figure 3</em> how I have the <code>2k19_linux.ps</code> file open in the <em>ADS</em> editor, and how that file contains some Docker commands. I open the integrated terminal in <em>ADS</em> through <strong>Ctrl + `</strong>, or by using the menu: &ldquo;View | Command Palette | View: Toggle Integrated Terminal&rdquo;:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio3.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Integrated Terminal</em></p>

<p>In <em>Figure 4</em> we see how the terminal is open (outlined in red) and it is the Powershell terminal (highlighted in red).</p>

<blockquote>
<p><strong>NOTE:</strong> The reason I use <em>ADS</em> is that I wanted to see what I can do with it, I could as easily have used the <em>Powershell</em> shell.</p>
</blockquote>

<p>Let us now get the SQL Server 2019 image, and I do it by copying the <code>docker pull ...</code>command from the file to the terminal and hit enter. In the terminal you now see something like so (output edited for readability):</p>

<pre><code class="language-bash">PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt; 
    docker pull mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
vNext-CTP2.0-ubuntu: Pulling from mssql/server
b234f539f7a1: Downloading [========&gt; ]  7.519MB/43.12MB
55172d420b43: Download complete
5ba5bbeb6b91: Download complete
43ae2841ad7a: Download complete
f6c9c6de4190: Download complete
28f02293f049: Download complete
5eb40916d530: Downloading [&gt;         ]   1.08MB/70.39MB
46e88947bdd0: Downloading [=&gt;        ]  8.634MB/414.5MB
26983ce22a89: Waiting
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Doing a Pull</em></p>

<p>We see in <em>Code Snippet 1</em> how Docker retrieves the image. In fact, it retrieves the layers the image consists of. The layers are identified by the <code>b234f539f7a1</code>, <code>55172d420b43</code>, and so forth as we see in <em>Code Snippet 1</em>. Eventually, the <code>pull</code> finishes, and we see in the terminal:</p>

<pre><code class="language-bash">PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt; 
    docker pull mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
vNext-CTP2.0-ubuntu: Pulling from mssql/server
b234f539f7a1: Pull complete
55172d420b43: Pull complete
5ba5bbeb6b91: Pull complete
43ae2841ad7a: Pull complete
f6c9c6de4190: Pull complete
28f02293f049: Pull complete
5eb40916d530: Pull complete
46e88947bdd0: Pull complete
26983ce22a89: Pull complete
Digest: sha256:87e691e2e5f738fd64a427ebe935e4e5ccd...
Status: Downloaded newer image for 
    mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Pull Finished</em></p>

<p>After the <code>pull</code> command has finished, we can check what images we have by executing <code>docker images</code>. When I do it on my machine I see this:</p>

<p><img src="/images/posts/sql_2k19Docker_pulled images.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Pulled Docker Images</em></p>

<p>We see from <em>Figure 5</em> how the SQL Server image now exists on the machine.</p>

<h4 id="creating-a-container">Creating a Container</h4>

<p>Cool, we have an image. However, an image is just that, an image, and you cannot interact with it. To relate it to SQL Server, think about the image as an <code>.iso</code> install file. We need to &ldquo;install&rdquo; the image, e.g. create and run a container. For this we use the second <code>docker</code> command from  <em>Figure 3</em> above, and it looks like so:</p>

<pre><code class="language-bash">docker run -e &quot;ACCEPT_EULA=Y&quot; \ 
           -e &quot;SA_PASSWORD=&lt;Strong!Passw0rd&gt;&quot; \
           -p 1433:1433 \
           --name sql2k19_1 \
           -d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create &amp; Run a Container</em></p>

<p>In <em>Code Snippet 3</em> we see how we use the <code>docker run</code> command to create the container. Let us look at the options:</p>

<ul>
<li><code>-e &quot;ACCEPT_EULA=Y&quot;</code>: As creating the container also installs SQL Server, we need to accept the SQL Server EULA. The <code>-e</code> option (also <code>--env</code>) sets environment variables. In this case, environment variables SQL Server requires.</li>
<li><code>-e &quot;SA_PASSWORD=&lt;Strong!Passw0rd&gt;&quot;</code>: A second environment variable. When running SQL Server in a container, you need to set a password which follows the SQL Server default password policy. Otherwise, the container can not setup SQL server and will stop working. By default, the password must be at least 8 characters long and contain characters from three of the following four sets: Uppercase letters, Lowercase letters, Base 10 digits, and Symbols.</li>
<li><code>-p 1433:1433</code>: The <code>-p</code> (or <code>--expose</code>) option binds a port on the host machine (to the left of the colon) to a port on the container. If you run multiple SQL Server containers, the SQL Server container uses port 1433 by default, and you should use different port numbers for the host machine: <code>-p 1401:1433</code> for example.</li>
<li><code>--name sql2k19_1</code>: The <code>--name</code> option assigns a name to the container. This is like a SQL Server instance name.</li>
<li><code>-d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu</code>: This indicates which image to create a container from. The <code>-d</code> option tells Docker we want to run the container detached from the calling process. In other words, it is still up and running after you close the terminal.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> I mentioned above about the <code>-p</code> option that if you run multiple instances you should have different host ports. This is also true if you run a non Docker SQL Server instance on you machine.</p>
</blockquote>

<p>After we execute the code in <em>Code Snippet 3</em> we can check that we have a new container: <code>docker ps</code>:</p>

<p><img src="/images/posts/sql_2k19Docker_created_container.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Docker Container</em></p>

<p>From what we see in <em>Figure 6</em>, it looks like we are in business! If we want to we can connect into the container and, for example, run a bash shell:</p>

<pre><code class="language-bash">docker exec -it sql2k19_1  /bin/bash
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Run bash Shell in the Container</em></p>

<p>That is all well and good, but what about SQL Server?</p>

<h2 id="test-the-container">Test the Container</h2>

<p>Right, so now we have a container, and that container hopefully runs SQL Server. Let us try and connect to the SQL Server via <em>ADS</em>.</p>

<p>So I switch from the <em>Explorer</em> view to <em>Servers</em>: <strong>Ctrl + G</strong>, and I click <em>New Connection</em>:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_new_connection.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>New Connection</em></p>

<p>The <em>New Connection</em> is what is highlighted in red in <em>Figure 7</em>, and clicking it I get a connection dialog:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_connect.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>New Connection</em></p>

<p>In the connection dialog we see, in <em>Figure 8</em>, how I want to connect to localhost (the highlighted &ldquo;.&rdquo; in the <code>Server</code> text box), the password is whatever password I set in <em>Code Snippet 3</em>, and I chose to give the connection a name (the highlighted part in the <code>Name</code> text box). So if everything works, when I click on <em>Connect</em> I should see something like so:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_connected.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Successful Connection</em></p>

<p>As we see in <em>Figure 9</em> everything worked, and I am now connected to SQL Server 2019 for Linux, running in Docker container! To further prove all works I click on the &ldquo;New Query&rdquo; button (highlighted in red), and I execute a trivial <code>SELECT</code> statement: <code>SELECT * FROM sys.databases</code>:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_query_result.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Result from Select</em></p>

<p>In <em>Figure 10</em> we see how we get the result back! We can now continue working with <em>SQL Server 2019 for Linux</em>. If you for some reason want to shut down the container you run <code>docker stop &lt;containername&gt;</code> , and to start it up again - surprise, surprise - <code>docker start &lt;containername&gt;</code>.</p>

<h2 id="summary">Summary</h2>

<p>In this post we covered how we can run <em>SQL Server 2019 for Linux</em> in a Docker container on our Windows machine. We mentioned the Docker commands to use:</p>

<ul>
<li><code>docker pull</code></li>
<li><code>docker run</code></li>
<li><code>docker images</code></li>
<li><code>docker ps</code></li>
<li><code>docker stop</code></li>
<li><code>docker start</code></li>
</ul>

<p>We mentioned how we map a port on the hosting machine to a port on the container, and how we should use different host ports when we have multiple SQL Server instances. The SQL Server in the container is by default using port 1433.</p>

<p>In the post I also spoke about <em>Azure Data Studio</em> and some of its new functionality.</p>

<p>In future blog posts I will talk more about <em>SQL Server 2019 for Linux</em>, especially the In-Database analytics and the Java extensions, as well as <em>Azure Data Studio</em>.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is New in SQL Server 2019 Public Preview]]></title>
    <link href="http://nielsberglund.com/2018/09/24/what-is-new-in-sql-server-2019-public-preview/" rel="alternate" type="text/html"/>
    <updated>2018-09-24T19:17:06+02:00</updated>
    <id>http://nielsberglund.com/2018/09/24/what-is-new-in-sql-server-2019-public-preview/</id>
    <content type="html"><![CDATA[<p>If you read my roundup for <a href="/2018/09/23/interesting-stuff---week-38/">week 38</a>, which I published yesterday, you probably noticed that <a href="https://www.microsoft.com/en-us/ignite"><strong>MS Ignite</strong></a> started today. I mentioned in the post that I was particularly interested in some of the <strong>SQL Server</strong> sessions, as they looked very interesting.</p>

<p>However, even before the sessions started, Microsoft released SQL Server 2019 CTP 2.0 for public preview and, naturally, I jumped on the <a href="https://www.microsoft.com/en-us/evalcenter/evaluate-sql-server-2019-ctp">download link</a> and started downloading. I managed to get to the link in time before the rest of the world started the download, so I managed to get it down and then did an install.</p>

<p>The rest of this post is about my initial findings mostly in the SQL Server Machine Learning Services space.</p>

<blockquote>
<p><strong>NOTE:</strong> I have looked at SQL Server 2019 the grand total of an hour, so this is a short post.</p>
</blockquote>

<p></p>

<h2 id="installation-versions">Installation &amp; Versions</h2>

<p>First of all, the installation took forever, at least it felt that way. I believe it took around an hour, just for the install. So if you install, make sure you are not in a hurry.</p>

<p>I chose to install R and Python services in-database. After the installation finished, (finally), I enabled the machine learning services:</p>

<pre><code class="language-sql">EXEC sp_configure 'external scripts enabled', 1
RECONFIGURE WITH OVERRIDE
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Enable External Scripts</em></p>

<p>After executing the code in <em>Code Snippet 1</em>, I restarted the SQL Server 2019 instance, and then executed my regular &ldquo;check everything works&rdquo; code:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
              @language = N'R'
        , @script = N'd&lt;-42'

EXEC sp_execute_external_script 
              @language = N'Python'
        , @script = N'd=42'

EXEC sp_execute_external_script
                  @language = N'R' ,
                  @script = N'print(R.Version()$version)'

EXEC sp_execute_external_script 
              @language = N'Python'
, @script = N'
import sys
print (sys.version)'
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Test Code</em></p>

<p>As you see, the code is exceptionally advanced (not), but at least the code indicates if there are any issues. The last two <code>sp_execute_external_script</code> statements return the R and Python versions. For R the engine is now running on version <code>3.4.4</code> whereas in SQL Server 2017 it is <code>3.3.3</code>. For Python, it is the same version in both 2017 and 2019: <code>3.5.2</code>.</p>

<h2 id="extensibility-framework">Extensibility Framework</h2>

<p>So, when I read <a href="https://docs.microsoft.com/en-us/sql/sql-server/what-s-new-in-sql-server-ver15?view=sql-server-ver15">What&rsquo;s new in SQL Server 2019</a>, I came across a lot of interesting &ldquo;stuff&rdquo;, but one thing that stood out was <em>Java language programmability extensions</em>. In essence, it allows us to execute Java code in SQL Server by using a pre-built Java language extension! The way it works is as with R and Python; the code executes outside of the SQL Server engine, and you use <code>sp_execute_external_script</code> as the entry-point.</p>

<p>I haven&rsquo;t had time to execute any Java code as of yet, but in the coming days, I definitely will drill into this. Something I noticed is that the architecture for SQL Server Machine Learning Services has changed (or had additions to it). If you remember from my <a href="/sql_server_2k16_r_services">SQL Server Machine Learning Services</a> posts, the flow when executing <code>sp_execute_external_script</code> looked something like so:</p>

<ul>
<li>We execute <code>sp_execute_external_script</code>.</li>
<li>SQL Server connects to the Launchpad service.</li>
<li>Based on the <code>@language</code> parameter, Launchpad calls into either <code>rlauncher.dll</code> or <code>pythonlauncher.dll</code>.</li>
<li>The respective launcher then launches the external engine.</li>
</ul>

<p>If now Java is supported is there also a Java launcher? No, as it turns out, there is not, at least not what I could find. However what I did find was this:</p>

<p><img src="/images/posts/sql_2k19_ml_impr1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Common Launcher</em></p>

<p>In the same directory as the R and Python launchers, I see this new <code>commonlauncher.dll</code> together with a config file. When looking at the config file I did not see anything giving any hints to what goes on, but - as I said above - I will investigate.</p>

<p>At this stage I have two theories about what happens when you execute Java code:</p>

<ol>
<li>The Launchpad service knows about the Java extension: <code>javaextension.dll</code>, which is in the same directory as the launchers, and routes everything with <code>@language = Java</code> to the extension.</li>
<li>For any <code>@language</code> parameter that is not <code>R</code> or <code>Python</code>, the Launchpad service calls the <code>commonlauncher.dll</code>.</li>
</ol>

<p>That&rsquo;s more or less what I found out after an hours &ldquo;playing around&rdquo; with SQL Server 2019 CTP 2.0.</p>

<h2 id="other-interesting-stuff">Other Interesting Stuff</h2>

<p>In the beginning of this post I mentioned about interesting things I found in the <a href="https://docs.microsoft.com/en-us/sql/sql-server/what-s-new-in-sql-server-ver15?view=sql-server-ver15">What&rsquo;s new &hellip;</a> article. In no particular order:</p>

<h3 id="big-data-clusters">Big Data Clusters</h3>

<ul>
<li>Deploy a Big Data cluster with SQL Server and Spark Linux containers on Kubernetes</li>
<li>Access your big data from HDFS</li>
<li>Run Advanced analytics and machine learning with Spark</li>
<li>Use Spark streaming to data to SQL data pools</li>
<li>Run Query books that provide a notebook experience in Azure Data Studio.</li>
</ul>

<h3 id="data-discovery-and-classification">Data discovery and classification</h3>

<ul>
<li>Helps meet data privacy standards and regulatory compliance requirements.</li>
<li>Supports security scenarios, such as monitoring (auditing), and alerting on anomalous access to sensitive data.</li>
<li>Makes it easier to identify where sensitive data resides in the enterprise, so that administrators can take the right steps to secure the database.</li>
</ul>

<h3 id="sql-server-machine-learning-services-failover-clusters-and-partition-based-modeling">SQL Server Machine Learning Services failover clusters and partition based modeling</h3>

<ul>
<li>Partition-based modeling: Process external scripts per partition of your data using the new parameters added to <code>sp_execute_external_script</code>. This functionality supports training many small models (one model per partition of data) instead of one large model.</li>
<li>Windows Server Failover Cluster: Configure high availability for Machine Learning Services on a Windows Server Failover Cluster.</li>
</ul>

<h3 id="azure-data-studio">Azure Data Studio</h3>

<p>Previously released under the preview name SQL Operations Studio, Azure Data Studio is a lightweight, modern, open source, cross-platform desktop tool for the most common tasks in data development and administration. With Azure Data Studio you can connect to SQL Server on premises and in the cloud on Windows, macOS, and Linux.</p>

<h2 id="other-resources">Other Resources</h2>

<p><a href="https://twitter.com/aaronbertrand">Aaron Bertrand</a> has an <a href="https://www.mssqltips.com/sqlservertip/5710/whats-new-in-the-first-public-ctp-of-sql-server-2019/">awesome writeup</a> of what&rsquo;s new in SQL Server 2019 from a more database engine perspective. In that writeup he also points to more resources about SQL Server 2019.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 38]]></title>
    <link href="http://nielsberglund.com/2018/09/23/interesting-stuff---week-38/" rel="alternate" type="text/html"/>
    <updated>2018-09-23T06:47:28+02:00</updated>
    <id>http://nielsberglund.com/2018/09/23/interesting-stuff---week-38/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/changing-face-etl">The Changing Face of ETL</a>. An article by <a href="https://twitter.com/rmoff">Robin Moffat</a> about the &ldquo;new&rdquo; ETL, based on event-driven architectures and streaming platforms.</li>
<li><a href="https://charlla.com/kafka-donuts/">Kafka Donuts</a>. This post is the introduction and TOC to a series of posts about Kafka. The author is my colleague <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>, and in the series, he discusses the use of Kafka in a company who manufactures and sells Donuts. Reading the series introduction post, it is clear that this series is a <strong>MUST</strong> for everyone interested in Kafka. The first episode: <strong>Donut Broker</strong> is <a href="https://charlla.com/kafka-donuts-1/">here</a>, and the second episode <strong>Donut Baker</strong> is <a href="https://charlla.com/kafka-donuts-2/">here</a>.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.paraesthesia.com/archive/2018/09/20/docker-on-wsl-with-virtualbox-and-docker-machine/">Docker on Windows Subsystem for Linux using VirtualBox and Docker Machine</a>. This post by <a href="https://twitter.com/tillig">Travis Illig</a> discusses how you can enable both <strong>VirtualBox</strong> as well as <strong>Docker for Windows</strong> on the same Windows box.</li>
</ul>

<h2 id="microsoft-ignite">Microsoft Ignite</h2>

<p>So, <strong>Microsoft Ignite</strong> starts tomorrow (September 24). It looks to be an awesome conference with lots and lots of announcements of new &ldquo;stuff&rdquo;, I for one cannot wait!</p>

<p>If you, like me, are not attending but still want to follow the key-notes and various sessions, <a href="https://www.microsoft.com/en-us/ignite">this link</a> takes you to the live stream.</p>

<p>The other day I looked at the sessions and here are some that interests me:</p>

<ul>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65955?source=sessions">BRK2416 - The roadmap for SQL Server</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65356?source=sessions">BRK2183 - SQL Server Machine Learning Services: An E2E platform for machine learning</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65957?source=sessions">BRK3228 - What’s new in SQL Server on Linux and containers</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65956?source=sessions">BRK2229 - The future of SQL Server and big data</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66199?source=sessions">THR2168 - The next generation of SQL Server tools</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65967?source=sessions">BRK4021 - Deep dive on SQL Server and big data</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66202?source=sessions">THR2171 - Deploying a highly available SQL Server solution in Kubernetes</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/64634?source=sessions">BRK3154 - SQL Server in containers for application development and DevOps</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66961?source=sessions">THR2308 - SQL Server vNext meets AI and Big Data</a></li>
</ul>

<p>As you see, mostly SQL Server related sessions, and I must say that the sessions around SQL Server and Big Data intrigues me.</p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/how-machine-learning-works-3-resources-to-learn-and-develop-ml-applications">How Machine Learning Works: 3 Resources To Learn And Develop ML Applications</a>. The <a href="https://www.lightbend.com/">Lightbend</a> team has put together some resources about how to design, build, run and manage machine learning applications in production.</li>
<li><a href="https://databricks.com/blog/2018/09/18/simplify-market-basket-analysis-using-fp-growth-on-databricks.html">Simplify Market Basket Analysis using FP-growth on Databricks</a>. In retail, you want to recommend to shoppers what to purchase, and often you base the recommendations on items that are frequently purchased together. A key technique to uncover associations between different items is known as market basket analysis. This blog post talks about how you run your market basket analysis using <strong>Apache Spark MLlib</strong> <code>FP-growth</code> algorithm on <strong>Databricks</strong>.</li>
<li><a href="https://ziedhy.github.io/2018/08/Introduction_Deep_Learning.html">Introduction to Deep Learning</a>. This blog post is the first in a series about <strong>Deep Learning</strong>. At a quick glance, the series looks very informative.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I am still working on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it soon:ish.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 37]]></title>
    <link href="http://nielsberglund.com/2018/09/16/interesting-stuff---week-37/" rel="alternate" type="text/html"/>
    <updated>2018-09-16T08:17:48+02:00</updated>
    <id>http://nielsberglund.com/2018/09/16/interesting-stuff---week-37/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://www.red-gate.com/products/dba/sql-monitor/entrypage/execution-plans">SQL Server Execution Plans, 3rd Edition</a>. The third edition of <a href="https://twitter.com/gfritchey">Grant Fritchey&rsquo;s</a> excellent book about SQL Server Query Plans. If you are a developer or a DBA, you need to get this book (and read it).</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/event-flow-distributed-systems">Complex Event Flows in Distributed Systems</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation how lightweight and highly-scalable state machines ease the handling of complex logic and flows in distributed systems.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/real-time-data-analytics-and-azure-data-lake-storage-gen2/">Real-time data analytics and Azure Data Lake Storage Gen2</a>. Microsoft recently announced <a href="https://azure.microsoft.com/en-us/services/storage/data-lake-storage/">Azure Data Lake Storage Gen 2</a> (ADLS2), and this blog post looks at how ADLS2 can be used for real-time analytics. ADLS2 is at the moment in preview. I certainly hope that MS releases it soon.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/netflix-techblog/keystone-real-time-stream-processing-platform-a3ee651812a">Keystone Real-time Stream Processing Platform</a>. This is a blog post about Keystone; Netflix’s data backbone. It is an essential piece of infrastructure focusing on data analytics. I found this post very interesting, and if you are interested in stream processing, you should really read this post.</li>
<li><a href="https://www.confluent.io/blog/streams-tables-two-sides-same-coin">Streams and Tables: Two Sides of the Same Coin</a>. This blog post announces the availability of the white-paper <a href="https://www.confluent.io/thank-you/streams-and-tables-two-sides-of-the-same-coin/">Streams and Tables: Two Sides of the Same Coin</a>. The paper introduces the Dual Streaming Model, which is used to reason about physical and logical order in data stream processing. This is a <strong>MUST</strong> read!</li>
<li><a href="https://www.confluent.io/blog/building-streaming-application-ksql/">Hands on: Building a Streaming Application with KSQL</a>. In this blog post, we see how we can build a demo streaming application with KSQL, the streaming SQL engine for Apache Kafka. The application continuously computes, in real time, top music charts based on a stream of song play events.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>So the SQL Saturday &ldquo;season&rdquo; is over for me for this year. I did one talk in <a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Johannesburg</a>, two in Cape Town (<a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">this</a> and <a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">this</a>), and one in <a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">Durban</a>.</p>

<p>In addition to the conference talks I also did a full-day workshop in Cape Town and Durban about SQL Server Machine Learning Services: <strong><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">A Day of SQL Server Machine Learning Services with Niels Berglund</a></strong>.</p>

<p>When we talk about SQL Saturdays I want to thank the organisers in the various cities:</p>

<ul>
<li><a href="https://twitter.com/MikeJohnsonZA/">Michael Johnson</a> and team in Johannesburg.</li>
<li><a href="https://twitter.com/Jody_WP">Jody Roberts</a> and <a href="https://twitter.com/TheSQLGirl">Jeanne Combrink</a> and their team in Cape Town.</li>
<li><a href="https://www.linkedin.com/in/jodi-craig-1827b844/">Jodi Craig</a> and team in Durban.</li>
</ul>

<p>They are doing a fantastic work, entirely voluntarily. A HUGE, HUGE <strong>THANK YOU</strong> to all of you!</p>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>Now when SQL Saturday is over, I plan to get back to write about <strong>SQL Server Machine Learning Services</strong>. I am working right now on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it in a week or two.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

