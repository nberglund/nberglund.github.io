<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="http://nielsberglund.com/atom.xml" rel="self"/>
  <link href="http://nielsberglund.com"/>
  <updated>2018-11-25T13:21:17+02:00</updated>
  <id>http://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 47]]></title>
    <link href="http://nielsberglund.com/2018/11/25/interesting-stuff---week-47/" rel="alternate" type="text/html"/>
    <updated>2018-11-25T13:21:17+02:00</updated>
    <id>http://nielsberglund.com/2018/11/25/interesting-stuff---week-47/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.tbray.org/ongoing/When/201x/2018/11/18/Post-REST">Post-REST</a>. A very interesting post by [Tim Bray] where he looks at <em>What Comes after REST</em>/how REST will evolve. Very interesting!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/using-apache-kafka-drive-cutting-edge-machine-learning">Using Apache Kafka to Drive Cutting-Edge Machine Learning</a>. A very, very &ldquo;cool&rdquo; post, discussing how Kafka and Machine Learning fits together.</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/dean-of-big-data-driving-the-snakes-of-data-science-out-of">Dean of Big Data Driving the Snakes of Data Science Out of Ireland</a>. Well, I don&rsquo;t know about &ldquo;St. Paddy&rdquo; and <a href="https://news.nationalgeographic.com/news/2014/03/140315-saint-patricks-day-2014-snakes-ireland-nation/">snakes in Ireland</a>, but this post tries to do away with some Data Science myths (get rid of the snakes). Really worth reading!</li>
<li><a href="https://databricks.com/blog/2018/11/21/mlflow-v0-8-0-features-improved-experiment-ui-and-deployment-tools.html">MLflow v0.8.0 Features Improved Experiment UI and Deployment Tools</a>. A week or so ago, <a href="https://twitter.com/databricks">databricks</a> released <a href="https://www.mlflow.org/">MLFlow 0.8.0</a>. This blog post describes a couple of new features in that release.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>After I published the <a href="/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/">SQL Server 2019 Big Data Cluster on Azure Kubernetes Service</a> post, I went back and started to look at the new Java functionality in SQL Server 2019. There is a post forthcoming shortly (the famous last words).</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 46]]></title>
    <link href="http://nielsberglund.com/2018/11/18/interesting-stuff---week-46/" rel="alternate" type="text/html"/>
    <updated>2018-11-18T08:22:25+02:00</updated>
    <id>http://nielsberglund.com/2018/11/18/interesting-stuff---week-46/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/11/12/building-c-8-0/">Building C# 8.0</a>. A post about new functionality in the upcoming C# 8 release. Some very interesting new features, and it looks like C# gets more and more features that you normally find in F#.</li>
<li><a href="https://adamsitnik.com/Sample-Perf-Investigation/">Sample performance investigation using BenchmarkDotNet and PerfView</a>. In this blog post <a href="https://twitter.com/SitnikAdam">Adam</a> describes how he approaches sample performance problem using available free .NET tools and best practices for performance engineering.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://muratbuffalo.blogspot.com/2018/11/my-emacs-journey.html">My Emacs journey</a>. This blog post by <a href="https://twitter.com/muratdemirbas">Murat</a>, where he talks about Emacs, brings me back to the early days of .NET, (pre 1.0), where I created an Emacs .NET intellisense extension. Ah, those were the days!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/11/16/overload-control-for-scaling-wechat-microservices/">Overload control for scaling WeChat microservices</a>. In this blog post <a href="https://twitter.com/adriancolyer">Adrian</a> looks at a white paper discussing how <a href="https://www.wechat.com/en/">WeChat</a> handles overload control. Some very impressive numbers and quotes: <em>&ldquo;WeChat’s microservice system accommodates more than 3000 services running on over 20,000 machines in the WeChat business system, and these numbers keep increasing as WeChat is becoming immensely popular… As WeChat is ever actively evolving, its microservice system has been undergoing fast iteration of service updates. For instance, from March to May in 2018, WeChat’s microservice system experienced almost a thousand changes per day on average.&rdquo;</em>. Think about that; 20,000 machines, thousand code changes per day, wow!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://data-artisans.com/blog/stream-processing-introduction-event-time-apache-flink">Stream processing: An Introduction to Event Time in Apache Flink</a>. Apache Flink supports multiple notions of time for stateful stream processing. This post focuses on event time support in Apache Flink.</li>
<li><a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained">Kafka Connect Deep Dive – Converters and Serialization Explained</a>. In this article <a href="https://twitter.com/rmoff">Robin Moffat</a> takes us through how serialization works in Kafka Connectors.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/11/13/creating-a-data-hub-for-your-analytics-with-polybase/">Creating a data hub for your analytics with PolyBase</a>. SQL Server 2019 CTP 2.0 introduces new connectors for PolyBase, and this blog post discusses how the new connectors enable customers to leverage PolyBase for creating a virtual data hub for a wide variety of data sources within the enterprise. Very interesting!<br /></li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/the-ultimate-r-cheatsheet">The Ultimate R Cheatsheet</a>. As the title says, this post points to an impressive cheatsheet for R.</li>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/data-science-for-managers-mindmap">Intro to Data Science for Managers [Mindmap]</a>. Data science is incredibly broad and complex discipline and can be daunting trying to get your head around. So this blog post tries to mind-map the various key data science concepts and techniques in order to make data science easier to grasp.</li>
<li><a href="https://powerbi.microsoft.com/en-us/blog/power-bi-announces-new-ai-capabilities/">Announcing new AI Capabilities for Power BI to make AI Accessible for Everyone</a>. I guess the title of this blog posts says it all. It announces new AI capabilities for Power BI. This is huge, and I can not wait to test it out!</li>
<li><a href="https://www.infoq.com/presentations/uber-big-data-dl-ml">Big Data and Deep Learning: A Tale of Two Systems</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation about how Uber tackles data caching in large-scale deep learning, its ML architecture and discusses how Uber uses Big Data. The presentation concludes by sharing AI use cases.</li>
<li><a href="https://towardsdatascience.com/implementing-facebook-prophet-efficiently-c241305405a3">Implementing Facebook Prophet efficiently</a>. I wrote back in 2017 about Facebook Prophet and how to run it in <a href="/2017/05/20/facebook-prophet-and-microsoft-r-server/">Microsoft R Server</a> as well as <a href="/2017/05/20/facebook-prophet-and-microsoft-r-server/">SQL Server Machine Learning Services</a>. The blog post I link to <a href="https://towardsdatascience.com/implementing-facebook-prophet-efficiently-c241305405a3">here</a> discusses how to optimize Prophet. Very interesting!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 45]]></title>
    <link href="http://nielsberglund.com/2018/11/11/interesting-stuff---week-45/" rel="alternate" type="text/html"/>
    <updated>2018-11-11T08:05:58+02:00</updated>
    <id>http://nielsberglund.com/2018/11/11/interesting-stuff---week-45/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<p>##.NET</p>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/11/07/understanding-the-whys-whats-and-whens-of-valuetask/">Understanding the Whys, Whats, and Whens of ValueTask</a>. The .NET Framework 4 saw the introduction of the <code>System.Threading.Tasks</code> namespace, and with it the <code>Task</code> class. In this post, <a href="https://github.com/stephentoub">Stephen Toub</a>, covers the newer <code>ValueTask</code> and <code>ValueTask&lt;TResult&gt;</code> types.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://data-artisans.com/blog/flink-sql-powerful-querying-of-data-streams">Flink SQL for powerful querying of data streams and data at rest</a>. This post covers Flink SQL, and discusses how we can quickly explore data in streams or data at rest. Very interesting!<br /></li>
<li><a href="https://azure.microsoft.com/en-us/blog/announcing-the-general-availability-of-azure-event-hubs-for-apache-kafka/">Announcing the general availability of Azure Event Hubs for Apache Kafka</a>. This post announces the general availability (GA) of Azure Event Hubs for Apache Kafka. So what does it mean? Well, it means that you can now stream events from applications using the Kafka protocol directly into Azure Event Hubs. Awesome!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/container-runtimes-kubernetes">Who Is Running My Kubernetes Pod? The Past, Present, and Future of Container Runtimes</a>. Container runtime choices are nowadays not only Docker as the &ldquo;Open Container Initiative&rdquo; (OCI) has successfully standardized the concept of a container and container image to guarantee interoperability between runtimes. This <a href="https://www.infoq.com/">InfoQ</a> article looks at the past, present, and future of container engine implementations.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/sqlserverstorageengine/2018/11/07/introducing-scalar-udf-inlining/">Introducing Scalar UDF Inlining</a>. SQL Server 2017 introduced <a href="https://docs.microsoft.com/en-us/sql/relational-databases/performance/intelligent-query-processing?view=sql-server-2017">Intelligent Query Processing</a> which is meant to improve the performance of existing workloads with minimal implementation effort. SQL Server 2019 further expands query processing capabilities, and this blog post discusses Scalar T-SQL UDF Inlining. The inlining of scalar UDF&rsquo;s is a feature to improve the performance of queries that invoke scalar UDFs, where UDF execution is the main bottleneck.</li>
<li><a href="/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/">SQL Server 2019 Big Data Cluster on Azure Kubernetes Service</a>. This is a blog post by &ldquo;yours truly&rdquo;. As the title implies, it discusses how to deploy <strong>SQL Server 2019 Big Data Cluster</strong> to <strong>Azure Kubernetes Service</strong> (AKS).</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Big Data Cluster on Azure Kubernetes Service]]></title>
    <link href="http://nielsberglund.com/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/" rel="alternate" type="text/html"/>
    <updated>2018-11-10T07:58:09+02:00</updated>
    <id>http://nielsberglund.com/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/</id>
    <content type="html"><![CDATA[<p>At the <a href="https://www.microsoft.com/en-us/ignite/agenda"><strong>Microsoft Ignite 2018</strong></a> conference back in September Microsoft released <strong>SQL Server 2019</strong> for public preview, and I wrote two short blog posts about it:</p>

<ul>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a>.</li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a>.</li>
</ul>

<p>What Microsoft also announced was <strong>SQL Server 2019 Big Data Clusters</strong>, which combines the SQL Server database engine, Spark, and HDFS into a unified data platform! Yes, you read that right: SQL Server, Spark, and Hadoop right out of the box. Seeing that both Spark and Hadoop are mainly Linux based, what makes the Big Data Cluster possible is <strong>SQL Server on Linux</strong>. When you deploy a <strong>SQL Server 2019 Big Data Cluster</strong>, you deploy it as containers on <strong>Kubernetes</strong>, where the Kubernetes cluster can be in the cloud, such as <a href="https://azure.microsoft.com/en-us/services/kubernetes-service/"><strong>Azure Kubernetes Service</strong></a>, or on-prem like <a href="https://www.openshift.com/learn/topics/kubernetes/"><strong>Red Hat OpenShift</strong></a> or even on a local dev-box/laptop using <a href="https://kubernetes.io/docs/setup/minikube/"><strong>Minikube</strong></a>.</p>

<p>Initially, this post was about <strong>SQL Server 2019 Big Data Clusters</strong> on Minikube, but after quite a few failed installation attempts I realised I did not have enough memory on my development box, so I decided to try it on <strong>Azure Kubernetes Service</strong> (AKS) instead.</p>

<blockquote>
<p><strong>NOTE:</strong> If you want to run <strong>SQL Server 2019 Big Data Clusters</strong> on Minikube it is suggested that your host machine (Minikube is essentially a VM) has at least 32Gb of memory, and you allocate at least 22Gb to the Minikube VM.</p>
</blockquote>

<p>Since I am a complete novice when it comes to Kubernetes, this post covers both how I set up AKS as well as the deployment of <strong>SQL Server 2019 Big Data Clusters</strong> to AKS, and the post is somewhat a summary of the official <a href="https://docs.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?view=sqlallproducts-allversions">documentation</a>.</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> SQL Server 2019 is in public preview, but the preview does not contain the Big Data Cluster parts. To deploy <strong>SQL Server 2019 Big Data Clusters</strong> you need to be part of the SQL Server 2019 Early Adoption Program, for which you can sign up for <a href="https://sqlservervnexteap.azurewebsites.net/">here</a>.</p>
</blockquote>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>Apart from having an Azure subscription and being enrolled in the SQL Server 2019 EAP, there are a couple of other pre-reqs needed.</p>

<h4 id="python">Python</h4>

<p>If you, like me, are a SQL Server guy, you are probably quite familiar with installing SQL Server instances by mounting an ISO file, and running setup. Well, you can forget all that when you deploy a <strong>SQL Server 2019 Big Data Cluster</strong>. The setup is all done via Python utilities, and various Docker images pulled from a private repository. So, you need Python3. On my box I have Python 3.5, and - according to Microsoft - version 3.7 also works. Make you that you have your Python installation on the path.</p>

<p>When you deploy you use a Python utility: <code>mssqlctl</code>. To download <code>mssqlctl</code>, you need Python&rsquo;s package management system <code>pip</code> installed. During installation you also need a Python HTTP library: <em>Requests</em>. If you do not have it you need to install it:</p>

<pre><code class="language-python">python -m pip install requests
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Installing Python Requests</em></p>

<p>Down below we talk some more about how to download the <code>mssqlctl</code> utility.</p>

<h4 id="azure-cli">Azure CLI</h4>

<p>When working with Azure, you can do it in three ways:</p>

<ul>
<li>Azure Portal</li>
<li>Cloud Shell from within the portal.</li>
<li>Azure CLI.</li>
</ul>

<p>The Azure CLI is Microsoft&rsquo;s cross-platform command-line experience for managing Azure resources, and you install it on your local machine. In this post I mainly use the Azure CLI, so if you want to follow along download it from <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest">here</a>.</p>

<h4 id="kubectl">Kubectl</h4>

<p>The <code>kubernetes-cli</code> (Kubernetes command line tool), gives you an executable <code>kubectl.exe</code> which you use to manage your Kubernetes cluster. Using <code>kubectl</code>, you can inspect cluster resources; create, delete, and update components; etc.</p>

<p>You can install <code>kubectl</code> in different ways, and I installed it from <a href="https://chocolatey.org/packages/kubernetes-cli">Chocolatey</a>: <code>choco install kubernetes-cli</code>.</p>

<h2 id="azure-kubernetes-cluster">Azure Kubernetes Cluster</h2>

<p>Ok, so having &ldquo;sorted&rdquo; out the pre-reqs, let us start with creating an Azure Kubernetes cluster through the Azure CLI.</p>

<h4 id="login">Login</h4>

<p>I start with open Powershell as administrator and from Powershell I log in to Azure:</p>

<pre><code class="language-bash">az login
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Login to Azure</em></p>

<p>When I execute the code in <em>Code Snippet 2</em> a tab opens in my browser, and I see a dialog that asks me to pick an account to log in to Azure with:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Azure Login</em></p>

<p>I choose the account from what I see in <em>Figure 1</em>, and after a little while I see in the browser a success message:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login_success.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Azure Login Success</em></p>

<p>At the same time as the success message in <em>Figure 2</em>, the code in <em>Code Snippet 1</em> returns with information what subscriptions I have access to in Azure:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login_return.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Azure Login Return</em></p>

<p>As we see in <em>Figure 3</em>, I have access to multiple subscriptions, and before we go any further, I set the subscription I target. I look at the <code>id</code> for the subscription I want and:</p>

<pre><code class="language-bash">az account set -s &lt;my_subscription_id&gt;
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Set Azure Context</em></p>

<p>I have now logged in and indicated, as in <em>Code Snippet 3</em>, what subscription to use.</p>

<h4 id="resource-groups">Resource Groups</h4>

<p>Everything we do in Azure is in the context of a resource group. A resource group is a logical group in which Azure resources are deployed and managed, and it exists in a physical location (Azure data center). So when I create a Kubernetes cluster, I need to define what resource group the cluster should belong to. So let us create a resource group:</p>

<pre><code class="language-bash">az group create --name kubernetes-rg --location westeurope
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Create Resource Group</em></p>

<p>In <em>Code Snippet 4</em> we see how I create a group named <code>kubernetes-rg</code>, and I want it in the West Europe region. After I run the code in <em>Code Snippet 4</em>, I get back a JSON blob:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_create_rg.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Azure Create Group Return</em></p>

<p>The JSON blob, as in <em>Figure 4</em>, contains details about my newly created resource group. If I log in to the Azure Portal:</p>

<p><img src="/images/posts/sql_2k19_bdc_portal_resource_group.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Azure Resource Group</em></p>

<p>In the portal, when I click the <em>Resource groups</em> link (outlined in red in <em>Figure 5</em>), I see my newly created resource group outlined in blue.</p>

<h2 id="create-kubernetes-cluster">Create Kubernetes Cluster</h2>

<p>I now have a resource group, and I go on to create the Kubernetes cluster.</p>

<blockquote>
<p><strong>NOTE:</strong> You do not need to create a new resource group as such. When you create the Kubernetes cluster, you can create it in an existing group.</p>
</blockquote>

<p>To create the Kubernetes cluster I continue to use the Azure CLI, and I use the <code>az aks create</code> command. The command has quite a few parameters, which you can read about <a href="https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create">here</a>, but I use only a few of them:</p>

<ul>
<li><code>--name</code>: name of the cluster.</li>
<li><code>--resource-group</code>: name of the resource group.</li>
<li><code>--disable-rbac</code>: disables Kubernetes Role-Based Access Control.</li>
<li><code>--generate-ssh-keys</code>: if no SSH keys exist, generate both public and private key files.</li>
<li><code>--node-vm-size</code>: the size of the VM&rsquo;s used for the nodes in the Kubernetes cluster. For a <strong>SQL Server 2019 Big Data Cluster</strong> you need a VM with at least 32Gb of memory. You can see a list of VM sizes and their features in the portal <a href="https://portal.azure.com/#create/microsoft.aks">here</a>. I use &ldquo;Standard E4s_v3&rdquo;.</li>
<li><code>--node-count</code>: number of nodes in the Kubernetes node pool. I use 3.</li>
<li><code>--kubernetes-version</code>: the version of Kubernetes to use for creating the cluster. The <strong>SQL Server 2019 Big Data Cluster</strong> requires at minimum the Kubernetes v1.10 version.</li>
</ul>

<p>Before I create the cluster, let us talk a little bit about <code>--node-vm-size</code> and <code>--node-count</code> as they are somewhat related to each other. In addition to defining how much memory a VM has the <code>--node-vm-size</code> also defines the number of virtual CPU&rsquo;s (VCPUS) for the VM. The number of VCPUS controls how many data disks the VM has, (normally it is 2 disks per VCPU). The number of disks per VM is important as the <strong>SQL Server 2019 Big Data Cluster</strong> mounts quite a lot of storage on individual disks and with too few disks the mount operations fail. To get more disks you either increase the VM size or the node count, and that is the relation between <code>--node-vm-size</code> and <code>--node-count</code>.</p>

<p>For a &ldquo;default&rdquo; <strong>SQL Server 2019 Big Data Cluster</strong> deployment around 20 disks are required. So if I choose the &ldquo;Standard E4s_v3&rdquo; VM as vm size, I want at least 3 nodes. With this in mind the code to create a Kubernetes cluster looks like so:</p>

<pre><code class="language-bash">az aks create --name sqlkubecluster \
--resource-group kubernetes-rg \
--disable-rbac \
--generate-ssh-keys \
--node-vm-size Standard_E4s_v3 \
--node-count 3 \
--kubernetes-version 1.10.8
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Kubernetes Cluster</em></p>

<p>In <em>Code Snippet 5</em> we see how:</p>

<ul>
<li>I want to create a cluster with the name <code>sqlkubecluster</code>.</li>
<li>I want the cluster in the <code>kubernetes-rg</code> resource group.</li>
<li>I do not want to use Kubernetes Role-Based Access Control.</li>
<li>I want SSH keys created.</li>
<li>I want the VM&rsquo;s to be &ldquo;Standard E4s_v3&rdquo;,</li>
<li>I want 3 nodes.</li>
<li>Finally I want the Kubernetes version to be 1.10.8.</li>
</ul>

<p>When I execute the code I see something like so:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_create_cluster.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Run Create Kubernetes Cluster</em></p>

<p>What we see in <em>Figure 6</em> runs for several minutes and when it completes I receive a JSON blob with information about the cluster.</p>

<p>I mentioned above how <code>kubectl</code> is used to manage your Kubernetes cluster, and now when the cluster is created, we need to configure <code>kubectl</code> to connect to the cluster. To do this, you use the <code>az aks get-credentials</code> command like so:</p>

<pre><code class="language-bash">az aks get-credentials --resource-group=kubernetes-rg --name sqlkubecluster
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Get Credentials</em></p>

<p>We see in <em>Code Snippet 6</em> how I pass in the name of the resource group and cluster as parameters and when I execute:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_credentials.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Get Credentials</em></p>

<p>The <code>config</code> file we see outlined in red in <em>Figure 7</em> holds, among other things, the keys for the Kubernetes cluster. To ensure that I can connect to the cluster I call <code>kubectl get nodes</code>, and I see some information about the cluster nodes.</p>

<h4 id="dashboard-namespaces">Dashboard &amp; Namespaces</h4>

<p>To monitor and manage a Kubernetes cluster you do not have to rely on <code>kubectl</code> solely, as you can also use the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Kubernetes Dashboard</a>. In Azure you access the Dashboard by the <code>az aks browse</code> command, and - as with <code>get-credentials</code> - you pass in the names of the resource group and cluster: <code>az aks browse --resource-group kubernetes-rg --name sqlkubecluster</code>, and a new tab opens in your browser:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_dashboard.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Kubernetes Dashboard</em></p>

<p>In <em>Figure 8</em> we see the dashboard right after I created the VM and the Kubernetes cluster. Notice <em>namespaces</em>, outlined in red. <em>Namespcaces</em> help different projects, teams, or customers to share a Kubernetes cluster, and when you deploy to Kubernetes, you deploy into a namespace. To see what <em>namespaces</em> exist in a cluster you execute: <code>kubectl get namespaces</code>. When I do it at this stage I see:</p>

<p><img src="/images/posts/sql_2k19_bdc_kubectl_namespaces.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Kubernetes Namespaces</em></p>

<p>When we look at <em>Figure 9</em> we see following <em>namespaces</em>:</p>

<ul>
<li><code>default</code>: A default, (duh), namespace to hold the default set of Pods, Services, and Deployments used by the cluster.</li>
<li><code>kube-public</code>: A namespace readable by everyone for public <a href="https://unofficial-kubernetes.readthedocs.io/en/latest/tasks/configure-pod-container/configmap/">ConfigMap&rsquo;s</a>.</li>
<li><code>kube-system</code>: A namespace for objects created by the Kubernetes system.</li>
</ul>

<p>So, coming back to Dashboard: when we want to monitor a deployment with Dashboard, we monitor a specific namespace. Enough of this, let us deploy!</p>

<h2 id="deploy-sql-server-2019-big-data-cluster">Deploy SQL Server 2019 Big Data Cluster</h2>

<p>I mentioned above that when we deploy a <strong>SQL Server 2019 Big Data Cluster</strong> we deploy using a Python utility: <code>mssqlctl</code>. So what we need to do is to download and install the utility:</p>

<pre><code class="language-bash">pip3 install \
   --index-url https://private-repo.microsoft.com/python/ctp-2.0 \
     mssqlctl
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Download and Install mssqlctl</em></p>

<p>We download and install <code>mssqlctl</code> from a Microsoft repository as we see in <em>Code Snippet 7</em>. After download the source is located in <code>&lt;python_path&gt;\lib\site-packages</code> and the executable - <code>mssqlctl.exe</code> - is at: <code>&lt;python_path&gt;\Scripts</code>.</p>

<h4 id="environment-variables">Environment Variables</h4>

<p>When you deploy the <strong>SQL Server 2019 Big Data Cluster</strong> using <code>mssqlctl</code> you customise the cluster configuration via environment variables read by <code>mssqlctl</code>. To see all available environment variables you go <a href="https://docs.microsoft.com/en-us/sql/big-data-cluster/deployment-guidance?view=sqlallproducts-allversions">here</a>. Below I list the ones I use:</p>

<ul>
<li>SET ACCEPT_EULA=Y - to accept the SQL Server license agreement.</li>
<li>SET CLUSTER_PLATFORM=aks - the Kubernetes platform you deploy to: Azure - <code>aks</code>, Kubernetes - <code>kubernetes</code>, Minikube - <code>minikube</code>.</li>
<li>SET CONTROLLER_USERNAME=admin - the user name for the cluster administrator. You can set this to anything.</li>
<li>SET CONTROLLER_PASSWORD=<some_secret_password> - the password for the cluster administrator.</li>
<li>SET KNOX_PASSWORD=<some_secret_password> - the password for the Knox user. <a href="https://knox.apache.org/">Knox</a> is an application gateway for interacting with the REST API&rsquo;s and UI&rsquo;s of Apache Hadoop deployments.</li>
<li>SET MSSQL_SA_PASSWORD=<some_secret_password> - the <code>sa</code> password for the master SQL instance. It needs to meet password complexity requirements.</li>
<li>SET DOCKER_REGISTRY=private-repo.microsoft.com - the registry for the images being pulled.</li>
<li>SET DOCKER_REPOSITORY=mssql-private-preview - the repository within the registry.</li>
<li>SET DOCKER_USERNAME=<docker_username> - user name to access the images. You get this when you sign up for the <a href="https://sqlservervnexteap.azurewebsites.net/">EAP</a>.</li>
<li>SET DOCKER_PASSWORD=<some_secret_password> - the password for the above user. You get this when you sign up for the <a href="https://sqlservervnexteap.azurewebsites.net/">EAP</a>.</li>
<li>SET DOCKER_EMAIL=<email_for_the_docker_user> - the email associated with the registry. You get this when you sign up for the <a href="https://sqlservervnexteap.azurewebsites.net/">EAP</a>.</li>
<li>SET DOCKER_PRIVATE_REGISTRY=1 - this has to be set to 1.</li>
</ul>

<p>Before you deploy the environment variables, need to be set, and if you are on Windows, you need to do it from a command prompt (not Powershell). Instead of having to enter these variables individually, I have a <code>bat</code> file I run before deploying: <code>set_env_variables_aks.bat</code>.</p>

<h4 id="create-cluster">Create Cluster</h4>

<p>After I have set the variables I create the cluster with the <code>mssqlctl</code> command, and I do it from the command prompt (not Powershell):</p>

<pre><code class="language-bash">mssqlctl create cluster sqlbigdata1 -v
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Create Big Data Cluster</em></p>

<p>Looking at <em>Code Snippet 8</em> we see how I call <code>mssqlctl</code> to create a <strong>SQL Server 2019 Big Data Cluster</strong>, and I want to create it in a namespace called <code>sqlbigdata1</code>. I use the <code>-v</code> flag (as in verbose) to get debug output. When I execute the code I see something like so:</p>

<p><img src="/images/posts/sql_2k19_bdc_create_cluster.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Create Big Data Cluster</em></p>

<p>What we see in <em>Figure 10</em> is how we have started to create the main controller and its pod. We also see a note (outlined in red) saying that it can take quite a while to create the cluster. To monitor the process you can use Dashboard:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_dashboard2.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Kubernetes Dashboard</em></p>

<p>In <em>Figure 11</em> we see an overview of the <code>sqlbigdata1</code> namespace. You may see errors in the dashboard, but you can ignore them initially. In addition to Dashboard to monitor progress you can use <code>kubectl</code> commands, for example: <code>kubectl get pods -n sqlbigdata1</code>:</p>

<p><img src="/images/posts/sql_2k19_bdc_kubectl1.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Pods being Created</em></p>

<p>The <strong>SQL Server 2019 Big Data Cluster</strong> exposes its own dashboard; the <em>Cluster Administration Portal</em>, which we can use to monitor the deployment as well. The portal becomes available as soon as the controller is up, and in a running state. The portal is exposed at the endpoint for the <code>service-proxy-lb</code> (proxy load balancer) service. To find the IP address, you call: <code>kubectl get svc service-proxy-lb -n &lt;name of your cluster&gt;</code>:</p>

<p><img src="/images/posts/sql_2k19_bdc_cluster_admin_ip.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Endpoint for Cluster Admin</em></p>

<p>In <em>Figure 13</em> we see how <code>svc service-proxy-lb</code> has an external IP of <code>13.94.174.28</code>, and it exposes two ports: <code>30777</code> and <code>31826</code>. The port for the portal is <code>30777</code>, and when I browse there, I first need to log in with the <code>CONTROLLER_USERNAME</code> (admin in my case) and <code>CONTROLLER_PASSWORD</code>. After login, I come to the <em>Overview</em> page. I then click on the <em>Deployment</em> link outlined in red:</p>

<p><img src="/images/posts/sql_2k19_bdc_cluster_admin2.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Deployment Progress</em></p>

<p>What we see in <em>Figure 14</em> is the progress of the <strong>SQL Server 2019 Big Data Cluster</strong> deployment, and we see that it is still in progress: yellow triangle by the namespace, (outlined in blue).</p>

<p>Eventually, the deployment finishes, and we know that either: by seeing that the triangle in <em>Figure 14</em> is now a green circle, or by the output from command line:</p>

<pre><code class="language-bash">2018-11-07 09:04:52.0147 UTC | INFO | Data pool is ready...
2018-11-07 09:04:52.0148 UTC | INFO | Storage pool is ready...
...
2018-11-07 09:06:55.0073 UTC | INFO | Compute pool is ready...
...
2018-11-07 09:07:36.0155 UTC | INFO | Cluster state: Ready
2018-11-07 09:07:36.0155 UTC | INFO | Cluster deployed successfully.
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Cluster Deployed Successfully</em></p>

<p>We see in <em>Code Snippet 9</em> how <code>mssqlctl</code> reports that the various pools are ready, followed by successful cluster deployment.</p>

<h4 id="connection-endpoints">Connection Endpoints</h4>

<p>So far, so good - but what do we do now? We know that a <strong>SQL Server 2019 Big Data Cluster</strong> consists both of a SQL Server master instance, as well as Hadoop/Spark, but where do we find them?</p>

<p>As with the portal, the endpoints are service load balancers endpoints. The service load balancer for the SQL Server master instance is: <code>service-master-pool-lb</code> and for Hadoop/Spark it is: <code>service-security-lb</code>. To retrieve the endpoints I call:</p>

<pre><code class="language-bash"># SQL Server master instance
kubectl get service service-master-pool-lb \
                 -o=custom-columns=&quot;IP:.status.loadBalancer.\
                 ingress[0].ip,PORT:.spec.ports[0].port&quot; \
                 -n sqlbigdata1

# Hadoop/Spark
kubectl get service service-security-lb \
                 -o=custom-columns=&quot;IP:.status.loadBalancer.\
                 ingress[0].ip,PORT:.spec.ports[0].port&quot; \
                 -n sqlbigdata1
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Get Endpoints</em></p>

<p>In <em>Code Snippet 10</em> we see how I customize what I want to be returned from the <code>get service</code> calls to only to return IP addresses and ports. With these endpoints, I can now connect to my <strong>SQL Server 2019 Big Data Cluster</strong>. Once again, to connect to the SQL Server master instance (databases), you use the <code>service-master-pool-lb</code> endpoint, and to connect to Hadoop/Spark, the <code>service-security-lb</code> endpoint is what you use.</p>

<p>The user names and passwords are:</p>

<ul>
<li>SQL Server master instance: <code>sa</code> as user name, and <code>MSSQL_SA_PASSWORD</code> as password.</li>
<li>Hadoop / Spark: <code>root</code> as user name, and <code>KNOX_PASSWORD</code> as password.</li>
</ul>

<p>In a future post I look at what we can do with <strong>SQL Server 2019 Big Data Cluster</strong>.</p>

<h2 id="summary">Summary</h2>

<p>In this post we looked at how to install <strong>SQL Server 2019 Big Data Cluster</strong> on <em>Azure Kubernetes Service</em> (AKS). We saw how to:</p>

<ul>
<li>Create a new Azure resource group using Azure CLI.</li>
<li>Create a Kubernetes cluster in that resource group.</li>
</ul>

<p>We discussed the size requirements for the VM&rsquo;s in the cluster, and mentioned they needed at least 32Gb of RAM. We also need quite a few disks to mount storage on, so the node count is important.</p>

<p>The actual deployment of a <strong>SQL Server 2019 Big Data Cluster</strong> is done using a Python utility <code>mssqlctl</code>. During the deployment process we can monitor the progress via:</p>

<ul>
<li><code>kubectl</code> commands.</li>
<li>the Kubernetes dashboard.</li>
<li>the <strong>SQL Server 2019 Big Data Cluster</strong>&rsquo;s <em>Cluster Administration Portal</em>.</li>
</ul>

<p>Access to the various services in a <strong>SQL Server 2019 Big Data Cluster</strong> is through service load balancers and their external IP addresses and ports:</p>

<ul>
<li>Cluster Administration Portal: <code>service-proxy-lb</code>.</li>
<li>SQL Server master instance: <code>service-master-pool-lb</code>.</li>
<li>Hadoop/Spark: <code>service-security-lb</code>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 44]]></title>
    <link href="http://nielsberglund.com/2018/11/04/interesting-stuff---week-44/" rel="alternate" type="text/html"/>
    <updated>2018-11-04T18:54:29+02:00</updated>
    <id>http://nielsberglund.com/2018/11/04/interesting-stuff---week-44/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/local-testing-with-live-data-means-faster-development-with-azure-stream-analytics-2/">Local testing with live data means faster development with Azure Stream Analytics</a>. An Azure Stream Analytics (ASA) announcement of a new feature which lets you test your ASA queries locally while using live data streams from cloud sources such as Azure Event Hubs, IoT Hub or Blob storage!</li>
<li><a href="https://www.confluent.io/blog/atm-fraud-detection-apache-kafka-ksql">ATM Fraud Detection with Apache Kafka and KSQL</a>. A blog post by <a href="https://twitter.com/rmoff">Robin Moffat</a> discussing how, by using Kafka and KSQL, you can take a stream of inbound ATM transactions and easily set up an application to detect transactions that look fraudulent. Awesome!</li>
<li><a href="https://blog.acolyer.org/2018/10/29/noria-dynamic-partially-stateful-data-flow-for-high-performance-web-applications/">Noria: dynamic, partially-stateful data-flow for high-performance web applications</a>. <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white-paper about Noria: a new streaming data-flow system designed to act as a fast storage backend for read-heavy web applications. It acts like a database but precomputes and caches relational query results so that reads are blazingly fast.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/11/02/the-fuzzylog-a-partially-ordered-shared-log/">The FuzzyLog: a partially ordered shared log</a>. Another white-paper dissected by <a href="https://twitter.com/adriancolyer">Adrian</a>. This time it is about FuzzyLog and the FuzzyLog abstraction which extends the shared log approach to partial orders, allowing applications to scale linearly without sacrificing transactional guarantees, and switch seamlessly between these guarantees when the network partitions and heals.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>I am still trying to get to grips with <strong>SQL Server 2019 Big Data Cluster</strong>, and now I better sort it out as I on Wednesday (November 7), is supposed to give a presentation about <a href="https://www.meetup.com/Azure-Transformation-Labs/events/255690875/?rv=ea1_v2&amp;_xtd=gatlbWFpbF9jbGlja9oAJGRjZmM2OTBjLWEwNTgtNDRjZi1iOWQyLWI5YzMyZTM1YmIzMg">Building a SQL 2019 Big Data Cluster on Azure Kubernetes Service</a>. If you are in Durban, please come by and say Hi!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 43]]></title>
    <link href="http://nielsberglund.com/2018/10/28/interesting-stuff---week-43/" rel="alternate" type="text/html"/>
    <updated>2018-10-28T07:44:55+02:00</updated>
    <id>http://nielsberglund.com/2018/10/28/interesting-stuff---week-43/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/announcing-automated-ml-capability-in-azure-machine-learning/">Announcing automated ML capability in Azure Machine Learning</a>. Somehow I must have missed this post announcing <strong>Azure Automated Machine Learning</strong>. What is it? Well, it is a way for the <a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">Azure Machine Learning Service</a> to automatically pick an algorithm for you, and generate a model from it. It sounds really interesting, and this is something I need to take a look at.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><p><a href="https://www.confluent.io/blog/kafka-summit-san-francisco-2018-roundup">That’s a Wrap! Kafka Summit San Francisco 2018 Roundup</a>. The San Francisco <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2018/">Kafka Summit</a> ran October 16 - 17, and this blog post is a summary of the conference. It also has links to some interesting sessions, and out of those, these are my three favorites:</p>

<ul>
<li><a href="https://www.confluent.io/kafka-summit-sf18/zen-and-the-art-of-streaming-joins">Zen and the Art of Streaming Joins—The What, When and Why</a>.</li>
<li><a href="https://www.confluent.io/kafka-summit-sf18/kafka-security-101-and-real-world-tips">Kafka Security 101 and Real-World Tips</a>.</li>
<li><a href="https://www.confluent.io/kafka-summit-sf18/breaking-down-a-sql-monolith">Breaking Down a SQL Monolith with Change Tracking, Kafka and KStreams/KSQL</a>.</li>
</ul></li>

<li><p><a href="https://data-artisans.com/blog/stateful-stream-processing-apache-flink-state-backends">Stateful Stream Processing: Apache Flink State Backends</a>. This post explores stateful stream processing and more precisely the different state backends available in Apache Flink. It presents the 3 state backends of Apache Flink, their limitations and when to use each of them depending on case-specific requirements.</p></li>

<li><p><a href="https://www.confluent.io/blog/apache-kafka-kubernetes-could-you-should-you">Apache Kafka on Kubernetes – Could you? Should you?</a>. This is a post discussing whether you should run Kafka on Kubernetes or not. At <a href="/derivco">work</a>, we are in the process of rolling out our first Kafka deployments, (not on Kubernetes), and this post definitely gives us &ldquo;food for thought&rdquo;.</p></li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Yeah, I kind of ask myself that question as well (what am I doing): at the moment I have a hard time getting any blog posts out, as I am busy at work as well as trying to get to grips with <strong>SQL Server 2019 Big Data Clusters</strong>. I hope to be able to publish something in a week (or twos) time.</p>

<p>In the meantime, if you are interested in <strong>SQL Server 2019</strong>, go and have a read at these two posts:</p>

<ul>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a>.</li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 42]]></title>
    <link href="http://nielsberglund.com/2018/10/21/interesting-stuff---week-42/" rel="alternate" type="text/html"/>
    <updated>2018-10-21T19:19:43+02:00</updated>
    <id>http://nielsberglund.com/2018/10/21/interesting-stuff---week-42/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/15/guidance-for-library-authors/">Guidance for library authors</a>. This blog post announces the publication of the <strong>.NET Library Guidance</strong>. It’s brand new set of articles for .NET developers who want to create high-quality libraries for .NET.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/16/automating-release-notes-with-azure-functions/">Automating Release Notes with Azure Functions</a>. This post is a walk through how Azure Functions and Azure Blob Storage can help generate release notes.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://eng.uber.com/uber-big-data-platform/">Uber’s Big Data Platform: 100+ Petabytes with Minute Latency</a>. This article looks in-depth at Uber&rsquo;s Hadoop platform and how the platform allows for analysis of over 100 petabytes of data with minimal latency.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/how-to-choose-a-machine-learning-model-some-guidelines">How to Choose a Machine Learning Model – Some Guidelines</a>. This blog post explores some broad guidelines for selecting machine learning models.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/event-driven-2-0">Event Driven 2.0</a>. In this article <a href="https://twitter.com/benstopford">Ben Stopford</a> discusses the next generation of event-driven architecture.</li>
<li><a href="https://www.youtube.com/watch?v=HeNegOzjnJY&amp;feature=em-uploademail">Jay Kreps | Kafka Summit SF 2018 Keynote (Kafka and Event-Oriented Architecture)</a>. The San Francisco <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2018/">Kafka Summit 2018</a> was held October 16 - 17, and this video is Jay Kreps keynote.</li>
<li><a href="https://www.youtube.com/watch?v=v2RJQELoM6Y&amp;feature=em-uploademail">Martin Kleppmann | Kafka Summit SF 2018 Keynote (Is Kafka a Database?)</a>. Another <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2018/">Kafka Summit 2018</a> keynote video. This is <a href="https://twitter.com/martinkl">Martin Kleppmann</a> comparing Kafka to databases.<br /></li>
<li><a href="https://www.infoq.com/articles/apache-kafka-best-practices-to-optimize-your-deployment">Apache Kafka: Ten Best Practices to Optimize Your Deployment</a>. An <a href="https://www.infoq.com/">InfoQ</a> article discussing the latest Kafka best practices for developers to manage the data streaming platform more effectively. Best practices include log configuration, proper hardware usage, Zookeeper configuration, replication factor, and partition count.</li>
<li><a href="https://data-artisans.com/blog/watermarks-in-apache-flink-made-easy">Watermarks in Apache Flink Made Easy</a>. This blog post looks at how watermarks work in Flink.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 41]]></title>
    <link href="http://nielsberglund.com/2018/10/14/interesting-stuff---week-41/" rel="alternate" type="text/html"/>
    <updated>2018-10-14T08:03:37+02:00</updated>
    <id>http://nielsberglund.com/2018/10/14/interesting-stuff---week-41/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/a-fast-serverless-big-data-pipeline-powered-by-a-single-azure-function/">A fast, serverless, big data pipeline powered by a single Azure Function</a>. This is a blog post about how to use Azure Serverless functions to build highly performant data pipelines. At work, we are looking at implementing something similar.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><p><a href="https://muratbuffalo.blogspot.com/2018/10/everything-is-broken.html">Everything is broken</a>. This is a very cool post by <a href="https://twitter.com/muratdemirbas">Murat</a> where he lists some very relevant quotes/statements from a recent <a href="https://www.meetup.com/Everything-Is-Broken/events/251899676/">Everything Is Broken</a> meetup he attended. Some of the quotes I particularly liked:</p>

<ul>
<li><em>Without observability you don&rsquo;t have chaos engineering, you have a chaos.</em></li>
<li><em>You don&rsquo;t know what you don&rsquo;t know, so dashboards are very limited utility. Dashboards are only for anticipated cases: every dashboard is an artifact of past failures. There are too many dashboards, and they are too slow.</em></li>

<li><p><em>prerequisites for chaos engineering:</em></p>

<ol>
<li><em>monitoring &amp; observability</em></li>
<li><em>on-call &amp; incident management</em></li>
<li><em>know the cost of your downtime per hour (British Airlines&rsquo;s 1 day outage costed $150 millon)</em></li>
</ol></li>

<li><p><em>How to choose a chaos experiment?</em></p>

<ul>
<li><em>identify top 5 critical systems</em></li>
<li><em>choose 1 system</em></li>
<li><em>whiteboard the system</em></li>
<li><em>select attack: resource/state/network</em></li>
<li><em>determine scope</em></li>
</ul></li>
</ul></li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/08/announcing-ml-net-0-6-machine-learning-net/">Announcing ML.NET 0.6 (Machine Learning .NET)</a>. Microsoft just released ML.NET 0.6, and this post highlights some of the new enhancements.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/@simon.aubury/machine-learning-kafka-ksql-stream-processing-bug-me-when-ive-left-the-heater-on-bd47540cd1e8">Machine learning &amp; Kafka KSQL stream processing — bug me when I’ve left the heater on</a>. I like this post as it combines two of my favorite topics: Streaming and Machine Learning. So anyway, the post is about how you can, by using Kafka and Machine Learning, monitor household power usage and alert when something out of the ordinary occurs.</li>
<li><a href="https://data-artisans.com/blog/an-introduction-to-acid-guarantees-and-transaction-processing">An introduction to ACID guarantees and transaction processing</a>. A while ago dataArtisans introduced serializable, distributed ACID transactions directly on data streams in Flink. This post here talks about the foundations of the capability.</li>
<li><a href="https://www.confluent.io/blog/ksql-recipes-available-now-stream-processing-cookbook">KSQL Recipes Available Now in the Stream Processing Cookbook</a>. A post which introduces the &ldquo;KSQL Cookbook&rdquo;: a collection of &ldquo;recipes&rdquo; designed to help people build event-driven, real-time systems.</li>
<li><a href="https://data-artisans.com/blog/how-apache-flink-manages-kafka-consumer-offsets">How Apache Flink manages Kafka consumer offsets</a>. This post explains with a step-by-step example of how Apache Flink works with Apache Kafka to ensure that records from Kafka topics are processed with exactly-once guarantees.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 40]]></title>
    <link href="http://nielsberglund.com/2018/10/07/interesting-stuff---week-40/" rel="alternate" type="text/html"/>
    <updated>2018-10-07T09:34:44+02:00</updated>
    <id>http://nielsberglund.com/2018/10/07/interesting-stuff---week-40/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/04/update-on-net-core-3-0-and-net-framework-4-8/">Update on .NET Core 3.0 and .NET Framework 4.8</a>. A blog post from the .NET engineering team, where they talk about the future of the .NET Framework and .NET Core. I wonder if this post was prompted by speculations recently about the future of the .NET Framework, where there were questions whether the .NET Framework 4.8 would be the last version, and all development would be concentrated on .NET Core.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/enabling-real-time-data-warehousing-with-azure-sql-data-warehouse/">Enabling real-time data warehousing with Azure SQL Data Warehouse</a>. This post is an announcement how <a href="https://www.striim.com/">Striim</a> now fully supports SQL Data Warehouse as a target for Striim for Azure. Striim is a system which enables continuous non-intrusive performant ingestion of enterprise data from a variety of sources in real time.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/event-streaming-new-big-thing-finance">Is Event Streaming the New Big Thing for Finance?</a>. An excellent blog post by <a href="https://twitter.com/benstopford">Ben Stopford</a> where he discusses the use of event streaming in the financial sector.</li>
<li><a href="https://www.confluent.io/blog/troubleshooting-ksql-part-2">Troubleshooting KSQL – Part 2: What’s Happening Under the Covers?</a>. The second post by <a href="https://twitter.com/rmoff">Robin Moffat</a> about debugging of KSQL. In this post - Robin, as the title says, goes under the covers to figure out what happens with KSQL queries.</li>
<li><a href="https://data-artisans.com/blog/6-things-to-consider-when-defining-your-apache-flink-cluster-size">6 things to consider when defining your Apache Flink cluster size</a>.  This post discusses how to plan and calculate a Flink cluster size. In other words; how to define the number of resources you need to run a specific Flink job.</li>
</ul>

<h2 id="ms-ignite">MS Ignite</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/2018/10/02/syllabuck-ignite-2018-conference/">Syllabuck: Ignite 2018 Conference</a>. A great list of MS Ignite sessions that <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a> found interesting! Now I know what to do in my spare time!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/10/03/customized-regression-model-for-airbnb-dynamic-pricing/">Customized regression model for Airbnb dynamic pricing</a>. This post by <a href="https://twitter.com/adriancolyer">Adrian</a> is about a white-paper which details the methods that Airbnb use to suggest prices to listing hosts.</li>
<li><a href="https://towardsdatascience.com/cleaning-and-preparing-data-in-python-494a9d51a878">Cleaning and Preparing Data in Python</a>. A post which lists Python methods and functions that helps to clean and prepare data.</li>
<li><a href="https://www.microsoft.com/en-us/research/blog/the-microsoft-infer-net-machine-learning-framework-goes-open-source/">The Microsoft Infer.NET machine learning framework goes open source</a>. A blog post from Microsoft Research, in which they announce the open-sourcing of <a href="https://dotnet.github.io/infer/">Infer.NET</a>. Is anyone else but me somewhat confused about the various data science frameworks that Microsoft has?</li>
<li><a href="https://towardsdatascience.com/how-to-build-a-simple-recommender-system-in-python-375093c3fb7d">How to build a Simple Recommender System in Python</a>. A blog post which discusses what a recommender system is and how you can use Python to build one.</li>
</ul>

<h2 id="what-is-niels-doing-wind">What Is Niels Doing (WIND)</h2>

<p>That is a good question! As you know, I wrote two blog posts about SQL Server 2019:</p>

<ul>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a></li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a></li>
</ul>

<p>My plan was to relatively quickly follow up those two posts with a third post how to run <strong>SQL Server Machine Learning Services</strong> on <strong>SQL Server 2019 on Linux</strong>, and do it inside a Docker container. After having spent some time trying to get it to work, (with no luck), I gave up and contacted a couple of persons in MS asking for help. The response was that, right now in <strong>SQL Server 2019 on Linux CTP 2.0</strong>, you cannot do it - bummer! The functionality will be in a future release.</p>

<p>I am now reworking the post I had started on to cover <strong>SQL Server Machine Learning Services</strong> in an <strong>Ubuntu</strong> based <strong>SQL Server 2019 on Linux</strong>. I should be able to publish something within a week or two.</p>

<p>I am also working on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series (still). Right now I have no idea when I can publish it - Sorry!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 39]]></title>
    <link href="http://nielsberglund.com/2018/09/30/interesting-stuff---week-39/" rel="alternate" type="text/html"/>
    <updated>2018-09-30T13:13:43+02:00</updated>
    <id>http://nielsberglund.com/2018/09/30/interesting-stuff---week-39/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://adamsitnik.com/ETW-Profiler/">Profiling .NET Code with BenchmarkDotNet</a>. If you want to benchmark your .NET code, you probably use <a href="https://benchmarkdotnet.org/">BenchMarkDotNet</a> (if you do not, you should). The man behind BenchMarkDotNet is <a href="https://twitter.com/SitnikAdam">Adam Sitnik</a>, and in the linked blog post he announces how you, soon, can use the EtwProfiler to profile benchmarked code! Very cool!</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/09/26/the-design-and-implementation-of-modern-column-oriented-database-systems/">The design and implementation of modern column-oriented database systems</a>. In this post, <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white paper about column-oriented databases. Having worked a little bit with SQL Server&rsquo;s column store indexes, it is very cool to get the &ldquo;lowdown&rdquo; on the design behind it.</li>
</ul>

<h2 id="azure-cloud">Azure Cloud</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/azure-databricks-delta-in-preview-9-regions-added-and-other-exciting-announcements/">Azure Databricks – Delta in preview, 9 regions added, and other exciting announcements</a>. A blog post announcing that Azure Databricks Delta is available in preview. This is very interesting since I have been &ldquo;chomping at the bits&rdquo;, to do some tests with Databricks Delta.</li>
<li><a href="https://azure.microsoft.com/en-us/blog/spark-debugging-and-diagnosis-toolset-for-azure-hdinsight/">Spark Debugging and Diagnosis Toolset for Azure HDInsight</a>. This post is another announcement from Microsoft. This time it is how <strong>Spark Diagnosis Toolset for HDInsight</strong> is now available in preview. The toolset allows you to identify low parallelization, to detect data skew and run data skew analysis, and quite a lot more.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-presence-detection-apache-kafka-aws">Real-Time Presence Detection at Scale with Apache Kafka on AWS</a>. This post discusses how <a href="https://www.zenreach.com/">Zenreach</a> has implemented a framework for real-time presence detection, using Kafka Streams.</li>
<li><a href="https://data-artisans.com/blog/state-ttl-for-apache-flink-how-to-limit-the-lifetime-of-state">State TTL for Apache Flink: How to Limit the Lifetime of State</a>. Instead of me summarising the post, I shamelessly copy the opening paragraph: <em>A common requirement for many stateful streaming applications is the ability to control how long application state can be accessed (e.g., due to legal regulations like GDPR) and when to discard it. This blog post introduces the state time-to-live (TTL) feature that was added to Apache Flink with the 1.6.0 release</em>. It is very, very interesting. I need to start to play around with Flink!</li>
<li><a href="https://www.confluent.io/blog/troubleshooting-ksql-part-1">Troubleshooting KSQL – Part 1: Why Isn’t My KSQL Query Returning Data?</a>. The obligatory Kafka link. The post is the first in a series how to troubleshoot KSQL. This and future posts in the series is, and, will be required reading for our Kafka team!</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/azure-data-studio-for-sql-server/">Azure Data Studio for SQL Server</a>. A post by <a href="https://twitter.com/vickyharp">Vicky Harp</a>. Vicky is Principal Program Manager Lead at Microsoft for SQL Server tooling, and in the post, she introduces <strong>Azure Data Studio</strong> (the artist formerly known as SQL Operations Studio). Azure Data Studio is a new cross-platform desktop environment for both on-premises and cloud data platforms on Windows, MacOS, and Linux.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/24/sql-server-2019-preview-combines-sql-server-and-apache-spark-to-create-a-unified-data-platform/">SQL Server 2019 preview combines SQL Server and Apache Spark to create a unified data platform</a>. An announcement by Microsoft how SQL Server 2019 comes with support for both Spark as well as Hadoop File System (HDFS). We do live in exciting times!</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/introducing-microsoft-sql-server-2019-big-data-clusters/">Introducing Microsoft SQL Server 2019 Big Data Clusters</a>. This post builds on top of the post above. It discusses how we can create big data clusters utilising the support in SQL Server 2019 of Spark and HDFS.</li>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a>. A post by yours truly. I do a quick look at what is new in SQL Server 2019, and I especially look at the Java language extension.</li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a>. Another post my myself. Since SQL Server 2019 for Linux now have support for SQL Server Machine Learning Services, I want to have a look at how it works. For that I obviously need it installed and I decided to install it as a Docker for Windows container. The post walks through what I did to get it installed. The post also discusses Azure Data Studio briefly.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 for Linux in Docker on Windows]]></title>
    <link href="http://nielsberglund.com/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/" rel="alternate" type="text/html"/>
    <updated>2018-09-29T12:06:09+02:00</updated>
    <id>http://nielsberglund.com/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/</id>
    <content type="html"><![CDATA[<p>By the time I publish this blog post <a href="https://www.microsoft.com/en-us/ignite">MS Ignite</a> is over. During Ignite, Microsoft announced quite a few new things, amongst them <strong>SQL Server 2019</strong> with a whole lot of new features and functionality.</p>

<p>I touched briefly on some of them in my <a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a> post. A couple of things that caught my eye were that <strong>SQL Server 2019 for Linux</strong> now supports In-Database analytics, what we know as <strong>SQL Server Machine Learning Services</strong> (R and Python), as well as the Java language extension.</p>

<blockquote>
<p><strong>NOTE:</strong> You may ask yourself what the Java language extension is; well, that is the ability to access Java code from T-SQL. It is a little bit like SQLCLR, but it executes outside of the SQL Server memory and process space.</p>
</blockquote>

<p>Seeing that I never really have played around with <em>SQL Server for Linux</em>, mostly due to that in previous versions (2017) it did not have support for In-Database analytics, I thought that now would be a good time to have a look.</p>

<p></p>

<p>Cool, so install <em>SQL Server 2019 for Linux</em> and start to play around! Hmm, what do I install it on - I am a Windows guy, this whole Linux thing is &ldquo;scary&rdquo;. Ok, I guess I could spin up a virtual machine and install it there, but I am lazy. Create a VM, and then install SQL Server seemed like too much work.</p>

<p>Then I thought about my mate and colleague <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>, a.k.a <a href="https://charlla.com/kafka-donuts/">The Donut Maker</a>, and how he raves about Docker. So maybe I should run <em>SQL Server 2019 for Linux</em> in a container, problem solved. Uh, maybe not; you see - I have never used Docker. I am an old guy (some would even call me a &ldquo;Grumpy Old Man&rdquo;, a <em>GOM</em>), and you know the saying about old dogs and new tricks.</p>

<p>So anyway, I decided to give it a go; how hard can it be (it turns out not hard at all), and this post is about the steps I took to get <em>SQL Server 2019 for Linux</em> running in Docker on Windows.</p>

<h2 id="docker-for-windows">Docker for Windows</h2>

<p>This post does not cover how to download and install Docker for Windows, as there are lots of posts out there about it. If you want somewhere to start; <a href="https://docs.docker.com/v17.09/docker-for-windows/">Get started with Docker for Windows</a> is an excellent starting point.</p>

<p>I do, however, want to point out a couple of things, that caught me out:</p>

<ul>
<li>Hyper-V needs to be enabled on your host computer. This means you cannot run Virtual Box VM&rsquo;s at the same time.</li>
<li>When you install Docker, you decide whether you want to run Linux or Windows containers. So, if you install Docker for Windows intending to run <em>SQL Server 2019 for Linux</em>, you choose Linux containers.</li>
</ul>

<p>You can change the choice between Linux and Windows containers from the Docker icon in the system tray (right click on the icon):</p>

<p><img src="/images/posts/sql_2k19Docker_container_type.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Change Container Type</em></p>

<p>In <em>Figure 1</em> we see the menu entry to change the Docker container type to Windows. To change container type works the other way around as well; changing from Windows to Linux.</p>

<h2 id="docker-basics">Docker Basics</h2>

<p>Before we look at how to get and install the SQL Server &ldquo;stuff&rdquo; let us discuss some basics, and let us start with some vocabulary:</p>

<ul>
<li>Layer: a set of read-only files or commands that describe how to set up the underlying system beneath the container</li>
<li>Image: this is the piece of &ldquo;something&rdquo;, in our case <em>SQL Server 2019 for Linux</em>, that you want to install. The image consists of one or more layers.</li>
<li>Container: you download an image and create a container, and this is what you interact with.</li>
<li>Registry: where images are stored and delivered from.</li>
</ul>

<p>In our case we:</p>

<ul>
<li>Connect to a registry which contains a <em>SQL Server 2019 for Linux</em> image.</li>
<li>We download the image and create a container.</li>
<li>We &ldquo;run&rdquo; the container and interact with SQL Server.</li>
</ul>

<p>The interaction with Docker (download image, create a container, etc.) is via CLI (Command Line Interface), using the <code>docker</code> base command followed by child commands and options/parameters (<code>docker childcommand</code>). Examples of child commands:</p>

<ul>
<li><code>login</code>: logs in to a Docker registry.</li>
<li><code>pull</code>: retrieve an image from a registry.</li>
<li><code>images</code>: returns a list of images on the machine.</li>
<li><code>run</code>: creates a new container from an image and starts it. If the image has not been <code>pull</code>:ed yet, it also pulls the image.</li>
<li><code>ps</code>: Lists containers.</li>
<li><code>exec</code>: executes a command in a container. For example, you want to run a command shell in the container.</li>
<li><code>stop</code>: stops a running container.</li>
<li><code>start</code>: starts up an existing stopped container.</li>
<li><code>rm</code>: removes a container.</li>
</ul>

<p>To see a full list of commands you can go <a href="https://docs.docker.com/engine/reference/commandline/docker/">here</a>.</p>

<p>As I mentioned above, we interact with Docker via the command line, and when you are on Windows, you most likely use <em>Powershell</em>. In this post I do it somewhat differently in that I do not use the actual <em>Powershell</em> shell, but instead <strong>Azure Data Studio</strong>.</p>

<h2 id="azure-data-studio">Azure Data Studio</h2>

<p>What is Azure Data Studio then? Well, it is the evolution of SQL Operations Studio. The blog post <a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/azure-data-studio-for-sql-server/">Azure Data Studio for SQL Server</a>, introduces it like so:</p>

<p>*Azure Data Studio is a new cross-platform desktop environment for data professionals using the family of on-premises and cloud data platforms on Windows, MacOS, and Linux. Previously released under the preview name SQL Operations Studio, Azure Data Studio offers a modern editor experience with lightning fast IntelliSense, code snippets, source control integration, and an <strong>integrated terminal</strong>. It is engineered with the data platform user in mind, with built-in charting of query resultsets and customizable dashboards.*</p>

<p>We can think what we want about the &ldquo;blurb&rdquo; above, but <em>ADS</em> does have some interesting features, and for the Docker CLI work we use the integrated terminal:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Azure Data Studio and Integrated Terminal</em></p>

<p>What we see in <em>Figure 2</em> is <em>ADS</em> with visualised resultsets, some dashboards and - outlined in red - the integrated terminal. Now, let us get down to business.</p>

<h2 id="getting-the-sql-server-2019-for-linux-image">Getting the SQL Server 2019 for Linux Image</h2>

<p>We get the <em>SQL Server 2019 for Linux</em> Docker image from the <a href="https://azure.microsoft.com/en-us/blog/microsoft-syndicates-container-catalog/">Microsoft Container Registry</a> (MCR). MCR acts as a single download source for Microsoft’s container images. Regardless of where customers discover Microsoft images, the pull source is <a href="https://azure.microsoft.com/en-us/services/container-registry/">mcr.microsoft.com</a>.</p>

<p>To get the image I open <em>Azure Data Studio</em>:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio2.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Helper Files</em></p>

<p>We see in <em>Figure 3</em> how I have the <code>2k19_linux.ps</code> file open in the <em>ADS</em> editor, and how that file contains some Docker commands. I open the integrated terminal in <em>ADS</em> through <strong>Ctrl + `</strong>, or by using the menu: &ldquo;View | Command Palette | View: Toggle Integrated Terminal&rdquo;:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio3.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Integrated Terminal</em></p>

<p>In <em>Figure 4</em> we see how the terminal is open (outlined in red) and it is the Powershell terminal (highlighted in red).</p>

<blockquote>
<p><strong>NOTE:</strong> The reason I use <em>ADS</em> is that I wanted to see what I can do with it, I could as easily have used the <em>Powershell</em> shell.</p>
</blockquote>

<p>Let us now get the SQL Server 2019 image, and I do it by copying the <code>docker pull ...</code>command from the file to the terminal and hit enter. In the terminal you now see something like so (output edited for readability):</p>

<pre><code class="language-bash">PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt; 
    docker pull mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
vNext-CTP2.0-ubuntu: Pulling from mssql/server
b234f539f7a1: Downloading [========&gt; ]  7.519MB/43.12MB
55172d420b43: Download complete
5ba5bbeb6b91: Download complete
43ae2841ad7a: Download complete
f6c9c6de4190: Download complete
28f02293f049: Download complete
5eb40916d530: Downloading [&gt;         ]   1.08MB/70.39MB
46e88947bdd0: Downloading [=&gt;        ]  8.634MB/414.5MB
26983ce22a89: Waiting
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Doing a Pull</em></p>

<p>We see in <em>Code Snippet 1</em> how Docker retrieves the image. In fact, it retrieves the layers the image consists of. The layers are identified by the <code>b234f539f7a1</code>, <code>55172d420b43</code>, and so forth as we see in <em>Code Snippet 1</em>. Eventually, the <code>pull</code> finishes, and we see in the terminal:</p>

<pre><code class="language-bash">PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt; 
    docker pull mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
vNext-CTP2.0-ubuntu: Pulling from mssql/server
b234f539f7a1: Pull complete
55172d420b43: Pull complete
5ba5bbeb6b91: Pull complete
43ae2841ad7a: Pull complete
f6c9c6de4190: Pull complete
28f02293f049: Pull complete
5eb40916d530: Pull complete
46e88947bdd0: Pull complete
26983ce22a89: Pull complete
Digest: sha256:87e691e2e5f738fd64a427ebe935e4e5ccd...
Status: Downloaded newer image for 
    mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Pull Finished</em></p>

<p>After the <code>pull</code> command has finished, we can check what images we have by executing <code>docker images</code>. When I do it on my machine I see this:</p>

<p><img src="/images/posts/sql_2k19Docker_pulled images.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Pulled Docker Images</em></p>

<p>We see from <em>Figure 5</em> how the SQL Server image now exists on the machine.</p>

<h4 id="creating-a-container">Creating a Container</h4>

<p>Cool, we have an image. However, an image is just that, an image, and you cannot interact with it. To relate it to SQL Server, think about the image as an <code>.iso</code> install file. We need to &ldquo;install&rdquo; the image, e.g. create and run a container. For this we use the second <code>docker</code> command from  <em>Figure 3</em> above, and it looks like so:</p>

<pre><code class="language-bash">docker run -e &quot;ACCEPT_EULA=Y&quot; \ 
           -e &quot;SA_PASSWORD=&lt;Strong!Passw0rd&gt;&quot; \
           -p 1433:1433 \
           --name sql2k19_1 \
           -d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create &amp; Run a Container</em></p>

<p>In <em>Code Snippet 3</em> we see how we use the <code>docker run</code> command to create the container. Let us look at the options:</p>

<ul>
<li><code>-e &quot;ACCEPT_EULA=Y&quot;</code>: As creating the container also installs SQL Server, we need to accept the SQL Server EULA. The <code>-e</code> option (also <code>--env</code>) sets environment variables. In this case, environment variables SQL Server requires.</li>
<li><code>-e &quot;SA_PASSWORD=&lt;Strong!Passw0rd&gt;&quot;</code>: A second environment variable. When running SQL Server in a container, you need to set a password which follows the SQL Server default password policy. Otherwise, the container can not setup SQL server and will stop working. By default, the password must be at least 8 characters long and contain characters from three of the following four sets: Uppercase letters, Lowercase letters, Base 10 digits, and Symbols.</li>
<li><code>-p 1433:1433</code>: The <code>-p</code> (or <code>--expose</code>) option binds a port on the host machine (to the left of the colon) to a port on the container. If you run multiple SQL Server containers, the SQL Server container uses port 1433 by default, and you should use different port numbers for the host machine: <code>-p 1401:1433</code> for example.</li>
<li><code>--name sql2k19_1</code>: The <code>--name</code> option assigns a name to the container. This is like a SQL Server instance name.</li>
<li><code>-d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu</code>: This indicates which image to create a container from. The <code>-d</code> option tells Docker we want to run the container detached from the calling process. In other words, it is still up and running after you close the terminal.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> I mentioned above about the <code>-p</code> option that if you run multiple instances you should have different host ports. This is also true if you run a non Docker SQL Server instance on you machine.</p>
</blockquote>

<p>After we execute the code in <em>Code Snippet 3</em> we can check that we have a new container: <code>docker ps</code>:</p>

<p><img src="/images/posts/sql_2k19Docker_created_container.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Docker Container</em></p>

<p>From what we see in <em>Figure 6</em>, it looks like we are in business! If we want to we can connect into the container and, for example, run a bash shell:</p>

<pre><code class="language-bash">docker exec -it sql2k19_1  /bin/bash
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Run bash Shell in the Container</em></p>

<p>That is all well and good, but what about SQL Server?</p>

<h2 id="test-the-container">Test the Container</h2>

<p>Right, so now we have a container, and that container hopefully runs SQL Server. Let us try and connect to the SQL Server via <em>ADS</em>.</p>

<p>So I switch from the <em>Explorer</em> view to <em>Servers</em>: <strong>Ctrl + G</strong>, and I click <em>New Connection</em>:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_new_connection.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>New Connection</em></p>

<p>The <em>New Connection</em> is what is highlighted in red in <em>Figure 7</em>, and clicking it I get a connection dialog:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_connect.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>New Connection</em></p>

<p>In the connection dialog we see, in <em>Figure 8</em>, how I want to connect to localhost (the highlighted &ldquo;.&rdquo; in the <code>Server</code> text box), the password is whatever password I set in <em>Code Snippet 3</em>, and I chose to give the connection a name (the highlighted part in the <code>Name</code> text box). So if everything works, when I click on <em>Connect</em> I should see something like so:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_connected.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Successful Connection</em></p>

<p>As we see in <em>Figure 9</em> everything worked, and I am now connected to SQL Server 2019 for Linux, running in Docker container! To further prove all works I click on the &ldquo;New Query&rdquo; button (highlighted in red), and I execute a trivial <code>SELECT</code> statement: <code>SELECT * FROM sys.databases</code>:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_query_result.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Result from Select</em></p>

<p>In <em>Figure 10</em> we see how we get the result back! We can now continue working with <em>SQL Server 2019 for Linux</em>. If you for some reason want to shut down the container you run <code>docker stop &lt;containername&gt;</code> , and to start it up again - surprise, surprise - <code>docker start &lt;containername&gt;</code>.</p>

<h2 id="summary">Summary</h2>

<p>In this post we covered how we can run <em>SQL Server 2019 for Linux</em> in a Docker container on our Windows machine. We mentioned the Docker commands to use:</p>

<ul>
<li><code>docker pull</code></li>
<li><code>docker run</code></li>
<li><code>docker images</code></li>
<li><code>docker ps</code></li>
<li><code>docker stop</code></li>
<li><code>docker start</code></li>
</ul>

<p>We mentioned how we map a port on the hosting machine to a port on the container, and how we should use different host ports when we have multiple SQL Server instances. The SQL Server in the container is by default using port 1433.</p>

<p>In the post I also spoke about <em>Azure Data Studio</em> and some of its new functionality.</p>

<p>In future blog posts I will talk more about <em>SQL Server 2019 for Linux</em>, especially the In-Database analytics and the Java extensions, as well as <em>Azure Data Studio</em>.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is New in SQL Server 2019 Public Preview]]></title>
    <link href="http://nielsberglund.com/2018/09/24/what-is-new-in-sql-server-2019-public-preview/" rel="alternate" type="text/html"/>
    <updated>2018-09-24T19:17:06+02:00</updated>
    <id>http://nielsberglund.com/2018/09/24/what-is-new-in-sql-server-2019-public-preview/</id>
    <content type="html"><![CDATA[<p>If you read my roundup for <a href="/2018/09/23/interesting-stuff---week-38/">week 38</a>, which I published yesterday, you probably noticed that <a href="https://www.microsoft.com/en-us/ignite"><strong>MS Ignite</strong></a> started today. I mentioned in the post that I was particularly interested in some of the <strong>SQL Server</strong> sessions, as they looked very interesting.</p>

<p>However, even before the sessions started, Microsoft released SQL Server 2019 CTP 2.0 for public preview and, naturally, I jumped on the <a href="https://www.microsoft.com/en-us/evalcenter/evaluate-sql-server-2019-ctp">download link</a> and started downloading. I managed to get to the link in time before the rest of the world started the download, so I managed to get it down and then did an install.</p>

<p>The rest of this post is about my initial findings mostly in the SQL Server Machine Learning Services space.</p>

<blockquote>
<p><strong>NOTE:</strong> I have looked at SQL Server 2019 the grand total of an hour, so this is a short post.</p>
</blockquote>

<p></p>

<h2 id="installation-versions">Installation &amp; Versions</h2>

<p>First of all, the installation took forever, at least it felt that way. I believe it took around an hour, just for the install. So if you install, make sure you are not in a hurry.</p>

<p>I chose to install R and Python services in-database. After the installation finished, (finally), I enabled the machine learning services:</p>

<pre><code class="language-sql">EXEC sp_configure 'external scripts enabled', 1
RECONFIGURE WITH OVERRIDE
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Enable External Scripts</em></p>

<p>After executing the code in <em>Code Snippet 1</em>, I restarted the SQL Server 2019 instance, and then executed my regular &ldquo;check everything works&rdquo; code:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
              @language = N'R'
        , @script = N'd&lt;-42'

EXEC sp_execute_external_script 
              @language = N'Python'
        , @script = N'd=42'

EXEC sp_execute_external_script
                  @language = N'R' ,
                  @script = N'print(R.Version()$version)'

EXEC sp_execute_external_script 
              @language = N'Python'
, @script = N'
import sys
print (sys.version)'
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Test Code</em></p>

<p>As you see, the code is exceptionally advanced (not), but at least the code indicates if there are any issues. The last two <code>sp_execute_external_script</code> statements return the R and Python versions. For R the engine is now running on version <code>3.4.4</code> whereas in SQL Server 2017 it is <code>3.3.3</code>. For Python, it is the same version in both 2017 and 2019: <code>3.5.2</code>.</p>

<h2 id="extensibility-framework">Extensibility Framework</h2>

<p>So, when I read <a href="https://docs.microsoft.com/en-us/sql/sql-server/what-s-new-in-sql-server-ver15?view=sql-server-ver15">What&rsquo;s new in SQL Server 2019</a>, I came across a lot of interesting &ldquo;stuff&rdquo;, but one thing that stood out was <em>Java language programmability extensions</em>. In essence, it allows us to execute Java code in SQL Server by using a pre-built Java language extension! The way it works is as with R and Python; the code executes outside of the SQL Server engine, and you use <code>sp_execute_external_script</code> as the entry-point.</p>

<p>I haven&rsquo;t had time to execute any Java code as of yet, but in the coming days, I definitely will drill into this. Something I noticed is that the architecture for SQL Server Machine Learning Services has changed (or had additions to it). If you remember from my <a href="/sql_server_2k16_r_services">SQL Server Machine Learning Services</a> posts, the flow when executing <code>sp_execute_external_script</code> looked something like so:</p>

<ul>
<li>We execute <code>sp_execute_external_script</code>.</li>
<li>SQL Server connects to the Launchpad service.</li>
<li>Based on the <code>@language</code> parameter, Launchpad calls into either <code>rlauncher.dll</code> or <code>pythonlauncher.dll</code>.</li>
<li>The respective launcher then launches the external engine.</li>
</ul>

<p>If now Java is supported is there also a Java launcher? No, as it turns out, there is not, at least not what I could find. However what I did find was this:</p>

<p><img src="/images/posts/sql_2k19_ml_impr1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Common Launcher</em></p>

<p>In the same directory as the R and Python launchers, I see this new <code>commonlauncher.dll</code> together with a config file. When looking at the config file I did not see anything giving any hints to what goes on, but - as I said above - I will investigate.</p>

<p>At this stage I have two theories about what happens when you execute Java code:</p>

<ol>
<li>The Launchpad service knows about the Java extension: <code>javaextension.dll</code>, which is in the same directory as the launchers, and routes everything with <code>@language = Java</code> to the extension.</li>
<li>For any <code>@language</code> parameter that is not <code>R</code> or <code>Python</code>, the Launchpad service calls the <code>commonlauncher.dll</code>.</li>
</ol>

<p>That&rsquo;s more or less what I found out after an hours &ldquo;playing around&rdquo; with SQL Server 2019 CTP 2.0.</p>

<h2 id="other-interesting-stuff">Other Interesting Stuff</h2>

<p>In the beginning of this post I mentioned about interesting things I found in the <a href="https://docs.microsoft.com/en-us/sql/sql-server/what-s-new-in-sql-server-ver15?view=sql-server-ver15">What&rsquo;s new &hellip;</a> article. In no particular order:</p>

<h3 id="big-data-clusters">Big Data Clusters</h3>

<ul>
<li>Deploy a Big Data cluster with SQL Server and Spark Linux containers on Kubernetes</li>
<li>Access your big data from HDFS</li>
<li>Run Advanced analytics and machine learning with Spark</li>
<li>Use Spark streaming to data to SQL data pools</li>
<li>Run Query books that provide a notebook experience in Azure Data Studio.</li>
</ul>

<h3 id="data-discovery-and-classification">Data discovery and classification</h3>

<ul>
<li>Helps meet data privacy standards and regulatory compliance requirements.</li>
<li>Supports security scenarios, such as monitoring (auditing), and alerting on anomalous access to sensitive data.</li>
<li>Makes it easier to identify where sensitive data resides in the enterprise, so that administrators can take the right steps to secure the database.</li>
</ul>

<h3 id="sql-server-machine-learning-services-failover-clusters-and-partition-based-modeling">SQL Server Machine Learning Services failover clusters and partition based modeling</h3>

<ul>
<li>Partition-based modeling: Process external scripts per partition of your data using the new parameters added to <code>sp_execute_external_script</code>. This functionality supports training many small models (one model per partition of data) instead of one large model.</li>
<li>Windows Server Failover Cluster: Configure high availability for Machine Learning Services on a Windows Server Failover Cluster.</li>
</ul>

<h3 id="azure-data-studio">Azure Data Studio</h3>

<p>Previously released under the preview name SQL Operations Studio, Azure Data Studio is a lightweight, modern, open source, cross-platform desktop tool for the most common tasks in data development and administration. With Azure Data Studio you can connect to SQL Server on premises and in the cloud on Windows, macOS, and Linux.</p>

<h2 id="other-resources">Other Resources</h2>

<p><a href="https://twitter.com/aaronbertrand">Aaron Bertrand</a> has an <a href="https://www.mssqltips.com/sqlservertip/5710/whats-new-in-the-first-public-ctp-of-sql-server-2019/">awesome writeup</a> of what&rsquo;s new in SQL Server 2019 from a more database engine perspective. In that writeup he also points to more resources about SQL Server 2019.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 38]]></title>
    <link href="http://nielsberglund.com/2018/09/23/interesting-stuff---week-38/" rel="alternate" type="text/html"/>
    <updated>2018-09-23T06:47:28+02:00</updated>
    <id>http://nielsberglund.com/2018/09/23/interesting-stuff---week-38/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/changing-face-etl">The Changing Face of ETL</a>. An article by <a href="https://twitter.com/rmoff">Robin Moffat</a> about the &ldquo;new&rdquo; ETL, based on event-driven architectures and streaming platforms.</li>
<li><a href="https://charlla.com/kafka-donuts/">Kafka Donuts</a>. This post is the introduction and TOC to a series of posts about Kafka. The author is my colleague <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>, and in the series, he discusses the use of Kafka in a company who manufactures and sells Donuts. Reading the series introduction post, it is clear that this series is a <strong>MUST</strong> for everyone interested in Kafka. The first episode: <strong>Donut Broker</strong> is <a href="https://charlla.com/kafka-donuts-1/">here</a>, and the second episode <strong>Donut Baker</strong> is <a href="https://charlla.com/kafka-donuts-2/">here</a>.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.paraesthesia.com/archive/2018/09/20/docker-on-wsl-with-virtualbox-and-docker-machine/">Docker on Windows Subsystem for Linux using VirtualBox and Docker Machine</a>. This post by <a href="https://twitter.com/tillig">Travis Illig</a> discusses how you can enable both <strong>VirtualBox</strong> as well as <strong>Docker for Windows</strong> on the same Windows box.</li>
</ul>

<h2 id="microsoft-ignite">Microsoft Ignite</h2>

<p>So, <strong>Microsoft Ignite</strong> starts tomorrow (September 24). It looks to be an awesome conference with lots and lots of announcements of new &ldquo;stuff&rdquo;, I for one cannot wait!</p>

<p>If you, like me, are not attending but still want to follow the key-notes and various sessions, <a href="https://www.microsoft.com/en-us/ignite">this link</a> takes you to the live stream.</p>

<p>The other day I looked at the sessions and here are some that interests me:</p>

<ul>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65955?source=sessions">BRK2416 - The roadmap for SQL Server</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65356?source=sessions">BRK2183 - SQL Server Machine Learning Services: An E2E platform for machine learning</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65957?source=sessions">BRK3228 - What’s new in SQL Server on Linux and containers</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65956?source=sessions">BRK2229 - The future of SQL Server and big data</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66199?source=sessions">THR2168 - The next generation of SQL Server tools</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65967?source=sessions">BRK4021 - Deep dive on SQL Server and big data</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66202?source=sessions">THR2171 - Deploying a highly available SQL Server solution in Kubernetes</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/64634?source=sessions">BRK3154 - SQL Server in containers for application development and DevOps</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66961?source=sessions">THR2308 - SQL Server vNext meets AI and Big Data</a></li>
</ul>

<p>As you see, mostly SQL Server related sessions, and I must say that the sessions around SQL Server and Big Data intrigues me.</p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/how-machine-learning-works-3-resources-to-learn-and-develop-ml-applications">How Machine Learning Works: 3 Resources To Learn And Develop ML Applications</a>. The <a href="https://www.lightbend.com/">Lightbend</a> team has put together some resources about how to design, build, run and manage machine learning applications in production.</li>
<li><a href="https://databricks.com/blog/2018/09/18/simplify-market-basket-analysis-using-fp-growth-on-databricks.html">Simplify Market Basket Analysis using FP-growth on Databricks</a>. In retail, you want to recommend to shoppers what to purchase, and often you base the recommendations on items that are frequently purchased together. A key technique to uncover associations between different items is known as market basket analysis. This blog post talks about how you run your market basket analysis using <strong>Apache Spark MLlib</strong> <code>FP-growth</code> algorithm on <strong>Databricks</strong>.</li>
<li><a href="https://ziedhy.github.io/2018/08/Introduction_Deep_Learning.html">Introduction to Deep Learning</a>. This blog post is the first in a series about <strong>Deep Learning</strong>. At a quick glance, the series looks very informative.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I am still working on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it soon:ish.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 37]]></title>
    <link href="http://nielsberglund.com/2018/09/16/interesting-stuff---week-37/" rel="alternate" type="text/html"/>
    <updated>2018-09-16T08:17:48+02:00</updated>
    <id>http://nielsberglund.com/2018/09/16/interesting-stuff---week-37/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://www.red-gate.com/products/dba/sql-monitor/entrypage/execution-plans">SQL Server Execution Plans, 3rd Edition</a>. The third edition of <a href="https://twitter.com/gfritchey">Grant Fritchey&rsquo;s</a> excellent book about SQL Server Query Plans. If you are a developer or a DBA, you need to get this book (and read it).</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/event-flow-distributed-systems">Complex Event Flows in Distributed Systems</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation how lightweight and highly-scalable state machines ease the handling of complex logic and flows in distributed systems.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/real-time-data-analytics-and-azure-data-lake-storage-gen2/">Real-time data analytics and Azure Data Lake Storage Gen2</a>. Microsoft recently announced <a href="https://azure.microsoft.com/en-us/services/storage/data-lake-storage/">Azure Data Lake Storage Gen 2</a> (ADLS2), and this blog post looks at how ADLS2 can be used for real-time analytics. ADLS2 is at the moment in preview. I certainly hope that MS releases it soon.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/netflix-techblog/keystone-real-time-stream-processing-platform-a3ee651812a">Keystone Real-time Stream Processing Platform</a>. This is a blog post about Keystone; Netflix’s data backbone. It is an essential piece of infrastructure focusing on data analytics. I found this post very interesting, and if you are interested in stream processing, you should really read this post.</li>
<li><a href="https://www.confluent.io/blog/streams-tables-two-sides-same-coin">Streams and Tables: Two Sides of the Same Coin</a>. This blog post announces the availability of the white-paper <a href="https://www.confluent.io/thank-you/streams-and-tables-two-sides-of-the-same-coin/">Streams and Tables: Two Sides of the Same Coin</a>. The paper introduces the Dual Streaming Model, which is used to reason about physical and logical order in data stream processing. This is a <strong>MUST</strong> read!</li>
<li><a href="https://www.confluent.io/blog/building-streaming-application-ksql/">Hands on: Building a Streaming Application with KSQL</a>. In this blog post, we see how we can build a demo streaming application with KSQL, the streaming SQL engine for Apache Kafka. The application continuously computes, in real time, top music charts based on a stream of song play events.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>So the SQL Saturday &ldquo;season&rdquo; is over for me for this year. I did one talk in <a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Johannesburg</a>, two in Cape Town (<a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">this</a> and <a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">this</a>), and one in <a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">Durban</a>.</p>

<p>In addition to the conference talks I also did a full-day workshop in Cape Town and Durban about SQL Server Machine Learning Services: <strong><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">A Day of SQL Server Machine Learning Services with Niels Berglund</a></strong>.</p>

<p>When we talk about SQL Saturdays I want to thank the organisers in the various cities:</p>

<ul>
<li><a href="https://twitter.com/MikeJohnsonZA/">Michael Johnson</a> and team in Johannesburg.</li>
<li><a href="https://twitter.com/Jody_WP">Jody Roberts</a> and <a href="https://twitter.com/TheSQLGirl">Jeanne Combrink</a> and their team in Cape Town.</li>
<li><a href="https://www.linkedin.com/in/jodi-craig-1827b844/">Jodi Craig</a> and team in Durban.</li>
</ul>

<p>They are doing a fantastic work, entirely voluntarily. A HUGE, HUGE <strong>THANK YOU</strong> to all of you!</p>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>Now when SQL Saturday is over, I plan to get back to write about <strong>SQL Server Machine Learning Services</strong>. I am working right now on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it in a week or two.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 36]]></title>
    <link href="http://nielsberglund.com/2018/09/09/interesting-stuff---week-36/" rel="alternate" type="text/html"/>
    <updated>2018-09-09T21:11:32+02:00</updated>
    <id>http://nielsberglund.com/2018/09/09/interesting-stuff---week-36/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/articles/Async-Streams">Async Streams in C# 8</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about, as the title says, C# support for async streams. This, proposed, new functionality in C# is to combine the async/awaiting feature with a yielding operator. Very interesting!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/microservices-post-kubernetes">Microservices in a Post-Kubernetes Era</a>. Another article from <a href="https://www.infoq.com/">InfoQ</a>. This article questions some of the original microservices ideas and acknowledges the fact that they are not standing as strong in the post-Kubernetes era as they were before. Well worth a read!</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/09/05/introduction-to-azure-durable-functions/">Introduction to Azure Durable Functions</a>. This post, from one of the .NET engineering teams is about Azure Durable Functions. Azure Durable functions is a new programming model based on Microsoft serverless’ platform Azure Functions. It allows you to write a workflow as code and have the execution run with the scalability and the reliability of serverless with high throughput.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/category/devops/">DevOps for Data Science </a>. A series of posts by <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a> where he discusses various aspects of DevOps in a Data Science world. This is a must read!</li>
<li><a href="https://www.confluent.io/blog/putting-power-apache-kafka-hands-data-scientists/">Putting the Power of Apache Kafka into the Hands of Data Scientists</a>. A blogpost which combines two of my favorite topics: Kafka and Data Science, how good is that?! The post discusses how they at <a href="https://www.stitchfix.com/">Stitch Fix</a> exposes a multitude of data sources to their Data Scientists, and allow the Data Scientists to create new topics etc., on the fly. This is a <strong>MUST</strong> read post!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/ebook/i-heart-logs-event-data-stream-processing-and-data-integration/">I Heart Logs: Event Data, Stream Processing, and Data Integration</a>. Registration page for downloading <a href="https://twitter.com/jaykreps">Jay Kreps</a> e-book &ldquo;I Heart Logs&rdquo;. If you are interested in streaming in general, you <strong>SHOULD</strong> really get this book!</li>
<li><a href="https://data-artisans.com/blog/serializable-acid-transactions-on-streaming-data">Serializable ACID Transactions on Streaming Data</a>. Guys, (and girls), this is <strong>BIG</strong>. This blogpost introduces <em>data Artisans Streaming Ledger</em>, a new technology that brings serializable ACID transactions to applications built on a streaming architecture!</li>
<li><a href="https://www.confluent.io/blog/data-wrangling-apache-kafka-ksql">Data Wrangling with Apache Kafka and KSQL</a>. A post by <a href="https://twitter.com/rmoff">Robin Moffat</a> about how we can use Kafka and KSQL to manipulate data.</li>
<li><a href="https://www.infoq.com/articles/democratizing-stream-processing-kafka-ksql-part2">Democratizing Stream Processing with Apache Kafka® and KSQL - Part 2</a>. Part 2 in a series of posts by <a href="https://twitter.com/rmoff">Robin Moffat</a> about Kafka and KSQL. In this post Robin covers how Apache Kafka® and KSQL can be used to build powerful data integration and processing applications. You find Part 1 in the series <a href="https://www.infoq.com/articles/democratizing-stream-processing-apache-kafka-ksql">here</a>.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>This weekend I did two talks at SQL Saturday in Cape Town:</p>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>I also did a full day workshop on the Friday (September 7) before the event: <strong><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a></strong>. I had 16 people attending, and I believe it went quite well!</p>

<p>Now there is only one SQL Saturday left, in Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<p>In Durban I also do a full day workshop on the Friday, (September 14), before the event: <strong><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">A Day of SQL Server Machine Learning Services with Niels Berglund</a></strong>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 35]]></title>
    <link href="http://nielsberglund.com/2018/09/02/interesting-stuff---week-35/" rel="alternate" type="text/html"/>
    <updated>2018-09-02T08:38:38+02:00</updated>
    <id>http://nielsberglund.com/2018/09/02/interesting-stuff---week-35/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<p>The content this week is a bit meagre, due to me not having had time to browse around that much, as I have been &ldquo;prepping&rdquo; for the upcoming <strong>SQL Saturdays</strong>.</p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://www.microsoft.com/en-us/research/blog/optimizing-imperative-functions-in-relational-databases-with-froid/">Optimizing imperative functions in relational databases with Froid</a>. A post from Microsoft Research, where they discuss <strong>Froid</strong>. Froid is an extensible framework for optimising imperative programs in relational databases, and the purpose is to enable developers to use the abstraction of UDFs without compromising on performance.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/hadoop-for-beginners">Hadoop for Beginners- Part 1</a>. The first post in a series about, as the title says, Hadoop. This series is really worthwhile reading if you are interested in what Hadoop is and what it can do for you.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/apache-kafka-talk-series/">Apache Kafka: Online Talk Series</a>. This is a registration page for an on-demand video series about Kafka. I bring the popcorn, and you bring the coke!</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>In a couple of previous roundups I have mentioned that the SQL Saturday &ldquo;season&rdquo; is here, and yesterday, (September, 1), I flew out to Johannesburg and did a presentation about SQL Server Machine Learning Services, <a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a> to around 40 people.</p>

<p>The event was a smashing success thanks to the awesome arrangements by <a href="https://twitter.com/MikeJohnsonZA/">Michael Johnson</a> and his fellow volunteers!</p>

<p>Having done Johannesburg, next in turn is Cape Town:</p>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>and finally Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 34]]></title>
    <link href="http://nielsberglund.com/2018/08/26/interesting-stuff---week-34/" rel="alternate" type="text/html"/>
    <updated>2018-08-26T10:22:37+02:00</updated>
    <id>http://nielsberglund.com/2018/08/26/interesting-stuff---week-34/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="http://mattwarren.org/2018/08/21/Monitoring-and-Observability-in-the-.NET-Runtime/">Monitoring and Observability in the .NET Runtime</a>. Yet another awesome blog-post by <a href="https://twitter.com/matthewwarren">Matthew</a>. This post covers how we can monitor .NET through <em>Diagnostics</em>, <em>Profiling</em> and <em>Debugging</em>.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/chaos-engineering-introduction">Chaos Engineering: Building Immunity in Production Systems</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation discussing Chaos Engineering, its purpose, how to go about it, metrics to collect, the purpose of monitoring and logging, etc.</li>
</ul>

<h2 id="databases-storage">Databases / Storage</h2>

<ul>
<li><a href="https://queue.acm.org/detail.cfm?id=3236388">Mind Your State for Your State of Mind</a>. A paper by <a href="https://twitter.com/pathelland">Pat Helland</a>, where Pat explores the evolution of computation from a single process to microservices, the evolution of storage from files to key-value, and how they interact. Just as a side-note, you should read anything by Pat. He certainly knows what he is talking about!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/pakk-your-alpakka-reactive-streams-integrations-for-aws-azure-google-cloud">Pakk Your Alpakka: Reactive Streams Integrations For AWS, Azure, &amp; Google Cloud</a>. The link of this post had Cloud in the title, but I believe it fits better into <em>Streaming</em>. Anyway, this post links to a webinar about <a href="https://akka.io/blog/2016/08/23/intro-alpakka">Alpakka</a>. Alpakka is an integration framework for Akka Streams and the webinar looks at how Alpakka can be used for integrations with other systems.</li>
<li><a href="https://www.youtube.com/watch?v=p9LBi11KR2c">Pat Helland | Kafka Summit 2017 Keynote (Standing on the Distributed Shoulders of Giants)</a>. Speaking about <a href="https://twitter.com/pathelland">Pat</a>. Here is a YouTube video from his Kafka Summit keynote in 2017. It is based on a paper he published in 2016: <a href="https://queue.acm.org/detail.cfm?id=2953944">Standing on Distributed Shoulders of Giants</a>.</li>
</ul>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/logical-index-organization-in-cosmos-db.html">Logical index organization in Cosmos DB</a>. Another Cosmos DB post by <a href="https://twitter.com/muratdemirbas">Murat</a>. In this post, he looks at Cosmos DB&rsquo;s logical indexing subsystem.</li>
<li><a href="https://towardsdatascience.com/from-big-data-to-micro-services-how-to-serve-spark-trained-models-through-aws-lambdas-ebe129f4849c">From Big Data to micro-services: how to serve Spark-trained models through AWS lambdas</a>. This blog-post looks at how you take a Spark trained model, deploy it to AWS and expose it as an AWS Lambda endpoint. Very cool!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/08/22/snorkel-rapid-training-data-creation-with-weak-supervision/">Snorkel: rapid training data creation with weak supervision</a>. In this post <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white paper which tackles one of the central questions in supervised machine learning: how do you get a large enough set of training data to power modern deep models?</li>
<li><a href="http://luisquintanilla.me/2018/08/21/serverless-machine-learning-mlnet-azure-functions/">Serverless Machine Learning with ML.NET and Azure Functions</a>. Earlier in this weeks roundup, I linked to a post about Spark models and AWS Lambdas. This post talks about training a classification model using <a href="https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet">ML.NET</a> and deploy it with <a href="https://azure.microsoft.com/en-us/services/functions/">Azure Functions</a>.</li>
<li><a href="https://blogs.msdn.microsoft.com/data_insights_global_practice/2018/08/22/measuring-model-goodness-part-1/">Measuring Model Goodness – Part 1</a>. This post, which is part one of two-part series, is focused on measuring model goodness, specifically looking at quantifying business value and converting typical machine learning performance metrics (like precision, recall, RMSE, etc.) to business metrics.</li>
<li><a href="http://101.datascience.community/2018/08/24/microsoft-weekly-data-science-news-for-august-24-2018">MICROSOFT WEEKLY DATA SCIENCE NEWS FOR AUGUST 24, 2018</a>. I found the <a href="https://blogs.msdn.microsoft.com/data_insights_global_practice/2018/08/22/measuring-model-goodness-part-1/">Measuring Model Goodness</a> post above thanks to <a href="https://twitter.com/ryanswanstrom">Ryan&rsquo;s</a> blog and this post. Ryan&rsquo;s blog is awesome if you are interested in what Microsoft does in data science and AI!</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>The third post in the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series, which I promised a couple of weeks ago would soon be finished has to wait a bit. Reason for this is my prep for the upcoming SQL Saturdays.</p>

<p>As usual I present in Johannesburg, Cape Town and Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/785/EventHome.aspx">Johannesburg</a>, September 1:

<ul>
<li><a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a>.</li>
</ul></li>
</ul>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 33]]></title>
    <link href="http://nielsberglund.com/2018/08/19/interesting-stuff---week-33/" rel="alternate" type="text/html"/>
    <updated>2018-08-19T09:20:03+02:00</updated>
    <id>http://nielsberglund.com/2018/08/19/interesting-stuff---week-33/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/canopy-scalable-tracing-analytics-facebook">Canopy: Scalable Distributed Tracing &amp; Analysis @ Facebook</a>. This post links to an <a href="https://www.infoq.com/">InfoQ</a> presentation about <strong>Canopy</strong>, which is Facebook&rsquo;s performance and efficiency tracing infrastructure. The presentation covers lessons learned applying Canopy and present case studies of its use in solving various performance and efficiency challenges. Very interesting!</li>
<li><a href="https://azure.microsoft.com/en-us/resources/designing-distributed-systems/">Designing Distributed Systems</a>. A download link to an e-book: <strong>Designing Distributed Systems</strong>. The e-book provides repeatable, generic patterns, and reusable components to make developing reliable systems easier and more efficient. It is written by <a href="https://twitter.com/brendandburns">Brendan Burns</a> who is a Distinguished Engineer at Microsoft and works on Azure.</li>
</ul>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/schema-agnostic-indexing-with-azure.html">Schema-Agnostic Indexing with Azure Cosmos DB</a>. In this blog post, <a href="https://twitter.com/muratdemirbas">Murat</a> dissects a white paper about the schema-agnostic indexing subsystem of Cosmos DB. The post (and the paper) is very interesting, go ahead and read it, please!</li>
<li><a href="https://azure.microsoft.com/en-us/blog/azure-hdinsight-interactive-query-simplifying-big-data-analytics-architecture-and-operations/">Azure #HDInsight Interactive Query: simplifying big data analytics architecture</a>. This post discusses a new feature of Hive 2, Low Latency Analytics Processing (LLAP). LLAP produces significantly faster queries on raw data stored in commodity storage systems such as Azure Blob store or Azure Data Lake Store. This is quite exciting, and I need to check it out!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/build-udf-udaf-ksql-5-0">How to Build a UDF and/or UDAF in KSQL 5.0</a>. Not one week without at least one Kafka related post - that is the &ldquo;law&rdquo;. This post discusses a new feature in <strong>KSQL</strong> 5, the ability for the users o write their own functions for KSQL to use. Think about the possibilities that open up!</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/neural-networks-from-a-bayesian-perspective">Neural Networks from a Bayesian Perspective</a>. This post covers different ways to obtain uncertainty in Deep Neural Networks from a Bayesian perspective. The post is quite theoretical but very interesting!</li>
<li><a href="https://databricks.com/blog/2018/08/15/100x-faster-bridge-between-spark-and-r-with-user-defined-functions-on-databricks.html">100x Faster Bridge between Apache Spark and R with User-Defined Functions on Databricks</a>. Spark exposes an API, SparkR User Defined Function API, which acts as a bridge between Spark and R. Unfortunately the bridge is far from efficient. Databricks has made the bridge more efficient when you run Spark on Databricks, and this post talks about how it is done.</li>
<li><a href="https://towardsdatascience.com/the-most-important-part-of-a-data-science-project-is-writing-a-blog-post-50715f37833a">The most important part of a data science project is writing a blog post</a>. A somewhat provocative title of this blog post, but it makes a good point. Always document your data science projects so other data scientists can see what you have achieved!</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>It is that time of the year again: SQL Saturday season! As usual I present in Johannesburg, Cape Town and Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/785/EventHome.aspx">Johannesburg</a>, September 1:

<ul>
<li><a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a>.</li>
</ul></li>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Test Post and Feed Apologies]]></title>
    <link href="http://nielsberglund.com/2018/08/18/test-post-and-feed-apologies/" rel="alternate" type="text/html"/>
    <updated>2018-08-18T09:38:50+02:00</updated>
    <id>http://nielsberglund.com/2018/08/18/test-post-and-feed-apologies/</id>
    <content type="html"><![CDATA[<p>In my <a href="/2018/08/18/goodbye-jekyll-welcome-hugo/">Goodbye Jekyll, Welcome Hugo!</a> post I wrote how I have changed blog engine from Jekyll to Hugo, and how everything seemed to work. Saying that obviously jinxed it, as after I deployed to the hosting site my Atom feed was completely hosed.</p>

<p>I have now spent 4 hours trying to fix everything, but I am not sure I have succeeded. So this is a test post to see if things look OK.</p>

<p></p>

<p>I also want to apologise if I have messed up your feeds, as at one stage I saw in my feeds a lot of duplicates posts from me. Sorry!</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Goodbye Jekyll, Welcome Hugo!]]></title>
    <link href="http://nielsberglund.com/2018/08/18/goodbye-jekyll-welcome-hugo/" rel="alternate" type="text/html"/>
    <updated>2018-08-18T05:03:30+02:00</updated>
    <id>http://nielsberglund.com/2018/08/18/goodbye-jekyll-welcome-hugo/</id>
    <content type="html"><![CDATA[<p>Back in 2013 I wrote a <a href="/2013/10/02/moving-to-a-new-blog-engine/">post</a> about how I moved from a self hosted <strong>WordPress</strong> blog to a static blog; <a href="http://octopress.org"><strong>OctoPress</strong></a>. OctoPress spoke to my &ldquo;geeky&rdquo; side, as the foundation of it is the static blog-generator <a href="https://jekyllrb.com/"><strong>Jekyll</strong></a>, which in turn is Ruby based. In fact, OctoPress is more or less just some extra Ruby plugins on top of Jekyll, and to generate sites with OctoPress / Ruby you need Ruby installed on you machine.</p>

<p>I thought that by changing the blog platform, I might write more posts just due to the &ldquo;geekiness&rdquo; of the blog engine. Fast forward to the end of 2016, and the number of posts I had written since the switch came to a grand total of three. Those three includes the post where I announced the switch. Yeah, switching increased indeed my productivity - NOT!</p>

<p>So what does this have to do with anything?</p>

<p></p>

<p>Before I get into the reason for this post, let us look at what a static site generator is.</p>

<h2 id="static-site">Static Site</h2>

<p>A static site generator is a framework that takes source files and generates an entirely static website. We deploy the files that make up the site to the web server, and when a user requests a page, the web server returns that page to the user. This opposed to something like WordPress, where WordPress builds the page from a number of templates, gets the content and other site data from the database and sends the complete HTML page back to the user.</p>

<p>The advantages of a static site are that it is usually less complicated than a dynamic site; no templates, no database and so forth. Quite often serving a page to the user is better performing as the page is not dynamically created.</p>

<p>The downside of a static site is that you build it for each time you do a change to a page, adding a post, and so on, and depending on the size of your site (number of pages etc.), the build can take a while. So that is where my problem lies and the reason for this post.</p>

<h2 id="problem">Problem</h2>

<p>I mentioned above how I didn&rsquo;t manage to write any blog posts up to the end of 2016. Since then, however, I have been reasonably productive in my writing - and managed at least one post per week. It helps when you have cool stuff like <strong>SQL Server Machine Learning Services</strong> to write about. While it is cool that I produce posts, what I noticed was that the build time of the site took longer and longer. I did not think much about it until a couple of weeks ago when I had just finished the <a href="/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/">sp_execute_external_script and SQL Compute Context - III</a> post and tried to generate the site. It did not work; it just hung, what to do?</p>

<p>I ended up removing all OctoPress plugins and edited all files that referenced the plugins, to be able to run a bare-bones Jekyll generated site. I eventually managed to get it to work again, but this made me look around for other site generators.</p>

<h2 id="hugo">Hugo</h2>

<p>In my looking around for static site generators I came across <a href="https://gohugo.io/">Hugo</a>. Like Jekyll it is an open source static site generator, but it is built on <a href="https://golang.org/"><strong>Go</strong></a> instead of Ruby. One of the differences between Hugo and Jekyll is that when you generate a site with Jekyll you execute Ruby commands, and, as I mentioned above, you need Ruby installed. Hugo, on the other hand, comes as an executable <code>Hugo.exe</code> and you do not need Go installed at all. That Hugo is a self contained executable is a big plus in my book, since I have had versioning issues with Ruby a couple of times.</p>

<p>The most significant difference between Jekyll and Hugo though is speed when generating a site, or at least that is what the Hugo website says: &ldquo;<em>The world’s fastest framework for building websites</em>&rdquo;.</p>

<p>With the above points in mind, I decided to try and convert this blog from Jekyll to Hugo.</p>

<h2 id="blog-conversion">Blog Conversion</h2>

<p>So, I spent a couple of hours a day for around a week converting my posts and pages to Hugo, and it was not that difficult. The biggest issue was how Jekyll refers to posts: <code>{% post_url 2018-08-04-sp-execute-external-script-and-sql-compute-context---iii %}</code> versus <code>/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/</code>, and I had a lot of those. I eventually wrote a small <code>C#</code> console application that trawled through the posts and did the conversion. Apart from that everything is pretty straightforward. Obviously there are differences, but nothing earthshattering, as far as I can tell.</p>

<p>What about the speed then? Well, to build the Jekyll site took around 20 seconds, to build the site using Hugo takes around 2 seconds! Based on this, I decided to switch to Hugo, and this is the first post that I publish using Hugo as blog site generator. I hope I have not missed anything and that all is Ok. If not, well then, hopefully, you, my readers, <a href="mailto:niels.it.berglund@gmail.com">tell me</a> if you notice something amiss.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

