<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="http://nielsberglund.com/atom.xml" rel="self"/>
  <link href="http://nielsberglund.com"/>
  <updated>2019-03-24T08:37:01+02:00</updated>
  <id>http://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 12, 2019]]></title>
    <link href="http://nielsberglund.com/2019/03/24/interesting-stuff---week-12-2019/" rel="alternate" type="text/html"/>
    <updated>2019-03-24T08:37:01+02:00</updated>
    <id>http://nielsberglund.com/2019/03/24/interesting-stuff---week-12-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/csharp-testing-strategy-tools">Unit Testing Strategies &amp; Patterns in C#</a>. This <a href="https://www.infoq.com/">InfoQ</a> presentation discusses design principles and ways to make C# code testable, as well as using testing tools such as Moq, Autofixture, &amp; MsTest.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/google-cloud/istio-routing-basics-14feab3c040e">Istio Routing Basics</a>. So, <a href="https://cloud.google.com/istio/">Istio</a> is an open source service mesh, and this blog post covers the basics of Istio and shows what it takes to build an Istio enabled &ldquo;Hello World&rdquo; application.</li>
<li><a href="https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677">Distributed Tracing Infrastructure with Jaeger on Kubernetes</a>. The blog post I link to here looks at distributed tracing on Kubernetes using Jaeger.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/03/18/the-march-release-of-azure-data-studio-is-now-available/">The March release of Azure Data Studio is now available</a>. What the title says! There are quite a few new features in the March release of Azure Data Studio, among them: support for SQL Notebooks, PowerShell extension, and PostgresSQL support. Go and get it!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-streams-take-on-watermarks-and-triggers">Kafka Streams’ Take on Watermarks and Triggers</a>. This blog post discusses a new Kafka Streams operator: <code>Suppress</code>. It gives you the ability to control when to forward KTable updates. The <code>Suppress</code> operator comes in very handy in various CEP scenarios: &ldquo;tell me when someone has done &ldquo;a&rdquo; more than &ldquo;x&rdquo; times within &ldquo;y&rdquo; time period&rdquo;. What normally happens is that if someone achieves the &ldquo;a&rdquo;, &ldquo;x&rdquo; times within the &ldquo;y&rdquo; time period every following &ldquo;a&rdquo; would trigger as well. With <code>Suppress</code> you - wait for it - suppress the extra &ldquo;a&rdquo;, until the end of the time period.<br /></li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (<em>What Is Niels Doing</em>)</h2>

<p>Since I did the two posts about <code>CREATE EXTERNAL LIBRARY</code> for Java code (<a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">here</a> and <a href="/2019/03/17/sql-server-2019-java--external-libraries---ii/">here</a>), I thought it would be a good idea to finish off my <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. So, I am at the moment working on a post discussing <code>CREATE EXTERNAL LIBRARY</code> in the R world. The post is somewhat like the ones covering Java, but it also covers permissions etc.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 11, 2019]]></title>
    <link href="http://nielsberglund.com/2019/03/17/interesting-stuff---week-11-2019/" rel="alternate" type="text/html"/>
    <updated>2019-03-17T21:16:22+02:00</updated>
    <id>http://nielsberglund.com/2019/03/17/interesting-stuff---week-11-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/net-core-container-images-now-published-to-microsoft-container-registry/">.NET Core Container Images now Published to Microsoft Container Registry</a>. A post discussing how Microsoft are now publishing .NET Core container images to Microsoft Container Registry (MCR).</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/istio-microservices">Reducing Microservices Architecture Complexity with Istio and Kubernetes</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation which introduces Istio, and explains how the service mesh works, the technology behind it, and how to use it with microservices.</li>
<li><a href="https://www.infoq.com/news/2019/03/microservices-recommendations">Recommendations When Starting with Microservices: Ben Sigelman at QCon London</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> article about the mistakes Google made in he beginning when adopting a microservices architecture, and recommendations to avoid making these mistakes when starting with microservices.</li>
</ul>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://towardsdatascience.com/machine-learning-with-big-data-86bcb39f2f0b">Machine Learning with Big Data</a>. Data is on overdrive. It’s being generated at break-neck pace. How do we analyze all this data? This article discusses how to easily create a scalable and parallelized machine learning platform on the cloud to process large-scale data.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://eng.uber.com/dbevents-ingestion-framework/">DBEvents: A Standardized Framework for Efficiently Ingesting Data into Uber’s Apache Hadoop Data Lake</a>. This blog post looks at Uber&rsquo;s  DBEvents, a change data capture system designed for high data quality and freshness. It facilitates bootstrapping, ingesting a snapshot of an existing table, and incremental, streaming updates.</li>
<li><a href="https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues">Kafka Connect Deep Dive – Error Handling and Dead Letter Queues</a>. In this blog post <a href="https://twitter.com/rmoff">Robin Moffat</a> looks at several common patterns for handling Kafka Connect problems and examines how the patterns can be implemented.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/03/17/sql-server-2019-java--external-libraries---ii/">SQL Server 2019, Java &amp; External Libraries - II</a>. This post by yours truly looks at how to use <code>CREATE EXTERNAL LIBRARY</code> to deploy Java code without having access to SQL Server&rsquo;s filesystem.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019, Java &amp; External Libraries - II]]></title>
    <link href="http://nielsberglund.com/2019/03/17/sql-server-2019-java--external-libraries---ii/" rel="alternate" type="text/html"/>
    <updated>2019-03-17T17:13:45+02:00</updated>
    <id>http://nielsberglund.com/2019/03/17/sql-server-2019-java--external-libraries---ii/</id>
    <content type="html"><![CDATA[<p>This post is part of the <a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a> series of posts, and it is the second post discussing SQL Server 2019, Java and the creation and use of external libraries.</p>

<p>In the previous post about external libraries, we said that they were beneficial as they reduced complexities when deploying code, but there were still some caveats. So, in this post, we look at how to overcome those caveats</p>

<p></p>

<h2 id="recap">Recap</h2>

<p>Let us start with a recap of what we covered in the previous post.</p>

<p>In the last post we saw how we can make the use of Java in SQL Server somewhat less complex (permissions, code paths, etc.), by using external libraries.</p>

<p>We create the external library using the DDL statement <code>CREATE EXTERNAL LIBRARY</code>, and we saw in the post that the signature, somewhat simplified, looks like so:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY library_name  
[ AUTHORIZATION owner_name ]  
FROM &lt;file_spec&gt; [ ,...2 ]  
WITH ( LANGUAGE = &lt;language&gt; )  
[ ; ]
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Signature CREATE EXTERNAL LIBRARY</em></p>

<p>To be able to use external libraries for your Java code, the code need be packaged either in a <code>.jar</code> file or your class files need to be archived into a <code>.zip</code> file. We give the external library a name, in the <code>file_spec</code> we point to where the file resides, and finally, we set the <code>LANGUAGE</code> parameter to <code>Java</code>:</p>

<pre><code class="language-sql">USE JavaTest;
GO

CREATE EXTERNAL LIBRARY myCalc
FROM (CONTENT = 'W:\javacodepath\MyCalcJar.jar')
WITH (LANGUAGE = 'Java');
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create External Library</em></p>

<p>The code we see in <em>Code Snippet 2</em> is fairly self-explanatory, where we name the external library <code>myCalc</code> and the code is at <code>W:\javacodepath\MyCalcJar.jar</code>. What is interesting when creating external libraries for Java is that the name does not matter (apart from that it has to be unique).</p>

<p>To see that it has worked we use catalog views to investigate:</p>

<pre><code class="language-sql">SELECT el.name, el.[language], ef.content
FROM sys.external_libraries el
JOIN sys.external_library_files ef
  ON el.external_library_id = ef.external_library_id
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>View External Libraries</em></p>

<p>In <em>Code Snippet 3</em> we do a <code>SELECT</code> against <code>sys.external_libraries</code> and <code>sys.external_library_files</code>, and when we execute the result looks like so:</p>

<p><img src="/images/posts/sql_2k19_java_view_ext_lib.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>External Libraries View</em></p>

<p>We see in <em>Figure 1</em> some information about the external library. The <code>content</code> column outlined in red is interesting in that it contains the binary representation of the external library. This is like assemblies in SQLCLR. They are persisted to system tables and, when needed, loaded from the tables based on the binary representation. External libraries are the same, persisted to system tables, and when needed they are loaded from those tables.</p>

<p>So by loading the code from the database, we no longer need to worry about permissions and where to load the code from. An additional benefit is that the external libraries are database bound. If you backup and restore the database to another machine, the external libraries are there, as opposed to if you load them from a file location.</p>

<p>As good as this is, there is a problem or rather a caveat. What we have done so far requires the code for the external library to be in a location SQL Server can read. I as a developer may not have access to the file system of the SQL box. So in the rest of this post, we look at some options how we can create an external library on a remote SQL Server, where we do not have access to the file system, but we can access the SQL Server instance via SSMS or in my case, <a href="https://github.com/Microsoft/azuredatastudio">Azure Data Studio</a>.</p>

<h2 id="demo-code">Demo Code</h2>

<p>Before diving into what we want to do, let us look at the code we use today (it looks very similar to the code in the previous post):</p>

<pre><code class="language-sql">USE master
GO

DROP DATABASE IF EXISTS JavaTest;
GO

DROP DATABASE IF EXISTS JavaTestLocal;
GO

DROP DATABASE IF EXISTS JavaTestRemote;
GO

CREATE DATABASE JavaTestLocal;
GO

CREATE DATABASE JavaTestRemote;
GO
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Create Databases</em></p>

<p>We see in <em>Code Snippet 4</em> how we create a couple of databases. Since I do not have access to a remote SQL Server right now, I emulate the remote SQL by the <code>JavaTestRemote</code> database. Oh, and the first database I drop, that is the one we used in the previous post. We also need some Java code. We assume the code below is in a source file named <code>Calculator.java</code>:</p>

<pre><code class="language-java">public class Calculator {
    public static short numberOfOutputCols;
    public static int x;
    public static int y;

    static public int[] outputDataCol1;
    static public boolean[][] outputNullMap;

    public static void adder() {
        numberOfOutputCols = 1;
        outputDataCol1 = new int[1];
        outputDataCol1[0] = x + y;
        outputNullMap = new boolean[1][1];
    }
}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Java Calculator</em></p>

<p>The code in <em>Code Snippet 5</em> is the same we used in <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">SQL Server 2019, Java &amp; External Libraries - I</a>. As I mentioned in the <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">last post</a> that if you wonder about the variables in the code, the other posts in the Java <a href="/s2k19_ext_framework_java">series</a> discuss them in detail.</p>

<p>The last thing to do before we can talk about how to solve the issue with having to have access to the file system of the box SQL Server is on is to compile the code in <em>Code Snippet 5</em> and create a <code>.jar</code> for it:</p>

<pre><code class="language-java">$ javac .\Calculator.java
$ jar -cf MyCalcJar.jar .\Calculator.class
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Compile and Create a jar File</em></p>

<p>After running the code in <em>Code Snippet 6</em> we have a <code>.jar</code> file which we use to create the external library.</p>

<h2 id="external-library">External Library</h2>

<p>The question is now how to create the external library on a remote SQL Server instance if we do not have access to the file system on that server? Let us look at <code>CREATE EXTERNAL LIBRARY</code>&rsquo;s signature again (we saw it in the previous <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">post</a>):</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY library_name  
[ AUTHORIZATION owner_name ]  
FROM &lt;file_spec&gt; [ ,...2 ]  
WITH ( LANGUAGE = &lt;language&gt; )  
[ ; ]  
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Signature CREATE EXTERNAL LIBRARY</em></p>

<p>Remember how we said in <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">SQL Server 2019, Java &amp; External Libraries - I</a>, that <code>file_spec</code> points to the content of the package/code, and we saw in <em>Code Snippet 2</em> how we set <code>file_spec</code> to the path of the file.</p>

<p>However, we also said in the last <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">post</a> that <code>file_spec</code> can be a hex literal, similar to what we do when we create assemblies in SQLCLR. The hex literal is the actual binary representation of the package, and if we can get hold of the binary somehow we are &ldquo;golden&rdquo;. So, let us look at a couple of ways we can get hold of the binary package representation:</p>

<ul>
<li>From a local database.</li>
<li>Generate binary from code.</li>
</ul>

<h4 id="local-database">Local Database</h4>

<p>Let us start with a way to get the binary from a local database.</p>

<blockquote>
<p><strong>NOTE:</strong> This is similar to what we do at <a href="/derivco">Derivco</a> when we generate SQL statements to deploy SQLCLR assemblies.</p>
</blockquote>

<p>We see in <em>Figure 1</em> the <code>content</code> column, which we said before contains the binary representation of the package. What we do is to log on to the local database <code>JavaTestLocal</code>, and create the external library from the <code>.jar</code> file we created in <em>Code Snippet 6</em>:</p>

<pre><code class="language-sql">USE JavaTestLocal;
GO

CREATE EXTERNAL LIBRARY myCalc
FROM (CONTENT = 'W:\javacodepath\MyCalcJar.jar')
WITH (LANGUAGE = 'Java');
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Create Local External Library</em></p>

<p>As SQL Server is installed on my local dev-machine, and SQL Server has access to the <code>W:\javacodepath</code> path, the code in <em>Code Snippet 8</em> executes ok.</p>

<p>We know from <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">SQL Server 2019, Java &amp; External Libraries - I</a> and from the summary above how the binary representation of the package is stored in the <code>content</code> column of the <code>sys.external_library_files</code> catalog view. Let us grab the content of the <code>content</code> column:</p>

<pre><code class="language-sql">USE JavaTestLocal;
GO

DECLARE @binrep varbinary(max);

SELECT @binrep = lf.content
FROM sys.external_library_files lf
JOIN sys.external_libraries l
  ON lf.external_library_id = l.external_library_id
WHERE l.name = 'myCalc';

PRINT @binrep;
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Retrieve the Binary Representation</em></p>

<p>We see in <em>Code Snippet 9</em> how we <code>DECLARE</code> a variable <code>@binrep</code> which is a <code>varbinary</code>, and then we <code>SELECT</code> the value of the <code>content</code> column into the variable. We finally <code>PRINT</code> the content of the variable and we get something like so when we execute:</p>

<p><img src="/images/posts/sql_2k19_java_ext_lib2_binrep.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Binary Representation</em></p>

<p>In <em>Figure 2</em> we see part of the binary value of the package. We copy that into a new query window connected to the remote server and database, and we do a new <code>CREATE EXTERNAL LIBRARY</code>, but instead of a file-path for the <code>CONTENT</code> parameter we paste in the binary representation:</p>

<pre><code class="language-sql">USE JavaTestRemote;
GO

CREATE EXTERNAL LIBRARY myCalcRemote
FROM (CONTENT = 0x504B03041400080808007B34684E...)
WITH (LANGUAGE = 'Java');
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Use Binary as CONTENT</em></p>

<p>We see how the <code>CONTENT</code> parameter in <em>Code Snippet 10</em> now contains the binary value of the external library. After we execute the code in <em>Code Snippet 10</em> we test to see that it has worked by executing on the remote SQL Server:</p>

<pre><code class="language-sql">USE JavaTestRemote;
GO

EXECUTE sp_execute_external_script
@language = N'Java',
@script = N'Calculator.adder',
@params = N'@x int, @y int',
@x = 21,
@y = 21;
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Execute Against Calculator.adder</em></p>

<p>The result of running the code in <em>Code Snippet 11</em> is:</p>

<p><img src="/images/posts/sql_2k19_java_ext_lib2_queryres1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Result</em></p>

<p>From what we see in <em>Figure 3</em>, everything has worked.</p>

<p>We used the binary representation of an external library on our local SQL Server instance to create an external library on a remote instance. We do this without having access to the remote file system.</p>

<p>That is all well and good, but what if we do not have access to a local SQL Server?</p>

<h4 id="generate-binary-from-code">Generate Binary from Code</h4>

<p>The second way we can get the binary representation is to generate it from code. When I started looking into this post and how to generate the binary representation I first started with C# as I am a .NET guy. However, boy, that was a lot of code (slight exaggeration), and wouldn&rsquo;t it be &ldquo;cool&rdquo; if I could just run a script, and send in a file-path to the package? Everyone told me that Python is what all the &ldquo;cool kids&rdquo; use, so I decided to go with Python, and this is the code I started with:</p>

<pre><code class="language-python">import binascii

filePath = &quot;W:\\javacodepath\\MyCalcJar.jar&quot;

with open(filePath, &quot;rb&quot;) as binaryfile :
    myArr = binaryfile.read()
    hex_bytes = '0x' + binascii.hexlify( \
                       bytearray(myArr)).decode('utf-8')

print(hex_bytes)
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>Generate Binary from Python</em></p>

<p>When we look at the code in <em>Code Snippet 12</em> we see how:</p>

<ul>
<li>I <code>import</code> the <code>binascii</code> module which contains a number of methods to convert between binary and various ASCII-encoded binary representations.</li>
<li>I hardcode (for now) the file-path to where the <code>.jar</code> file is.</li>
<li>I open the file in binary mode. The <code>&quot;rb&quot;</code> in the <code>open(filePath, &quot;rb&quot;)</code> indicates I want the file as binary.</li>
<li>I read the file into a byte array (<code>myArr</code>).</li>
<li>I turn the byte-array into hex representation, and then I print the hex representation.</li>
</ul>

<p>The code is in a source file named <code>outputBinary.py</code>, and when I execute it from a command prompt I see the following:</p>

<p><img src="/images/posts/sql_2k19_java_ext_lib2_python1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Python Output</em></p>

<p>The (cropped) binary output in <em>Figure 4</em> is the same as we used in <em>Code Snippet 10</em>. We can now copy the output as we see in <em>Figure 4</em> and do exactly what we did before.</p>

<p>However, why do copy and paste when we can connect directly from Python to the remote database and execute <code>CREATE EXTERNAL LIBRARY</code>:</p>

<pre><code class="language-python">import pyodbc
import binascii

filePath = &quot;W:\\javacodepath\\MyCalcJar.jar&quot;
extLibName = 'myCalcRemote'

with open(filePath, &quot;rb&quot;) as binaryfile :
    myArr = binaryfile.read()
    hex_bytes = '0x' + binascii.hexlify(bytearray(myArr)).decode('utf-8')

# connect to db

dbServer = 'localhost\s2k19_ctp23_1'
dataBase = 'JavaTestRemote'
userName = '&lt;some_user_name'
password = '&lt;some_pwd&gt;'

drvr = '{ODBC Driver 17 for SQL Server}'
connStr = f'DRIVER={drvr};SERVER={dbServer};DATABASE={dataBase};UID={userName};PWD={password}'
conn = pyodbc.connect(connStr)
cursor = conn.cursor()

execStmt = f'CREATE EXTERNAL LIBRARY {extLibName}\n'
execStmt = execStmt + f'FROM (CONTENT = {hex_bytes})\n'
execStmt = execStmt + f&quot;WITH (LANGUAGE = 'Java');\n&quot;

cursor.execute(execStmt)
conn.commit()
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>Create External Library from Python Code</em></p>

<p>Before we look at the code in <em>Code Snippet 13</em> let us drop the external library we just created in the remote SQL Server instance: <code>DROP EXTERNAL LIBRARY myCalcRemote</code>. This to ensure we are back in a state with no external libraries installed.</p>

<p>So, what do we do in <em>Code Snippet 13</em>? Well, we use the same code as in <em>Code Snippet 12</em> to generate the binary representation, but we do not do a <code>PRINT</code> of it. Instead, we connect to the database using the <code>pyodbc</code> module, and the latest SQL Server ODBC driver. The <code>hex_bytes</code> variable is now a parameter in the <code>CREATE EXTERNAL LIBRARY</code> statement, and we have a hardcoded variable for the name of the external library.</p>

<p>As the code is just sample code, the connection details for the database is also hardcoded. In a real-world scenario, the script should prompt for the various details; file path, name, connection details etc., and assign the inputs to the variables:</p>

<pre><code class="language-python">extLibName = input(&quot;Provide a unique name for \
                   the external library you want to create: &quot;)
filePath = input(&quot;Provide full path to the JAR \
                  file you want to use - \
                  Example: 'W:\\javacodepath\\myJarFile.jar': &quot;)
dbServer = input(&quot;Provide name/ip address of your \
                  database server. If instance also instance name \ 
                  - Example: 'mydbServer\myInstance: &quot;)
dataBase = input(&quot;Provide name of the database where you \
                  want to create the external library: &quot;)
userName = input(&quot;Provide the user name with which you \
                   want to connect to the server: &quot;)
password = input(&quot;Provide password with which to \
                  connect to the database: &quot;)
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>Input Variables</em></p>

<p>It is worth noting that the way the script captures the password variable is not particularly secure. Instead of <code>input</code>, we should use <code>getpass</code> or something similar.</p>

<blockquote>
<p><strong>NOTE:</strong> Unless the user with which you connect is part of <code>db_owner</code>, the user needs explicit permissions to execute <code>CREATE EXTERNAL LIBRARY</code>.</p>
</blockquote>

<p>To test this, you replace the variables in <em>Code Snippet 13</em> with relevant values for your environment and run the code. The code should run OK, and you have now created an external library in a database in a remote SQL Server (well, in my case an emulated remote SQL Server).</p>

<h2 id="summary">Summary</h2>

<p>In this post, we set out to solve the issue of how to create an external library of some Java code without having access to the filesystem of the SQL Server where we want to create the external library.</p>

<p>We have seen two ways of doing it:</p>

<h4 id="local-datbase">Local Datbase</h4>

<ol>
<li>Create the external assembly from a file-path in a local SQL Server where we have access to the filesystem (like <code>localhost</code>).</li>
<li>Copy the binary representation from the <code>content</code> column in <code>sys.external_library_files</code>.</li>
<li>Assign the copied value to the <code>CONTENT</code> parameter in <code>CREATE EXTERNAL LIBRARY</code>.</li>
<li>Execute <code>CREATE EXTERNAL LIBRARY</code>.</li>
</ol>

<h4 id="generate-from-code">Generate from Code</h4>

<ol>
<li>Write script code which generates the binary representation.</li>
<li>Follow from step 2 above (local database).</li>
</ol>

<p>Alternatively, you in addition to generate the binary in the script, connect to the database from inside the script and call <code>CREATE EXTERNAL LIBRARY</code> from the script.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 10, 2019]]></title>
    <link href="http://nielsberglund.com/2019/03/10/interesting-stuff---week-10-2019/" rel="alternate" type="text/html"/>
    <updated>2019-03-10T13:14:44+02:00</updated>
    <id>http://nielsberglund.com/2019/03/10/interesting-stuff---week-10-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/03/08/a-generalised-solution-to-distributed-consensus/">A generalised solution to distributed consensus</a>. Distributed consensus is hard! In this blog post <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white-paper which re-examines the foundations of distributed consensus.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/microsoft-opens-first-datacenters-in-africa-with-general-availability-of-microsoft-azure/">Microsoft opens first datacenters in Africa with general availability of Microsoft Azure</a>. I guess the title says it all! On March 6, Microsoft opened two data centers in South Africa: South Africa North (Johannesburg) and South Africa West (Cape Town). At the moment the offerings are somewhat sparse, but I have no doubt we&rsquo;ll soon see quite a lot of services.</li>
<li><a href="https://azure.microsoft.com/en-us/blog/service-fabric-processor-in-public-preview/">Service Fabric Processor in public preview</a>. Azure Event Hub is an elegant way to ingest data into the Azure ecosystem, and Service Fabric is awesome for hosting and running microservices. Quite often some of the services need to consume from Azure Event Hubs, and until now you have had to write your own consumer, most likely based on <em>Event Processor Host</em>. That changes now with the preview of <em>Service Fabric Processor</em>, which is a new library for consuming events from an Event Hub that is directly integrated with Service Fabric. Awesome!</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/announcing-net-core-3-preview-3/">Announcing .NET Core 3 Preview 3</a>. What the title says; .NET Core 3 Preview 3 is available for download. Go and get it!</li>
<li><a href="https://devblogs.microsoft.com/dotnet/collecting-net-core-linux-container-cpu-traces-from-a-sidecar-container/">Collecting .NET Core Linux Container CPU Traces from a Sidecar Container</a>. This blog post gives a step-by-step guide of using a sidecar container to collect CPU trace of an ASP.NET application running in a Linux container.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://101.datascience.community/2019/03/07/microsoft-launches-data-science-certifications/">MICROSOFT LAUNCHES DATA SCIENCE CERTIFICATIONS</a>. In this blog post <a href="https://twitter.com/ryanswanstrom">Ryan</a> discusses 3 new certifications Microsoft recently announced aimed at Data Scientists/Engineers. I have always been skeptic to certifications by vendors, brain dumps anyone, but I will definitely have a look at this.<br /></li>
<li><a href="https://eng.uber.com/machine-learning-capacity-safety/">Using Machine Learning to Ensure the Capacity Safety of Individual Microservices</a>. This is a very interesting post by Uber&rsquo;s engineering team, discussing how they apply Machine Learning to forecast micro-services issues!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">SQL Server 2019, Java &amp; External Libraries - I</a>. Earlier today I published this post, in which I talk about how to deploy Java code to a database, so it can be loaded from there.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019, Java &amp; External Libraries - I]]></title>
    <link href="http://nielsberglund.com/2019/03/10/sql-server-2019-java--external-libraries---i/" rel="alternate" type="text/html"/>
    <updated>2019-03-10T10:22:51+02:00</updated>
    <id>http://nielsberglund.com/2019/03/10/sql-server-2019-java--external-libraries---i/</id>
    <content type="html"><![CDATA[<p>A couple of months ago I wrote a series of posts about one of the new features in SQL Server 2019; the ability to call out to Java code from inside SQL Server.</p>

<p>To see the posts, go to <a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a>.</p>

<p>In the posts, we discussed how the Java extension differs from R and Python in that R and Python are an integrated part of the SQL Server install (when enabling in-database analytics), but Java is not. In other words, the use of the Java extension requires Java to be installed beforehand, and this then has implications on permissions. We also discussed how Java is a compiled language, and we execute against a method in a class, whereas with R and Python we send a script to the external engine. The consequence of this is that when we execute Java code, we need to indicate where the compiled code resides, and those locations need specific permissions.</p>

<p>All this creates a level of complexity, and it would potentially be easier if we load the Java code from a well-known place, where we do not need to worry about permissions and so forth.</p>

<p>This post is the first of a couple where we see how new functionality in SQL Server 2019 CTP 2.3 can help.</p>

<p></p>

<h2 id="code-background">Code &amp; Background</h2>

<p>Let us start with looking at the code we use today, and also remind ourselves of some of the complexities when calling Java from SQL server.</p>

<p>So, the code:</p>

<pre><code class="language-java">public class Calculator {
    public static short numberOfOutputCols;
    public static int x;
    public static int y;

    static public int[] outputDataCol1;
    static public boolean[][] outputNullMap;

    public static void adder() {
        numberOfOutputCols = 1;
        outputDataCol1 = new int[1];
        outputDataCol1[0] = x + y;
        outputNullMap = new boolean[1][1];
    }

}
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Java Calculator</em></p>

<p>As we see in *Code Snippet 1`, the code is very simple, and we have seen variants of it in my other <a href="/s2k19_ext_framework_java">Java posts</a>. If you wonder about some of the variables in the code, the previous <a href="/s2k19_ext_framework_java">posts</a> discuss them in detail.</p>

<p>To use the code from SQL Server, we compile the source file <code>Calculator.java</code>: <code>$ javac Calculator.java</code>, into a <code>.class</code> file: <code>Calculator.class</code>.
After compilation, we can now place the <code>Calculator.class</code> in any of the locations a pre-defined <code>CLASSPATH</code> environment variable points to. To call the <code>adder</code> method from inside SQL Server we execute like so:</p>

<pre><code class="language-sql">EXECUTE sp_execute_external_script
    @language = N'Java',
    @script = N'Calulator.adder',
    @params = N'@x int, @y int',
    @x = 21,
    @y = 21;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Execute from SQL Server</em></p>

<p>By the fact that the <code>.class</code> file is in a <code>CLASSPATH</code> location, the code in <em>Code Snippet 2</em> succeeds, <strong>IF</strong> the right permissions exist on the location.</p>

<blockquote>
<p><strong>NOTE:</strong> The required permission is <code>READ</code> for the <code>ALL APPLICATION PACKAGES</code> group.</p>
</blockquote>

<p>Having the code in a <code>CLASSPATH</code> location is one way to load and execute the code. Another way is to have the code in an arbitrary location and explicitly set a parameter in the SQL call to point to that location:</p>

<pre><code class="language-sql">EXECUTE sp_execute_external_script
    @language = N'Java',
    @script = N'Calculator.adder',
    @params = N'@x int, @y int, @CLASSPATH nvarchar(512)',
    @x = 21,
    @y = 21,
    @CLASSPATH = N'W:\javacodepath';
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Using @CLASSPATH Parameter</em></p>

<p>In <em>Code Snippet 3</em> we see how we set a parameter <code>CLASSPATH</code> to point to where the code is. The permission requirements for this scenario are the same as for when we have a defined <code>CLASSPATH</code>: the location need <code>READ</code> permission for the <code>ALL APPLICATION PACKAGES</code> group.</p>

<blockquote>
<p><strong>NOTE:</strong> You may wonder where the <code>CLASSPATH</code> parameter in <em>Code Snippet 3</em> comes from, as it is not part of the signature of <code>sp_execute_external_script</code>? This parameter is a well-known parameter for the SQL Server Java language extension, and if this parameter exists the extension sets the <code>--classpath</code> option in the <code>java</code> command.</p>
</blockquote>

<p>In the code snippets above we execute against <code>.class</code> files. In the &ldquo;real world&rdquo; however you are unlikely to do that, but instead, you use <code>.jar</code> files. So let us see how we do that from SQL Server. First, we compile the <code>.java</code> source, followed by creating the <code>.jar</code>:</p>

<pre><code class="language-java">$ javac .\Calculator.java
$ jar -cf MyCalcJar.jar .\Calculator.class
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Create a jar File</em></p>

<p>After we have created the <code>MyCalcJar.jar</code> as in <em>Code Snippet 4</em>, we copy the <code>.jar</code> to either the <code>CLASSPATH</code> location or an arbitrary location. To execute we call it like so:</p>

<pre><code class="language-sql">EXECUTE sp_execute_external_script
@language = N'Java',
@script = N'Calculator.adder',
@params = N'@x int, @y int, @CLASSPATH nvarchar(max)',
@x = 21,
@y = 21,
@CLASSPATH = N'W:\javacodepath\MyCalcJar.jar'
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Execute Against a jar File</em></p>

<p>We see in <em>Code Snippet 5</em> how we when we execute against a <code>.jar</code> need to:</p>

<ul>
<li>Set the path to the <code>.jar</code>, using the using the <code>CLASSPATH</code> parameter. This is required <strong>even</strong> if the <code>.jar</code> is in the actual <code>CLASSPATH</code>.</li>
<li>Include the name of the <code>.jar</code> file.</li>
</ul>

<p>We also need to ensure that the permissions mentioned above exist where ever the <code>.jar</code> is.</p>

<p>So the examples above re-enforce what we mentioned in the beginning, Java incurs some complexity which we do not have when executing R/Python code:</p>

<ul>
<li>Where to load the code from.</li>
<li>Permissions on said location.</li>
</ul>

<p>Apart from pointing out the complexities at the beginning of this post I also mentioned that new functionality in SQL Server 2019 CTP 2.3 helps to solve this. That functionality is the ability to create external libraries.</p>

<h2 id="external-libraries">External Libraries</h2>

<p>External libraries in SQL Server enables the ability to load artefacts needed for any new language runtimes and OS platforms supported by SQL Server from the database. For example, if you need an R package that is not part of the default install of the engine you can upload to the database the particular R package as an external library and use it from the database in question. An external library is similar to a CLR assembly in that the actual library exists in the database as a byte-stream and SQL Server loads it from the database.</p>

<p>You create an external library in a similar to how you create a CLR assembly; you use a DDL statement <code>CREATE EXTERNAL LIBRARY</code>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY library_name  
[ AUTHORIZATION owner_name ]  
FROM &lt;file_spec&gt; [ ,...2 ]  
WITH ( LANGUAGE = &lt;language&gt; )  
[ ; ]  
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Signature CREATE EXTERNAL LIBRARY</em></p>

<p>In <em>Code Snippet 6</em> we see the signature for <code>CREATE EXTERNAL LIBRARY</code>. We see only the main parts:</p>

<ul>
<li><code>library_name</code> - the name we want the library to have. When we create an external library for Java code we can assign any name we want. However, when we create an external library for R, the name must be the same as what we refer to the package when we load it in the external script.</li>
<li><code>owner_name</code> - optional, and it specifies the name of the user or role that owns the external library.</li>
<li><code>file_spec</code> - this is the content of the package/code. For Java it has to be a <code>.jar</code> file, or a <code>.zip</code> file with relevant <code>.class</code> files in it. The <code>file_spec</code> can be either a path to the file, or a byte array. Part of the <code>file_spec</code> is also the platform on which SQL Server is running. For now, only Windows is supported.</li>
<li><code>language</code> - the language of the package/code.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> I mentioned above that we see the main parts of <code>CREATE EXTERNAL LIBRARY</code>, and we have not drilled down in detail. If you are interested in the details look <a href="https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-library-transact-sql?view=sql-server-ver15">here</a>.</p>
</blockquote>

<p>To see this in practice, we need first to create a database: <code>CREATE DATABASE JavaTest</code>, (we can obviously use an existing db as well). Then, based on the code in <em>Code Snippet 4</em> and <em>Code Snippet 5</em>, the call to create an external library for our calculator looks like so:</p>

<pre><code class="language-sql">USE JavaTest;
GO

CREATE EXTERNAL LIBRARY myCalc
FROM (CONTENT = 'W:\javacodepath\MyCalcJar.jar')
WITH (LANGUAGE = 'Java');
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Create External Library</em></p>

<p>What we see in <em>Code Snippet 7</em> is how we create an external library named <code>myCalc</code>, where the external library is based on a <code>.jar</code> file at <code>W:\javacodepath\MyCalcJar.jar</code>. The last thing we do is to indicate that the language is <code>Java</code>. As the only platform supported right now is Windows, we do not bother with the <code>PLATFORM</code> parameter.</p>

<p>To verify this works after we execute the code in <em>Code Snippet 7</em> we use exactly the same code as in <em>Code Snippet 2</em>:</p>

<pre><code class="language-sql">USE JavaTest;
GO

EXECUTE sp_execute_external_script
@language = N'Java',
@script = N'Calculator.adder',
@params = N'@x int, @y int',
@x = 21,
@y = 21;
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Execute Java Code Loaded from Database</em></p>

<p>In <em>Code Snippet 8</em> we see how we no longer define the <code>.jar</code> file as a parameter, (what we had to do in <em>Code Snippet 5</em>), but when we execute all works OK.</p>

<p>The question is now where the <code>.jar</code>,(or <code>.zip</code>), loads from. The answer to that is, (like with SQLCLR), that it loads from system tables in the database. When we create an SQLCLR assembly in a database, SQL Server stores the assembly in system tables, and we use catalog views to view the assemblies: <code>sys.assemblies</code>, <code>sys.assembly_files</code>, and so on. External libraries do not use the same underlying tables or catalog views, but to see the external libraries you use:</p>

<ul>
<li><code>sys.external_libraries</code> - contains a row for each external library that has been uploaded into the database.</li>
<li><code>sys.external_library_files</code> - lists a row for each file in the external library.</li>
<li><code>sys.external_libraries_installed</code> - shows what libraries have been loaded, e.g. used.</li>
</ul>

<p>An example of this:</p>

<pre><code class="language-sql">SELECT el.name, el.[language], ef.content
FROM sys.external_libraries el
JOIN sys.external_library_files ef
  ON el.external_library_id = ef.external_library_id
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>View External Libraries</em></p>

<p>When we run the code in <em>Code Snippet 9</em> we get:</p>

<p><img src="/images/posts/sql_2k19_java_view_ext_lib.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>External Libraries View</em></p>

<p>We see in <em>Figure 1</em> some information about the external library. The <code>content</code> column outlined in red is interesting in that it contains the binary representation of the external library.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we saw how we can make the use of Java in SQL Server somewhat less complex (permissions, code paths, etc.), by using external libraries.</p>

<p>To be able to use external libraries for your Java code, the code need be packaged either in a <code>.jar</code> file, or your class files need to be archived into a <code>.zip</code> file.</p>

<p>We create the external library using the DDL statement <code>CREATE EXTERNAL LIBRARY</code> where we:</p>

<ul>
<li>Define a name for the library.</li>
<li>Indicate where the <code>.jar</code> or <code>.zip</code> file is.</li>
<li>Set the language to Java.</li>
</ul>

<p>When we execute against the code we no longer need to have the code copied to the <code>CLASSPATH</code> or define a <code>@CLASSPATH</code> parameter, and no special permissions are required. Well, you need permissions to execute <code>sp_execute_external_script</code> but apart from that nothing else.</p>

<p>When we call <code>sp_execute_external_script</code> SQL Server loads the code from a system table, and we can view what external libraries exist in the database by using the <code>sys.external_libraries</code> and <code>sys.external_library_files</code> catalog views.</p>

<p>As good as all this sounds, there is one minor, (well perhaps not so minor), detail to be aware of: the way we create external libraries in this post - from a file path - requires SQL Server to be able to read from that path. In a production environment that may not be possible, so in a future post we look at how to overcome that.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 9, 2019]]></title>
    <link href="http://nielsberglund.com/2019/03/03/interesting-stuff---week-9-2019/" rel="alternate" type="text/html"/>
    <updated>2019-03-03T10:54:21+02:00</updated>
    <id>http://nielsberglund.com/2019/03/03/interesting-stuff---week-9-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/architecting-transactional-system">Achieving High Throughput with Reliability in Transactional Systems</a>. This presentation from <a href="https://www.infoq.com/">InfoQ</a> discusses architecting and designing a high performance, throughput &amp; data processing transactional system and real-time access to large datasets via APIs.</li>
<li><a href="https://www.infoq.com/presentations/monolith-microservices-refactoring-analysis-tools">Getting from Monolith to Microservices</a>. This <a href="https://www.infoq.com/">InfoQ</a> presentation looks at strategies to break a monolith, from the front-end to the back, including database refactoring and analysis tools to see dependencies in legacy code.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/training-your-first-classifier-with-spark-and-scala-893d7c6f7d88">Training Your First Classifier with Spark and Scala</a>. This post is an excellent introduction to machine learning with Spark and Scala.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.ververica.com/blog/monitoring-apache-flink-applications-101">Monitoring Apache Flink Applications 101</a>. This blog post provides an introduction to Apache Flink’s built-in monitoring and metrics system, that allows developers to monitor their Flink jobs effectively.</li>
<li><a href="https://www.confluent.io/blog/journey-to-event-driven-part-3-affinity-between-events-streams-serverless">Journey to Event Driven – Part 3: The Affinity Between Events, Streams and Serverless</a>. This post is the third part in the <a href="https://www.confluent.io/blog/journey-to-event-driven-part-1-why-event-first-thinking-changes-everything">Journey to Eventdriven</a> series, and it looks at how event-driven streaming architectures fit with serverless.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/03/01/sql-server-2019-community-technology-preview-2-3-is-now-available/">SQL Server 2019 community technology preview 2.3 is now available</a>. What the title of the post says: CTP2.3 of SQL Server 2019 is now available for download. I have already downloaded and installed the Windows version, and right now I am in the process of installing the SQL Server 2019 Big Data Cluster on Azure Kubernetes Service. Happy Days!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 8, 2019]]></title>
    <link href="http://nielsberglund.com/2019/02/24/interesting-stuff---week-8-2019/" rel="alternate" type="text/html"/>
    <updated>2019-02-24T10:08:50+02:00</updated>
    <id>http://nielsberglund.com/2019/02/24/interesting-stuff---week-8-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<p>For some reason I did not find that much interesting to me this week, but here is what caught my eye.</p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.ververica.com/blog/batch-as-a-special-case-of-streaming-and-alibabas-contribution-of-blink">Batch as a Special Case of Streaming and Alibaba&rsquo;s contribution of Blink</a>. This post discusses the in-house improvements to Flink batch processing Alibaba has done and contributed to open source. Oh, when you read the post, and you wonder what Ververica is, read <a href="https://www.ververica.com/blog/introducing-our-new-name">this post</a>.</li>
<li><a href="https://www.infoq.com/presentations/apache-flink-streaming-app">Patterns of Streaming Applications</a>. This presentation from <a href="https://www.infoq.com/">InfoQ</a> talks about a blueprint for streaming data architectures and a review of desirable features of a streaming engine. The presentation also discusses streaming application patterns and anti-patterns, and use cases and concrete examples using Apache Flink.</li>
<li><a href="https://www.infoq.com/presentations/wepay-database-streaming">The Whys and Hows of Database Streaming</a>. This <a href="https://www.infoq.com/">InfoQ</a> presentation discusses how database streaming is becoming more and more essential and the many functions that database streaming serves. The presentation also covers challenges faced with streaming peer-to-peer distributed databases.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 7, 2019]]></title>
    <link href="http://nielsberglund.com/2019/02/17/interesting-stuff---week-7-2019/" rel="alternate" type="text/html"/>
    <updated>2019-02-17T08:20:06+02:00</updated>
    <id>http://nielsberglund.com/2019/02/17/interesting-stuff---week-7-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/chaos-engineering-gamedays">Chaos Engineering with Containers</a>. This <a href="https://www.infoq.com/">InfoQ</a> presentation discusses the benefits of using Chaos Engineering to inject failures to make container infrastructure more reliable.</li>
<li><a href="https://www.infoq.com/presentations/10-kubernetes">The 10 Kubernetes Commandments</a>. The presenters in this <a href="https://www.infoq.com/">InfoQ</a> presentation explore topics from booting Kubernetes clusters to running complex workloads as a list of 10 items. They share ideas that teams can employ to make working Kubernetes less of a chore and more of a way of life.</li>
<li><a href="https://www.infoq.com/presentations/kubernetes-stateful-containers">The Highs and Lows of Stateful Containers</a>. So far, it has been a lot of containers and <a href="https://www.infoq.com/">InfoQ</a> presentations, and the third item this week is more of the same. In this <a href="https://www.infoq.com/">InfoQ</a> presentation the presenter walks through his experiences of how to reliably run a distributed database on Kubernetes, and optimize its performance. He looks at what kinds of stateful applications can most easily be run in containers, and some pitfalls he encountered along the way.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/02/13/the-february-release-of-azure-data-studio-is-now-available/">The February release of Azure Data Studio is now available</a>. I must say that Azure Data Studio (ADS) has grown on me, and now I use it almost exclusively (instead of SQL Server Management Studio). The post I link to here announces the latest release of ADS with quite a few new features. Go and grab it, and see what you think.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-connect-jdbc-source-connector-deep-dive">Kafka Connect JDBC Source Connector – Deep Dive</a>. An excellent blog post by <a href="https://twitter.com/rmoff">Robin Moffat</a>, where he drills deep into the inner workings of the Kafka Connect JDBC connector.</li>
<li><a href="https://www.confluent.io/blog/journey-to-event-driven-part-2-programming-models-event-driven-architecture">Journey to Event Driven – Part 2: Programming Models for the Event-Driven Architecture</a>. In <a href="/2019/02/03/interesting-stuff---week-5-2019/">Interesting Stuff - Week 5, 2019</a> I linked to <a href="https://www.confluent.io/blog/journey-to-event-driven-part-1-why-event-first-thinking-changes-everything">Journey to Event Driven – Part 1: Why Event-First Thinking Changes Everything</a>. This weeks blog post is part 2 of the series, and it looks at different styles of event-driven architectures and compares and contrasts scaling, persistence and runtime models.</li>
</ul>

<h2 id="azure-force-recon">Azure Force Recon</h2>

<p>I just want to remind you of the <a href="https://www.meetup.com/Azure-Transformation-Labs/events/258705868/">Azure Force Recon</a> boot camp held here in Durban February 23. I have the privilege to do a presentation: <a href="https://www.linkedin.com/feed/update/activity:6500043306041384960/">Live and Die with your Data</a>, where I talk about SQL Server 2019 Big Data Clusters. So, if you do not have anything else to do, sign up and come and hear about Azure!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 6, 2019]]></title>
    <link href="http://nielsberglund.com/2019/02/10/interesting-stuff---week-6-2019/" rel="alternate" type="text/html"/>
    <updated>2019-02-10T08:37:39+02:00</updated>
    <id>http://nielsberglund.com/2019/02/10/interesting-stuff---week-6-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/02/Oreilly-microservices-maturity">O’Reilly Publishes “The State of Microservices Maturity” Report</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> article discussing a report by O&rsquo;Reilly about microservices. In the report, O&rsquo;Reilly concludes that microservices are evolving into a trend and that DevOps and microservices feed off each other.</li>
<li><a href="https://www.infoq.com/presentations/messaging-architecture-future">Point-to-Point Messaging Architecture - The Reactive Endgame</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenters explore the current state of messaging architecture and provide an R&amp;D perspective on the future of distributed systems.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/02/CSharp-Lambda-Attributes">C# Futures: Lambda Attributes</a>. In this article <a href="https://www.infoq.com/">InfoQ</a> looks at a proposal for adding attributes to lambdas and anonymous functions.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://blogs.microsoft.com/ai/azure-data-explorer-lake-storage/">Solving a common corporate conundrum: Making sense of all that data</a>. This post discusses the newly announced Azure Data Explorer and its capabilities of analyzing 1 billion records of streaming data per second, as well as data stored in Azure Data Lake Storage.</li>
<li><a href="https://databricks.com/blog/2019/02/07/high-performance-modern-data-warehousing-with-azure-databricks-and-azure-sql-dw.html">High-Performance Modern Data Warehousing with Azure Databricks and Azure SQL Data Warehouse</a>. This blog post discusses how we can use Azure Data Factory, Azure Data Lake Storage together with Azure Databricks to load data into Azure SQL Data Warehouse for analysis, etc.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2019/02/07/announcing-ml-net-0-10-machine-learning-for-net/">Announcing ML.NET 0.10 – Machine Learning for .NET</a>. This post does what the title says; it announces the release of ML.NET 0.10. Read the post to see what new features are part of this release.</li>
<li><a href="https://www.confluent.io/blog/machine-learning-with-python-jupyter-ksql-tensorflow">Machine Learning with Python, Jupyter, KSQL and TensorFlow</a>. As it says in this post: &ldquo;This blog post focuses on how the Kafka ecosystem can help solve the impedance mismatch between data scientists, data engineers and production engineers.&rdquo;.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/processing-trillions-of-events-per-day-with-apache-kafka-on-azure/">Processing trillions of events per day with Apache Kafka on Azure</a>. This is cool; the post talks about the optimal setup to run one of the largest Kafka deployments in the world, and achieve a throughput of trillion events per day.</li>
<li><a href="https://www.confluent.io/blog/beginners-perspective-kafka-streams-building-real-time-walkthrough-detection">A Beginner’s Perspective on Kafka Streams: Building Real-Time Walkthrough Detection</a>. In retail, it is essential to detect when a customer walks in or out of a store. This blog post discusses how a company used Kafka and KSQL to be able to react quicker and with more accuracy.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/2019/02/02/sql-server-big-data-clusters-workshop-at-sql-bits/">SQL Server Big Data Clusters Workshop at SQL Bits</a>. UK&rsquo;s leading SQL Server conference <a href="https://sqlbits.com/">SQLBits</a> takes place in Manchester at the end of February. If you are attending, don&rsquo;t miss <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody&rsquo;s</a> one day SQL Server 2019 Big Data Cluster workshop. Buck knows what he talks about and he also has members of the SQL Server 2019 Big Data Cluster team on-site. I wish I could be there!</li>
</ul>

<h2 id="azure-force-recon">Azure Force Recon</h2>

<p>Speaking about conferences and SQL Server Big Data Clusters; February 23 the first of many <a href="https://www.meetup.com/Azure-Transformation-Labs/events/258705868/">Azure tactical bootcamps</a> is being held here in Durban, and I have the privilege to do a presentation: <a href="https://www.linkedin.com/feed/update/activity:6500043306041384960/">Live and Die with your Data</a>, where I talk about SQL Server 2019 Big Data Clusters. So, if you do not have anything else to do, sign up and learn about Azure!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 5, 2019]]></title>
    <link href="http://nielsberglund.com/2019/02/03/interesting-stuff---week-5-2019/" rel="alternate" type="text/html"/>
    <updated>2019-02-03T09:15:51+02:00</updated>
    <id>http://nielsberglund.com/2019/02/03/interesting-stuff---week-5-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/containers-infrastructure-virtualization">P to V to C: The Value of Bringing “Everything” to Containers</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation examining the benefits of containerization, the role of infrastructure virtualization, discussing containers, pods, controllers, policies and more.</li>
<li><a href="https://www.infoq.com/presentations/kubernetes-yaml">Dissecting Kubernetes (K8s) - An Intro to Main Components</a>. Another <a href="https://www.infoq.com/">InfoQ</a> presentation, and in this presentation we see how to deploy containers, building up a mini cluster one Kubernetes component at a time, and it also explains what happens to the YAML files involved. The presentation is an excellent introduction to the inner workings of Kubernetes.</li>
<li><a href="https://srcco.de/posts/kubernetes-failure-stories.html">KUBERNETES FAILURE STORIES</a>. Kubernetes is not only roses and plain sailing. This blog post looks at what can go wrong with Kubernetes.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/01/IAsyncDisposable-IAsyncEnume">Update on IAsyncDisposable and IAsyncEnumerator</a>. This <a href="https://www.infoq.com/">InfoQ</a> blog post looks at changes made recently to the .NET Core async streams proposal.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/journey-to-event-driven-part-1-why-event-first-thinking-changes-everything">Journey to Event Driven – Part 1: Why Event-First Thinking Changes Everything</a>. This article is the first in a series about event architecture. Event-driven architecture is a topic that I am extremely interested in, so I am really looking forward to next &ldquo;episode&rdquo; in the series.</li>
</ul>

<h2 id="microsoft-ignite-the-tour">Microsoft Ignite The Tour</h2>

<p>So, Microsoft Ignite The Tour did a stop in Johannesburg February 28 &amp; 29, and I had the privilege to present at the conference. I did three sessions:</p>

<ul>
<li><a href="http://bit.ly/2S3xtg8">What is That Cup of Coffee Doing in my Database</a>: This 15-minute talk takes a look at the new Java integration in SQL Server 2019.</li>
<li><a href="http://bit.ly/2MvQySO">SQL Server Machine Learning Services: An E2E platform for machine learning</a>: We talk about how SQL Server Machine Learning Services can function as an end-to-end platform for AI and Machine Learning.</li>
<li><a href="http://bit.ly/2sPvA8E">Deep dive on SQL Server and big data</a>: We look at how SQL Server 2019 Big Data Clusters work.</li>
</ul>

<p>If you attended the conference and would like to get the slide decks, please <a href="mailto:niels.it.berglund@gmail.com">ping</a> me as I cannot find where to upload them to.</p>

<p>I had a great time at the conference, kudos to the organizers!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 4, 2019]]></title>
    <link href="http://nielsberglund.com/2019/01/27/interesting-stuff---week-4-2019/" rel="alternate" type="text/html"/>
    <updated>2019-01-27T06:36:21+02:00</updated>
    <id>http://nielsberglund.com/2019/01/27/interesting-stuff---week-4-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/microsoft-tfs-azure-devops">Journey to Cloud Architecture</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation discussing the architectural challenges faced turning TFS into Azure DevOps, the evolution of the architecture, and lessons learned along the way.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.infoq.com/articles/why-architectural-diagrams">Why Do We Need Architectural Diagrams?</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about software architecture diagrams. Software architecture diagrams, when created well, and sparingly, can greatly improve communication within the development team and with external stakeholders. They require an understanding of the intended audience and thoughtful restraint on what to include. Resist the temptation to think that diagrams are unnecessary or unhelpful, simply because there have been plenty of cases of bad diagrams.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/hdinsight-tools-for-visual-studio-code-now-generally-available/">HDInsight Tools for Visual Studio Code now generally available</a>. A post announcing GA (general availability) of HDInsight Tools for <strong>VS Code</strong>. Seeing that I have started using <strong>VS CODE</strong> more and more, this is something I need to check out.</li>
<li><a href="https://azure.microsoft.com/en-us/blog/analyze-data-in-azure-data-explorer-using-kql-magic-for-jupyter-notebook/">Analyze data in Azure Data Explorer using KQL magic for Jupyter Notebook</a>. In this post, Microsoft announces the ability to, from inside a Jupyter Notebook, use <em>Keyword Query Language</em> (KQL) to query and analyze data in Azure Data Explorer. Very cool!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/confluent-raises-a-125m-series-d-funding-round">Confluent Raises a $125M Series D Funding Round</a>. This blog post is about how Confluent has raised &ldquo;some&rdquo; money from investors, good for them. What is interesting in the post for me is this: <em>We think there is a really fundamental missing ingredient in the software architecture of a company, namely the idea of &ldquo;events&rdquo;. These events are the orders, sales, and customer experiences, that constitute the operation of the business. Databases have long helped to store the current state of the world, but we think this is only half of the story. What is missing is the continually flowing stream of events that represents everything happening in a company, and that can act as the lifeblood of its operation.</em>. This is music to my ears!</li>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/apache-kafka-ksql-tensorflow-for-data-scientists-via-python">Apache Kafka + KSQL + TensorFlow for Data Scientists via Python + Jupyter Notebook</a>. A very, very interesting post, discussing how to use Kafka and KSQL together with machine learning tools such as Jupyter notebooks, Python, and deep learning frameworks.</li>
</ul>

<h2 id="msignite-the-tour">MSIgnite | The Tour</h2>

<p>Later today, (Sunday, Jan 27), I am off to Johannesburg and The <strong>MSIgnite | The Tour</strong> conference, where I give some presentations on Monday and Tuesday (Jan 28 and 29):</p>

<ul>
<li><a href="http://bit.ly/2S3xtg8">What is That Cup of Coffee Doing in my Database</a>: This 15-minute talk takes a look at the new Java integration in SQL Server 2019.</li>
<li><a href="http://bit.ly/2MvQySO">SQL Server Machine Learning Services: An E2E platform for machine learning</a>: We talk about how SQL Server Machine Learning Services can function as an end-to-end platform for AI and Machine Learning.</li>
<li><a href="http://bit.ly/2sPvA8E">Deep dive on SQL Server and big data</a>: We look at how SQL Server 2019 Big Data Clusters work.</li>
</ul>

<p>If you are attending the conference, please come by and say Hi!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 3, 2019]]></title>
    <link href="http://nielsberglund.com/2019/01/20/interesting-stuff---week-3-2019/" rel="alternate" type="text/html"/>
    <updated>2019-01-20T18:00:59+02:00</updated>
    <id>http://nielsberglund.com/2019/01/20/interesting-stuff---week-3-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/01/rearchitecture-system-success">An Incremental Architecture Approach to Building Systems</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about building systems and how, to avoid over-engineering, we should start with a simple architecture and evolve it as needs arise.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/01/Net-Core-3-System-Data">System.Data in .NET Core 3.0</a>. The first of two <a href="https://www.infoq.com/">InfoQ</a> articles about <code>System.Data</code> in .NET Core 3.0. The article looks at what changes there are in .NET Core 3.0 for <code>System.Data</code>.</li>
<li><a href="https://www.infoq.com/news/2019/01/Net-Core-3-System-Data-SqlClient">SQL Server and .NET Core 3.0</a>. The second <a href="https://www.infoq.com/">InfoQ</a> article about <code>System.Data</code> in .NET Core 3.0. This time the attention is at <code>System.Data.SqlClient</code>, which is the SQL Server driver.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/stream-processing-part-2-testing-your-streaming-application">Getting Your Feet Wet with Stream Processing – Part 2: Testing Your Streaming Application</a>. The second and last part of a two-part blog series about developing and validating real-time streaming applications. This post looks at how we test streaming applications.</li>
<li><a href="https://www.infoq.com/articles/distributed-stream-processor-container">Scaling a Distributed Stream Processor in a Containerized Environment</a>. This <a href="https://www.infoq.com/">InfoQ</a> article presents ideas and experiences around scaling a distributed stream processor in Kubernetes. The article discusses how the stream processor should identify the level of resource requirement and scale accordingly.<br /></li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What is Niels Doing)</h2>

<ul>
<li><a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">SQL Server 2019 &amp; Java with Visual Studio Code</a>. A blog-post looking at how we can write SQL Server Java code using <strong>Visual Studio Code</strong>, the <em>VS Code</em>&rsquo;s Java extension, and Maven.</li>
</ul>

<p>If you are at <a href="https://www.microsoft.com/en-za/ignite-the-tour/Johannesburg"><strong>Microsoft Ignite | The Tour</strong></a> in Johannesburg at the end of the month, please come by and say Hi! I deliver three sessions:</p>

<ul>
<li><strong>What is That Cup of Coffee Doing in my Database?</strong> - A 15 minutes whirl-wind tour about the new Java language extension in SQL Server 2019.</li>
<li><strong>SQL Server Machine Learning Services: An E2E platform for machine learning</strong> - A 60-minute break-out session where we look at how SQL Server Machine Learning Services serves as an end-to-end ML platform for customers.</li>
<li><strong>Deep dive on SQL Server and big data</strong> - A 60-minute break-out session where we do a deep dive behind the technology for big data integration with SQL Server including Kubernetes, Polybase futures, and scalable performance.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 &amp; Java with Visual Studio Code]]></title>
    <link href="http://nielsberglund.com/2019/01/17/sql-server-2019--java-with-visual-studio-code/" rel="alternate" type="text/html"/>
    <updated>2019-01-17T06:16:42+02:00</updated>
    <id>http://nielsberglund.com/2019/01/17/sql-server-2019--java-with-visual-studio-code/</id>
    <content type="html"><![CDATA[<p>As you know, SQL Server 2019 introduces the Java language extensions as part of the <strong>SQL Server Extensibility Framework</strong>. The Java language extensions make it possible to execute Java code from inside SQL Server, the same way we can run R/Python code. Seeing that I am &ldquo;somewhat&rdquo; interested in the <strong>SQL Server Extensibility Framework</strong> I wrote some posts about the Java language extensions: <a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a>.</p>

<p>The code in the posts is very, very simplistic where I was just trying to get the ideas across how and what to do. Another reason the code is simplistic is that I am not a Java &ldquo;person&rdquo;; in fact, the only Java code I have ever written is what is in those posts. Me not being a Java &ldquo;person&rdquo; I do not have a Java IDE, (for the code I wrote I did not need one either), so I used my regular text editor <a href="https://www.sublimetext.com/">Sublime Text 3</a>, and I compiled the code from command line (<code>$ javac ...</code>).</p>

<p>I then started on a new Java post (which I have not finished yet), and it became clear I needed an IDE, (intelli-sense, debug, etc.), but which IDE?</p>

<p></p>

<p>As you, my readers, probably know by now, I am a Microsoft and Windows guy at heart, so for me, the natural choice for an IDE has always been Visual Studio. However, even though it might be possible, writing Java code in Visual Studio is not straightforward. So, I thought; &ldquo;I have heard a lot of <a href="https://code.visualstudio.com/"><strong>Visual Studio Code</strong></a>, why not see what that can do&rdquo;. I know that <em>VS Code</em> does support Java, so why not &ldquo;kill two birds with one stone&rdquo;:</p>

<ol>
<li>Get to know <em>VS Code</em>.</li>
<li>Write some Java code.</li>
</ol>

<p>So, in the rest of this post, we see what I did to be able to write, debug, and compile Java code in <em>VS Code</em> for use in SQL Server 2019.</p>

<blockquote>
<p><strong>NOTE:</strong> Even though the title says SQL 2019 and I mention SQL 2019 above, there is very little in the post that is SQL Server 2019 specific. So, this post should be useful as well for you who are only interested in using <em>VS Code</em> for writing and debugging Java applications, no SQL Server at all.</p>
</blockquote>

<h2 id="pre-reqs-enable-java-in-vs-code">Pre-reqs &amp; Enable Java in VS Code</h2>

<p>Let us start with looking at what we need to have installed for Java in <em>VS Code</em> to work:</p>

<h4 id="visual-studio-code">Visual Studio Code</h4>

<p>Well <em>duh</em> - this is kind of obvious, and you can download it from <a href="https://code.visualstudio.com/">here</a>. Below we discuss some more about what we need to do to enable <em>VS Code</em> for Java.</p>

<h4 id="java">Java</h4>

<p>As obvious as <em>VS Code</em> from above. When installing Java, there are a couple of things to think about:</p>

<ul>
<li>Version - personally I use Java 1.8.x, as that works across platforms, and works with SQL Server on both Windows as well as Linux.</li>
<li>Paths  - ensure that the directory where <code>java</code> and <code>javac</code> exists is on the <code>PATH</code>.</li>
<li><code>JAVA_HOME</code> - you need an environment variable - <code>JAVA_HOME</code> - pointing to where the JDK directory is.</li>
</ul>

<h4 id="maven">Maven</h4>

<p><a href="https://maven.apache.org/index.html">Maven</a> is a build automation tool for primarily Java projects. Maven manages a project&rsquo;s build, dependencies, reporting and documentation from a central piece of information, the project object model (<strong>POM</strong>) file. Maven is not absolutely required, but since it is, today, the de-facto standard for Java-based projects, it is a good idea to use it.</p>

<p>You download Maven from <a href="https://maven.apache.org/download.cgi">here</a> and install instructions are <a href="https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-pack">here</a>. After installation, make sure that the directory where the <code>mvn</code> command is, exists on the <code>PATH</code>.</p>

<h2 id="vs-code-extensions">VS Code Extensions</h2>

<p><em>VS Code</em> is a lightweight cross-platform (Windows, Mac, Linux) source code editor, and it comes with built-in support for JavaScript, TypeScript and Node.js. Hmm, this is a Microsoft product, and it has its roots in Visual Studio, and no support for .NET? It is not as bad as it may sound, to add other languages you do it by installing extensions.</p>

<h4 id="installation">Installation</h4>

<p>So, in our case, we should add some Java extensions. Open <em>VS Code</em>, and you see something like this:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_open1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>VS Code Start</em></p>

<p>What you see may not exactly match <em>Figure 1</em> as it opens up in the same state it was in when you last closed it. The important part is what is outlined in red on the <em>Activity</em> bar at the left and towards the bottom of the picture: the context menu for extensions. When you click on that, the result is something like so:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_extensions1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Enabled VS Code Extensions</em></p>

<p>In <em>Figure 2</em> we see that I have, (on my development box), enabled extensions for C#, Docker, Python and R, and we also see expandable labels for <code>RECOMMENDED</code> and <code>DISABLED</code> extensions, as well as a search box. To find Java extension we enter Java in the search box and hit enter:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_java_extensions.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>VS Code Java Extensions</em></p>

<p>We see quite a few extensions for Java in <em>Figure 3</em>, and I do not know which is best, but I have used the one outlined in red before: <em>Java Extension Pack</em>, so I click on the green <em>Install</em> button:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_java_extension_install.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>VS Code Install Java Extension</em></p>

<p>What we see in <em>Figure 4</em> is that not only the <em>Java Extension Pack</em> gets installed but a few other extensions as well. What is interesting is that among the extensions installed is an extension for Maven. Restart <em>VS Code</em> after the installation of the extensions, and we are ready to go.</p>

<blockquote>
<p><strong>NOTE:</strong> You can read more about the <em>Java Extension Pack</em> <a href="https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-pack">here</a>.</p>
</blockquote>

<h4 id="un-install-remove">Un-install &amp; Remove</h4>

<p>If you do not need an extension any more, you can uninstall it by right-clicking on it and choose <code>Uninstall</code>. In the case of an extension containing other extensions (like the <em>Java Extension Pack</em>), those will be uninstalled as well.</p>

<p>Notice that uninstalling an extension does not remove it, so you still see it among your installed extensions. To completely remove an extension you delete it from the filesystem. You find the extensions at:</p>

<ul>
<li>Windows: <code>%USERPROFILE%\.vscode\extensions</code>.</li>
<li>Mac: <code>~/.vscode/extensions</code>.</li>
<li>Linux: <code>~/.vscode/extensions</code>.</li>
</ul>

<h2 id="java-in-vs-code">Java in VS Code</h2>

<p>We are now ready to write some Java code, so let us open <em>VS Code</em> (if it is not open yet). If you are used to <em>Visual Studio</em>, you would now probably go <strong>File | New Project</strong>, however that is not how you do it in <em>VS Code</em>: create a directory which will be the root of your &ldquo;project&rdquo;, and in <em>VS Code</em> open the directory: <strong>File | Open Folder</strong>:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_open_folder.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>VS Code Open Folder</em></p>

<p>In <em>Figure 5</em> we see how the directory I created, (<code>javatest</code> - outlined in red), is open in the <em>VS Code</em> Explorer. We now go ahead and add a source file to the directory by clicking on the <em>New File</em> icon outlined in blue:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_add_file.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>VS Code New File in Folder</em></p>

<p>After I click on the <em>New File</em> icon we see in <em>Figure 6</em> how a text box opens under the folder name, and how I name the file: <code>Calculator.java</code>. When I do that the file opens in the <em>VS Code</em> Editor area and I can start editing the file:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_source_file.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>VS Code Source File</em></p>

<p>The, in red, outlined area in <em>Figure 7</em> is the source file in <em>VS Code</em>&rsquo;s Editor.</p>

<p>The code we want to write is a variant of some code we wrote in <a href="/2018/12/30/sql-server-2019-extensibility-framework--java---misc.-stuff/">SQL Server 2019 Extensibility Framework &amp; Java - Misc. &ldquo;Stuff&rdquo;</a>:</p>

<pre><code class="language-java">public class Calculator {
  public static short numberOfOutputCols;
  public static int x;
  public static int y;

  static public int[] outputDataCol1;
  static public boolean[][] outputNullMap;

  public static void main(String[] args) {
      x = 21;
      y = 21;
      adder();
  }

  public static void adder() {
    numberOfOutputCols = 1;
    outputDataCol1 = new int[1];
    int res = x + y;
    outputDataCol1[0] = res;
    System.out.printf(&quot;The result of adding %d and %d = %d&quot;, x, y, res); 
    outputNullMap = new boolean[1][1];
  }
}
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Calculator</em></p>

<p>There are two differences between the code in <em>Code Snippet 1</em>, and the code in <a href="/2018/12/30/sql-server-2019-extensibility-framework--java---misc.-stuff/">SQL Server 2019 Extensibility Framework &amp; Java - Misc. &ldquo;Stuff&rdquo;</a>:</p>

<ul>
<li>In <em>Code Snippet 1</em> we have a <code>main</code> method.</li>
<li>The <code>adder</code> method in <em>Code Snippet 1</em> is doing a <code>System.out.printf</code>.</li>
</ul>

<p>The <code>main</code> method in <em>Code Snippet 1</em> is there so we can run the code as an application in <em>VS Code</em> and debug the code. The <code>System.out.printf</code> is there just to get an output during debugging:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_debug.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Debugging Java in VS Code</em></p>

<p>So, we see in <em>Figure 8</em> how I have set three breakpoints in my <code>Calculator.java</code> code: at line 8, 16 and 21. I have then hit <strong>F5</strong> to run the code in debug mode, and I eventually hit the breakpoint at line 21. We see, outlined in red, the output of the <code>adder</code> method. Cool, it looks like our code is working, but how do we compile the code into a <code>.class</code> file?</p>

<p>Well, that is the thing, there is no built-in way to compile as we have not (or been able to) treat the code as a project, we just opened a directory and added a file to that directory. If we now want to compile we need to do it from command-line:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_compile.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Compile Java in VS Code</em></p>

<p>We open the terminal in <em>VS Code</em> by <strong>View | Terminal</strong>, and in <em>Figure 9</em> we see how we:</p>

<ul>
<li>are in the terminal (the terminal tab, outlined in red).</li>
<li>compile the <code>Calculator.java</code> source file by calling: <code>javac .\Calculator.java</code> (outlined in blue).</li>
<li>see the compiled <code>Calculator.class</code> file (outlined in yellow).</li>
</ul>

<p>When we have the compiled <code>.class</code> file we copy that to the <code>CLASSPATH</code> location and execute against it from SQL Server like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'Calculator.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2 
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Execute from SQL Server</em></p>

<p>What we see in <em>Code Snippet 2</em> is nothing different from what we have done in the posts in the <a href="/s2k19_ext_framework_java">SQL Server 2019 Extensibility Framework &amp; Java</a> series.</p>

<p>We have now seen how we use <em>VS Code</em> to write and debug Java code. However, the code we wrote and debugged was very simple. The question is, how do we handle more complex scenarios - dependencies on external JAR files, compile without having to do it from the command prompt, etc.? That is where Maven enters the picture.</p>

<h2 id="maven-projects">Maven Projects</h2>

<p>I mentioned above how Maven is a build automation tool for primarily Java projects. There are other build tools as well, but in this post, I use Maven as it is - which I mentioned above -  the de-facto standard for Java-based projects.</p>

<p>Above we saw how we installed Maven as well as the <em>VS Code</em> Maven extension, however before we start to use it let us talk a little about Maven archetypes and naming conventions.</p>

<h4 id="maven-archetypes">Maven Archetypes</h4>

<p>A Maven archetype is like a template for a project. Templates can be:</p>

<ul>
<li>Internal: part of the Maven install.</li>
<li>Local: you create archetypes and install them locally.</li>
<li>Remote: archetypes are uploaded to, and exists in repositories.</li>
</ul>

<p>To see what internal archetypes you have installed you run following Maven command: <code>$ mvn archetype:generate -DarchetypeCatalog=internal</code>:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_archetypes_internal.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Maven Internal Archetypes</em></p>

<p>In <em>Figure 10</em> we see the internal archetypes and we see (outlined in red) a default archetype, the <code>maven-archetype-quickstart</code>. Oh, the command <code>mvn archetype:generate</code> creates a new Maven project. The command allows you to define what archetype to use, plus properties of the project.</p>

<h4 id="naming-conventions">Naming Conventions</h4>

<p>As mentioned above, when we create a Maven project based on an archetype, we need to define some properties of the project:</p>

<ul>
<li><code>groupId</code>: uniquely identifies a project across all projects. Typically it follows Java&rsquo;s package name rules, (<code>org.apache.maven</code> for example), but it is not required to follow those rules.</li>
<li><code>artifactId</code>: this is like a .NET project name.</li>
<li><code>version</code>: version number (<code>1.0</code>, <code>1.1</code>, etc.).</li>
<li><code>packageName</code>: this is optional. If you do not define a package name, the <code>groupId</code> is used as package name.</li>
</ul>

<p>Now, let us see how we use Maven together with <em>VS Code</em>.</p>

<h2 id="vs-code-maven">VS Code &amp; Maven</h2>

<p>When you click in the Activity pane to view the Explorer (or <strong>View</strong> | <strong>Explorer</strong>), you should see a new tab for <code>MAVEN PROJECTS</code>:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_project1.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Maven Projects - I</em></p>

<p>We see, outlined in red, the Maven Projects tab in <em>Figure 11</em>. To create a new Maven project we use the plus icon, outlined in yellow, also in <em>Figure 11</em>, and when we click on the icon we get:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_project2.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Select Archetype</em></p>

<p>We see in <em>Figure 12</em> an archetype search box, together with some internally installed archetypes, and we see how we can look for more archetypes in remote repositories. For now, we use the archetype outlined in red: <code>maven-archetype-quickstart</code>. When we choose the <code>quickstart</code> archetype, a file dialog pops up for us to choose a directory for the project. After we have chosen a directory Maven starts to download jar files etc.:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_project3.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Maven Downloads</em></p>

<p>When the download finishes (it can take a while), we are asked for some information about the project:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_project4.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Project Properties</em></p>

<p>We see in <em>Figure 14</em> how we define the Maven properties:</p>

<ul>
<li><code>groupId</code>: <code>com.nielsberglund.calculator</code>.</li>
<li><code>artifactId</code>: <code>myCalculator</code>.</li>
<li><code>version</code>: <code>1.0</code>.</li>
<li><code>package</code>: <code>javasqlcalc</code>.</li>
</ul>

<p>In <em>Figure 14</em> we see how the package name defaulted to the value of the <code>groupId</code>, but I decided to override and set a shorter package name, as it is preferred not to have multiple levels of package names in the SQL Server Java extension. After we confirmed the properties Maven created the project:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_project5.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Project Properties</em></p>

<p>Maven created the project in, (as we see outlined in red in <em>Figure 15</em>),  <code>D:\maventest1\myCalculator</code>. When navigating to <code>D:\maventest1</code> in File Explorer we see the directory hierarchy:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_project6.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>Project Directory Hierarchy</em></p>

<p>The projects directory/file hierarchy, as we see in <em>Figure 16</em>, is your typical Java project hierarchy with the package name as a directory. If the package name had been something like: <code>com.nielsberglund.calculator</code>, we would see three directories: <code>com</code>, <code>nielsberglund</code>, and <code>calculator</code>.</p>

<p>Ok, so now what - we have a Maven project, but when we look in <em>VS Code</em>, we do not see anything different from when we created the project. We need to, from the <em>File</em> menu open the folder Maven created, in our case the <code>myCalculator</code> folder:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_project7.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>Open Project Folder in VS Code</em></p>

<p>We opened the <code>myCalculator</code> folder in <em>VS Code</em> as we see in <em>Figure 17</em>, and what is somewhat strange is that we see more folders and files in <em>Figure 17</em>, (outlined in red), than what we do in <em>Figure 16</em>. Those directories/files get created by one of the installed Java extensions when we open the folder. We also see in <em>Figure 17</em> the <code>pom.xml</code> file which defines project dependencies etc.</p>

<p>When we drill down in the directory structure we arrive at a source file:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_source_maven_file.png" alt="" /></p>

<p><strong>Figure 18:</strong> <em>Source File</em></p>

<p>When Maven created the project, it also added the file we see in <em>Figure 18</em>, and this is now our &ldquo;starter&rdquo; file for the project. In this project, we will have multiple source files and to start with we rename the <code>App.java</code> to <code>MyCalculator.java</code>, and also change the class name to <code>MyCalculator</code>. We then add a second source file in the <code>javasqlcalc</code> folder, as we did in <em>Figure 6</em>, we name the file <code>Calculator.java</code>, and in that file create a class named `Calculator. Let us not forget to add the definition at the top of the file.</p>

<p>We want the <code>Calculator</code> class to have an <code>adder</code> instance method, taking two parameters as input, and returning the result of the addition of those two parameters:</p>

<pre><code class="language-java">package javasqlcalc;

public class Calculator {

    public int adder(int x, int y) {
        return x + y;
    }
}
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Calculator Class</em></p>

<p>Nothing strange with the <code>Calculator</code> class as we see in <em>Code Snippet 3</em>.</p>

<p>The <code>MyCalculator</code> class should have a static method looking something like the method in <em>Code Snippet 1</em>, but instead of doing the addition in the method, it creates a new instance of the <code>Calculator</code> class and calls the <code>adder</code> method in that class:</p>

<pre><code class="language-java">public static void adder() {
    numberOfOutputCols = 1;
    outputDataCol1 = new int[1];

    Calculator calc = new Calculator();

    int res = calc.adder(x, y);
    outputDataCol1[0] = res;
    System.out.printf(&quot;The result of adding %d and %d = %d&quot;, x, y, res); 
    outputNullMap = new boolean[1][1];
}
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>The adder method in MyCalculator Class</em></p>

<p>In <em>Code Snippet 4</em> we see the changed, (from what we see in <em>Code Snippet 1</em>), <code>adder</code> method. Copy the code in the <code>main</code> method as we see in <em>Code Snippet 1</em> into the <code>MyCalculator</code> class, and we are now ready to test the code. Set a couple of breakpoints in the code, similar to what we did in <em>Figure 8</em>, and then hit <strong>F5</strong> to debug. The code should now run without any issues.</p>

<h4 id="maven-compile">Maven Compile</h4>

<p>We are now at the same stage we were after <em>Figure 8</em> above when we compiled the code from the command line as in <em>Figure 9</em>. The question is how do we compile when we use Maven? That&rsquo;s easy:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_compile.png" alt="" /></p>

<p><strong>Figure 19:</strong> <em>Maven Compile Menu</em></p>

<p>To compile using Maven we:</p>

<ul>
<li>Right-click on our project under the Maven tab as in <em>Figure 19</em> (outlined in red).</li>
<li>That brings up a menu.</li>
</ul>

<p>We click on the <code>compile</code> menu entry, outlined in yellow in <em>Figure 19</em>, which calls <code>mvn compile ...</code> from <em>VS Code</em>&rsquo;s built-in terminal:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_compile2.png" alt="" /></p>

<p><strong>Figure 20:</strong> <em>Maven Compile</em></p>

<p>We see in <em>Figure 20</em> the <code>compile</code> command, (outlined in red), and how Maven compiles it into the <code>target\classes</code> directory (outlined in yellow). When we, in File Explorer, navigate to <code>target\classes</code>, we see something like so:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_compile_output.png" alt="" /></p>

<p><strong>Figure 21:</strong> <em>Output After Compile</em></p>

<p>As we see in <em>Figure 21</em> the compilation created a directory with the same name as the package, (outlined in red), and put the two <code>.class</code> files in that directory, (the class files are outlined in yellow). To use this from SQL we now copy the <code>javasqlcalc</code> directory to the <code>CLASSPATH</code> location, and we call it like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'javasqlcalc.MyCalculator.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2 
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Execute from SQL Server</em></p>

<p>In <em>Code Snippet 5</em>, in the <code>@script</code> parameter, we see what we discussed in <a href="/2018/12/30/sql-server-2019-extensibility-framework--java---misc.-stuff/">SQL Server 2019 Extensibility Framework &amp; Java - Misc. &ldquo;Stuff&rdquo;</a>; how to call into a method inside a Java package. Once again, do not forget to copy the directory together with the <code>.class</code> files to the location of <code>CLASSPATH</code>.</p>

<h4 id="maven-jar-files">Maven &amp; JAR Files</h4>

<p>We have now seen how we compile Java projects with Maven, and how that generates <code>.class</code> files. In <a href="/2018/12/30/sql-server-2019-extensibility-framework--java---misc.-stuff/">SQL Server 2019 Extensibility Framework &amp; Java - Misc. &ldquo;Stuff&rdquo;</a>, we said, however, that in a production environment you do not usually call into <code>.class</code> files directly, but you use JAR files. So, the question is then whether we can compile and create JAR files in <em>VS Code</em> using Maven?</p>

<p>Of course we can, and we do it in almost the same way we compiled in <em>Figure 19</em>:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_jar.png" alt="" /></p>

<p><strong>Figure 22:</strong> <em>Package Java Application</em></p>

<p>To create a JAR file using the <em>VS Code</em> Maven extension we, as we see in <em>Figure 22</em>, right click on our project, (outlined in red), and then in the menu, we click on the <code>package</code> menu item which is outlined in yellow. The result of this is like so:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_jar_compile.png" alt="" /></p>

<p><strong>Figure 23:</strong> <em>Create JAR - I</em></p>

<p>What happens, as we see in <em>Figure 23</em>, is that - in <em>VS Code</em>&rsquo;s built-in terminal the command <code>mvn package ...</code> executes, (outlined in red). Part of the <code>package</code> command is a recompilation, which we see outlined in yellow. The recompilation only happens if necessary, e.g. if there has been code changes between last compilation/packaging and now. After potential recompilation Maven builds the JAR file:</p>

<p><img src="/images/posts/sql_2k19_java_vscode_maven_jar_build.png" alt="" /></p>

<p><strong>Figure 24:</strong> <em>Create JAR - II</em></p>

<p>Finally, Maven builds the JAR file as we see in <em>Figure 24</em> and places it in the <code>target</code> directory. We see how Maven names the JAR file: <code>artifactId</code>-<code>version</code> by default. To use it from SQL Server we copy the JAR file to the <code>CLASSPATH</code> location, as discussed in <a href="/2018/12/30/sql-server-2019-extensibility-framework--java---misc.-stuff/">SQL Server 2019 Extensibility Framework &amp; Java - Misc. &ldquo;Stuff&rdquo;</a>, and we call it like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'javasqlcalc.MyCalculator.adder'
, @params = N'@CLASSPATH nvarchar(256), @x int, @y int'
, @CLASSPATH = N'C:/javacodepath/myCalculator-1.0.jar'
, @x = @p1
, @y = @p2 
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>SQL Call to JAR</em></p>

<p>As we call into a method packaged in a JAR file, we need to set the <code>@CLASSPATH</code> parameter - as we do in <em>Code Snippet 6</em> - and include the JAR file name. The location to where the <code>@CLASSPATH</code> parameter points to, does not need to be the location of the <code>CLASSPATH</code> environment variable, but it is important that the <code>ALL APPLICATION PACKAGES</code> group has <code>READ</code> permissions on the location. See <a href="/2018/12/30/sql-server-2019-extensibility-framework--java---misc.-stuff/">SQL Server 2019 Extensibility Framework &amp; Java - Misc. &ldquo;Stuff&rdquo;</a> for more about this.</p>

<h2 id="summary">Summary</h2>

<p>As I mentioned i the beginning, this post came about because I needed an IDE to help when writing Java code.</p>

<p>In this post we covered <em>VS Code</em> and its Java extensions, and we also saw how to use Maven to compile Java code and/or package it into JAR files.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 2, 2019]]></title>
    <link href="http://nielsberglund.com/2019/01/13/interesting-stuff---week-2-2019/" rel="alternate" type="text/html"/>
    <updated>2019-01-13T19:43:46+02:00</updated>
    <id>http://nielsberglund.com/2019/01/13/interesting-stuff---week-2-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/articles/dotnet-core-article-second-series">Article Series - .NET Core - 2nd Series</a>. An <a href="https://www.infoq.com/">InfoQ</a> series, exploring some of the benefits of .NET Core and how it can help traditional .NET developers and all technologists that need to bring robust, performant and economical solutions to market.</li>
<li><a href="https://adamsitnik.com/ConcurrencyVisualizer-Profiler/">Profiling Concurrent .NET Code with BenchmarkDotNet and visualizing it with Concurrency Visualizer</a>. A blog post by <a href="https://twitter.com/SitnikAdam">Adam Sitnik</a> who is .NET Mr Performance. In the post, he talks about how to profile multithreaded .NET code using <a href="https://benchmarkdotnet.org/">BenchmarkDotNet</a> and <a href="https://marketplace.visualstudio.com/items?itemName=Diagnostics.ConcurrencyVisualizer2017">Concurrency Visualizer</a>. If you are a .NET developer, do yourself a favor and follow <a href="https://twitter.com/SitnikAdam">Adam</a>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/stream-processing-part-1-tutorial-developing-streaming-applications">Getting Your Feet Wet with Stream Processing – Part 1: Tutorial for Developing Streaming Applications</a>. The first part of a two-part blog series about developing and validating real-time streaming applications. This part is an introduction to streaming application development.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://databricks.com/blog/2019/01/07/introducing-databricks-runtime-5-1-for-machine-learning.html">Introducing Databricks Runtime 5.1 for Machine Learning</a>. A blog post which introduces the latest beta release of Databricks runtime for Machine Learning (version 5.1). The release includes some of the best frameworks for deep learning.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/01/09/how-to-automate-machine-learning-on-sql-server-2019-big-data-clusters/">How to automate machine learning on SQL Server 2019 big data clusters</a>. This post explores how to use automated machine learning (AutoML) to create new machine learning models over data in SQL Server 2019 big data clusters.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/01/09/the-january-release-of-azure-data-studio-is-now-available/">The January release of Azure Data Studio</a>. A post announcing the latest release of <a href="https://github.com/microsoft/azuredatastudio">Azure Data Studio</a>. Go and download it from <a href="https://docs.microsoft.com/en-us/sql/azure-data-studio/download?view=sql-server-ver15">here</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Christmas, New Year, Week 1, 2019]]></title>
    <link href="http://nielsberglund.com/2019/01/06/interesting-stuff---christmas-new-year-week-1-2019/" rel="alternate" type="text/html"/>
    <updated>2019-01-06T17:13:57+02:00</updated>
    <id>http://nielsberglund.com/2019/01/06/interesting-stuff---christmas-new-year-week-1-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This is the &ldquo;roundup&rdquo; of the posts that has been most interesting to me over the Christmas and New Year period.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/12/19/identifying-impactful-service-system-problems-via-log-analysis/">Identifying impactful service system problems via log analysis</a>. This is the dissection by <a href="https://twitter.com/adriancolyer">Adrian</a> of a white-paper about <a href="https://github.com/logpai/Log3C">Log3C</a>, which is a general framework that identifies service system problems from system logs. Go off and read <a href="https://twitter.com/adriancolyer">Adrian</a>&rsquo;s take on it, and then go and download <a href="https://github.com/logpai/Log3C">Log3C</a>!</li>
<li><a href="https://blog.acolyer.org/2018/12/21/towards-a-theory-of-software-development-expertise/">Towards a theory of software development expertise</a>. The last paper <a href="https://twitter.com/adriancolyer">Adrian</a> looked at in 2018. The paper is about what is a good software developer, and how do you get better?</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/devops-database">DevOps for the Database</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation trying to find the answer to why it is difficult to apply principles of DevOps to databases.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://muratbuffalo.blogspot.com/2018/12/2-phase-commit-and-beyond.html">Two-phase commit and beyond</a>. An excellent post by <a href="https://twitter.com/muratdemirbas">Murat</a> discussing the two-phase commit protocol. It is an excellent read, and if you have any interest in distributed computing, you should read the post.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.kdnuggets.com/2018/08/github-python-data-science-spotlight.html">GitHub Python Data Science Spotlight: AutoML, NLP, Visualization, ML Workflows</a>. A blog post which spotlights a select group of open source Python data science projects with GitHub repos. I am particularly interested in <a href="http://autokeras.com">Auto-Keras</a>, and <a href="https://mlflow.org/docs/latest/index.html">MLFlow</a>.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/articles/testing-aspnet-core-web-api">How to Test ASP.NET Core Web API</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> article investigating testing ASP.NET Core 2.0 Web API solutions. The article looks at internal testing with Unit Testing and externally testing with a new testing framework in ASP.NET Core called <em>Integration Testing</em>.</li>
<li><a href="https://www.infoq.com/articles/netcore-cli">A Quick Tour of the .NET CLI</a>. Another <a href="https://www.infoq.com/">InfoQ</a> article, this time it covers how several .Net OSS tools take advantage of the dotnet cli and how we can use the new cli tooling in our daily development.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/microsoft-open-sources-trill-to-deliver-insights-on-a-trillion-events-a-day/">Microsoft open sources Trill to deliver insights on a trillion events a day</a>. A blog post introducing <a href="https://github.com/Microsoft/trill">Trill</a>; a high-performance streaming analytics engine. This is something I want to keep my eyes on!</li>
<li><a href="https://www.confluent.io/blog/easy-ways-generate-test-data-kafka">Easy Ways to Generate Test Data in Kafka</a>. As the title says, a post discussing how to generate data for testing in Kafka.</li>
</ul>

<h2 id="what-did-niels-do-during-the-holidays">What Did Niels Do During the Holidays</h2>

<p>I was a, (somewhat), busy bee during the holidays and managed to publish three blog posts:</p>

<ul>
<li><a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">SQL Server 2019 Extensibility Framework &amp; Java - Null Values</a>. A post where I look at how to handle null values when calling Java code from SQL Server 2019.</li>
<li><a href="/2018/12/24/sql-server-ml-services---multiple-input-data-sets/">SQL Server ML Services - Multiple Input Data Sets</a>. We look at how to push in multiple datasets to external scripts.</li>
<li><a href="/2018/12/30/sql-server-2019-extensibility-framework--java---misc.-stuff/">SQL Server 2019 Extensibility Framework &amp; Java - Misc. &ldquo;Stuff&rdquo;</a>. We look at SQL Server 2019 Java Extensions, and Java packages, the CLASSPATH and JAR files.</li>
</ul>

<p>Right now I am working on two more posts related to the <strong>SQL Server 2019 Extensibility Framework</strong>. I hope to publish at least one this coming week.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Extensibility Framework &amp; Java - Misc. &#39;Stuff&#39;]]></title>
    <link href="http://nielsberglund.com/2018/12/30/sql-server-2019-extensibility-framework--java---misc.-stuff/" rel="alternate" type="text/html"/>
    <updated>2018-12-30T12:24:53+02:00</updated>
    <id>http://nielsberglund.com/2018/12/30/sql-server-2019-extensibility-framework--java---misc.-stuff/</id>
    <content type="html"><![CDATA[<p>This post is the fourth post in a series where I look at the Java extension in SQL Server, i.e. the ability to execute Java code from inside SQL Server.</p>

<p>To see what other posts there are in the series, go to <a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a>.</p>

<p>This fourth post acts as a &ldquo;roundup&rdquo; of miscellaneous &ldquo;stuff&rdquo; I did not cover in the three previous posts, and some of the things we look at are:</p>

<ul>
<li>Java packages.</li>
<li>The <code>CLASSPATH</code> environment variable.</li>
<li>JAR files.</li>
</ul>

<p>Ok, let us get into it.</p>

<p></p>

<blockquote>
<p><strong>DISCLAIMER:</strong> *This post contains Java code. I am not a Java guy, in fact, the only Java I have ever written is the code in this post and the previous <strong>SQL Server 2019 Java</strong> posts. So, the code is not elegant in any shape or form, and I am absolutely certain it can be done in a much better way. However, this is not about Java as such, but how you call Java code from SQL Server, and what you need to implement on the Java side.*</p>
</blockquote>

<h2 id="java-packages">Java Packages</h2>

<p>We use packages in Java to prevent naming conflicts, to control access, to make searching/locating and usage of classes, interfaces, enumerations and annotations easier, etc. A Java package is somewhat similar to a .NET namespace in that it can be used to group &ldquo;like-minded&rdquo; classes etc. together.</p>

<p>In <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a> we saw some code looking like so:</p>

<pre><code class="language-java">public class JavaTest1 {
  public static short numberOfOutputCols;
  public static int x;
  public static int y;

  public static void adder() {
    System.out.printf(&quot;The result of adding %d and %d = %d&quot;, x, y, x + y);   
  }

  public static void helloWorld() {
    System.out.print(&quot;Hello World from SQL Java&quot;);
  }
}
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Sample Code</em></p>

<p>As we see in <em>Code Snippet 1</em>, we have one class: <code>JavaTest1</code> with two methods:</p>

<ul>
<li>The method <code>adder</code> which adds two integers together.</li>
<li>The <code>helloWorld</code> method which says hello.</li>
</ul>

<p>Let us break out the <code>adder</code> method, and put it into a class of its own - <code>Calculator</code>:</p>

<pre><code class="language-java">public class Calculator {
  public static short numberOfOutputCols;
  public static int x;
  public static int y;

  public static void adder() {
    System.out.printf(&quot;The result of adding %d and %d = %d&quot;, 
                        x, y, x + y);
  }
}
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Calculator - I</em></p>

<p>We assume the code in <em>Code Snippet 2</em> is in a source file: <code>Calculator.java</code>. We then compile the code into a <code>.class</code> file: <code>javac Calculator.java</code>, and after the compile, we move the <code>Calculator.class</code> to the <code>CLASSPATH</code> location.</p>

<blockquote>
<p><strong>NOTE:</strong> If you wonder why the <code>adder</code> method has no parameters, why we have two class members, <code>x</code> and <code>y</code>, and what is this <code>CLASSPATH</code> thing about, go back to <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a> where it is all explained.</p>
</blockquote>

<p>To execute the code in <em>Code Snippet 2</em> we use more or less the same code as in <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">Hello World</a>:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'Calculator.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2 
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Execute adder Method</em></p>

<p>In <em>Code Snippet 3</em> we see how we send in two parameters to the java code, and how we call the <code>Calculator.adder</code> method. All works ok, and we get back a result. This application is now deployed, but after a while someone creates a new and better calculator:</p>

<pre><code class="language-java">public class Calculator {
  public static short numberOfOutputCols;
  public static int x;
  public static int y;

  static public int[] outputDataCol1;
  static public boolean[][] outputNullMap;

  static public short numberOfOutputCols;

  public static void adder() {

    numberOfOutputCols = 1;

    outputDataCol1 = new int[1];
    outputDataCol1[0] = x + y;

    outputNullMap = new boolean[1][1];

  }
}
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Calculator - II</em></p>

<p>The new and better calculator, as we see in <em>Code Snippet 4</em>, still takes two class members, but now returns the result as a one column resultset. As we did with the code in <em>Code Snippet 2</em> we put it into a source file, <code>Calculator.java</code> and compile it into <code>Calculator.class</code>. Ok, so far so good. However, what if we try and copy the <code>.class</code> file to the <code>CLASSPATH</code> location, then we overwrite the existing <code>Calculator.class</code>. Not good!</p>

<p>This issue can potentially be resolved by having multiple <code>CLASSPATH</code> locations and copy the <code>.class</code> file to another location, but doing it that way would not guarantee which version of the application we call. That&rsquo;s where Java packages come to the rescue. Change the code in <em>Code Snippet 4</em> slightly:</p>

<pre><code class="language-java">package mycalculator
public class Calculator {
  
  ...

  public static void adder() {
    '''
  }
}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Calculator &amp; Package</em></p>

<p>In <em>Code Snippet 5</em> we added at the top of the source file: <code>package mycalculator</code>. Ok, so what does this do? Let us experiment:</p>

<ul>
<li>Compile <code>Calculator.java</code> as usual (the edited one).</li>
<li>Rename the <code>Calculator.class</code> file in the <code>CLASSPATH</code> location to something different, (like <code>Calcuator.xxx</code>).</li>
</ul>

<p>After copying the newly compiled <code>Calculator.class</code> to the <code>CLASSPATH</code> location, execute the code in <em>Code Snippet 3</em> again and see if it works:</p>

<p><img src="/images/posts/sql_2k19_java_misc_error1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Cannot Find Class</em></p>

<p>So that didn&rsquo;t work! We see in <em>Figure 1</em> how we get an exception, saying something about not finding class <code>Calculator</code>. Seeing that the code now includes a package name, and reading-up some more about packages, maybe I should include the package name as well in the <code>@script</code> parameter: <code>@script = N'mycalculator.Calculator.adder</code>.</p>

<p>When we try that it still does not work, but I get a somewhat different error message: <code>... Failed to find class mycalculator/Calculator</code>. That error message almost seems like a path. Let us create a directory in the <code>CLASSPATH</code> location named as our package, <code>mycalculator</code>, and move the <code>.class</code> file to there, and see what happens when we execute (still with <code>@script = N'mycalculator.Calculator.adder</code>):</p>

<p><img src="/images/posts/sql_2k19_java_misc_pkg_success.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Package Success</em></p>

<p>As we see in <em>Figure 2</em>, it works! So, if you compile into packages you need to:</p>

<ul>
<li>Place the code in a directory named as the package in the <code>CLASSPATH</code> location.</li>
<li>Execute the code with the package name in the <code>@script</code> variable: <code>@script=N'packagename.ClassName.methodName</code>.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> The Java compiler can automatically create the directory for the package by using the <code>-d</code> option. If you are new to Java (like me), I can recommend <a href="https://www.guru99.com/java-packages.html">this site</a> to read more about packages.</p>
</blockquote>

<h2 id="classpath">CLASSPATH</h2>

<p>In <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a> we discussed the <code>CLASSPATH</code> environment variable briefly. We said that the <code>CLASSPATH</code> variable is there so that the Java Compiler and Java Runtime can find Java classes referenced in your program. It maintains a list of directories (containing many Java class files) and JAR file (a single-file archive of Java classes), and in all of the SQL 2019 Java posts we have copied our <code>.class</code> files to the location of the <code>CLASSPATH</code> variable.</p>

<p>However, when you execute a Java application from the command line you do not have to rely on the <code>CLASSPATH</code> variable, but you can define where to find the application by the <code>-cp</code> option:</p>

<pre><code class="language-bash">W:\&gt;java -cp .\nielsb-work\GitHub-Repos\nielsberglund.com \
               \_code\sql_2k19_java_misc\java Calculator`.
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Usage of -cp Option</em></p>

<p>In <em>Code Snippet 6</em> we see how we, from the root of <code>W:\</code>, call the <code>Calculator</code> application by setting the <code>-cp</code> option to where the application is.</p>

<blockquote>
<p><strong>NOTE:</strong> When calling the application as in <em>Code Snippet 6</em>, the application has to have a <code>main</code> method: <code>public static void main(String[] args)</code>.</p>
</blockquote>

<p>When executing <code>sp_execute_external_script</code> we have the same functionality. We do this by defining, in <code>sp_execute_external_script</code>&rsquo;s <code>@params</code> parameter list, a parameter <code>@CLASSPATH</code>, something like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'some_method'
, @params = N'@CLASSPATH nvarchar(256)'
, @CLASSPATH = N'my_path_to_app`

</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Execute with Classpath</em></p>

<p>So, we see, in <em>Code Snippet 7</em>, how we define the <code>@CLASSPATH</code> parameter, and set it to a value. The parameter is a well-known parameter to the Java extension components, and they know how to handle it. The <code>@CLASSPATH</code> parameter is similar to <code>@r_rowsPerRead</code>, for the R/Python components. In <a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services: sp_execute_external_script - III</a> you can read more about <code>@r_rowsPerRead</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> The <code>@r_rowsPerRead</code> parameter is not supported in the Java extensions.</p>
</blockquote>

<p>To see how to use the <code>@CLASSPATH</code> parameter, (and some &ldquo;gotchas&rdquo;), we create two almost identical applications:</p>

<pre><code class="language-java">// this is the first app
public class Hello {
  
  public static short numberOfOutputCols;
  public static void world() {
    System.out.print(&quot;Hello World 1 from SQL Java&quot;);
  }
}

// this is the second app
public class Hello {
  
  public static short numberOfOutputCols;
  public static void world() {
    System.out.print(&quot;Hello World 2 from SQL Java&quot;);
  }
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Java Apps</em></p>

<p>In <em>Code Snippet 8</em> we see both applications (I put them in one code snippet to save space). The only difference between them is what we print out to the &ldquo;console&rdquo;. Copy the two classes into two separate source files (<code>Hello.java</code>), compile and copy the first <code>Hello.class</code> to the location which the <code>@CLASSPATH</code> environment variable points to. Create a new directory, (<code>testpath</code> or something like so), and copy the second <code>Hello.class</code> to that directory.</p>

<p>To be sure that we start afresh, restart the <em>Launchpad</em> service, and then execute:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'Hello.world'
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Execute against CLASSPATH Environment Variable</em></p>

<p>The code in <em>Code Snippet 9</em> succeeds and returns <code>Hello World 1 from SQL Java</code>, cool. When we executed, the Java runtime found the code in the <code>CLASSPATH</code> location.</p>

<p>Rename the <code>Hello.class</code> file in the <code>CLASSPATH</code> location to something like <code>Hello2.xxx</code>, and now we use code like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'Hello.world'
, @params = N'@CLASSPATH nvarchar(256)
, @CLASSPATH = N'C:/testpath'
</code></pre>

<p><strong>Code Snippet 10:</strong> *Execute with <em>CLASSPATH</em> Parameter*</p>

<p>In <em>Code Snippet 10</em> we see how we have defined the <code>@CLASSPATH</code> parameter and set it to the directory where our second Java app is. When we execute the code in <em>Code Snippet 10</em>, we get a surprise:</p>

<p><img src="/images/posts/sql_2k19_java_misc_classpath_error.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Exception</em></p>

<p>Hmm, we get an exception, and the exception says something about not being able to find the class (outlined in red). What is that all about, we know that the <code>.class</code> file contains the class and the method? The issue we see here is related to something we discussed in <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>, more specifically permissions. Remember how we said that we need to give the <code>ALL APPLICATION PACKAGES</code> group <code>READ</code> permissions on the <code>CLASSPATH</code> directory. The same is true for the directory(is) you define in the <code>@CLASSPATH</code> parameter. So what we need to do is assign <code>READ</code> permissions to the <code>ALL APPLICATION PACKAGES</code> group, and that should then fix it.</p>

<p>Before we do that, however, go to the <code>CLASSPATH</code> location where you have the renamed <code>Hello.class</code> application, and rename it back to its original name. Then execute the code in <em>Code Snippet 10</em> as it stands. Notice how the execution succeeds, and you get <code>Hello World 1 from SQL Java</code> as output. This indicates that when we execute, we first try and find the application in the <code>@CLASSPATH</code> parameter location, and if that does not succeed we try in the <code>CLASSPATH</code> environment variable location. With this in mind the exception we received in <em>Figure 3</em> makes somewhat more sense:</p>

<ul>
<li>We first tried to execute against the code in the <code>@CLASSPATH</code> parameter location.</li>
<li>An exception was thrown before we could execute the actual code.</li>
<li>We fell back to the <code>CLASSPATH</code> environment variable location.</li>
<li>We didn&rsquo;t find the class (as we renamed it).</li>
<li>We throw the exception we see in <em>Figure 3</em>.</li>
</ul>

<p>Ok, so now we can go ahead and assign the <code>ALL APPLICATION PACKAGES</code> group <code>READ</code> permissions on the location of the <code>@CLASSPATH</code> parameter, and then execute the code in <em>Code Snippet 10</em> again:</p>

<p><img src="/images/posts/sql_2k19_java_misc_classpath_success.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Success</em></p>

<p>We see in <em>Figure 4</em> how we get the expected output back when we execute. Assigning the right permissions fixes it as we expected.</p>

<blockquote>
<p><strong>NOTE:</strong> In future SQL Server 2019 CTP releases (and RTM) we most likely will not need to bother with either <code>CLASSPATH</code> or <code>@CLASSPATH</code> as there will be a way to upload JAR files (see below) to the database, and the code gets loaded from there. This is somewhat similar to what we do with SQLCLR today, as well as external libraries for R (<code>CREATE EXTERNAL LIBRARY...</code>).</p>
</blockquote>

<h2 id="jar-files">JAR Files</h2>

<p>So far, in this post as well as previous posts about <strong>SQL Server 2019 Java</strong> extensions, we have dealt with individual <code>.class</code> files, and since the code has just been simple snippets of test code it&rsquo;s okay. In a production environment, outside of SQL Server you would most likely use <strong>JAR</strong> files, as it is common practice and it makes the overall experience more manageable.</p>

<p>JAR stands for <strong>J</strong>ava <strong>AR</strong>chive and is a package file format typically used to aggregate many Java class files and associated metadata and resources (text, images, etc.) into one file for distribution.</p>

<p>To see how this works in SQL Server we start with the <code>Calculator</code> class we see in <em>Code Snippet 2</em>. Compile it into a class file: <code>javac Calculator.java</code>. After compilation, we have a <code>Calculator.class</code>, and we want to create a JAR file containing that <code>Calculator.class</code> file:</p>

<pre><code class="language-bash">jar cvf testjar1.jar Calculator.class
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Create JAR File</em></p>

<p>To create a JAR file, we use something called the <em>Java Archive Tool</em> which is part of the Java Development Kit (JDK). To invoke the <em>Java Archive Tool</em>, we use the <code>jar</code> command as we see in <em>Code Snippet 11</em>. The options and arguments following the <code>jar</code> command in <em>Code Snippet 11</em> are (the following is taken from the <a href="https://docs.oracle.com/javase/tutorial/deployment/jar/build.html">Oracle Java Documentation</a>):</p>

<ul>
<li>The <code>c</code> option indicates that you want to create a JAR file.</li>
<li>The <code>v</code> option produces verbose output while the JAR file builds.</li>
<li>The <code>f</code> option indicates that you want the output to go to a file.</li>
<li>Following <code>cvf</code> is the name of the JAR file you want created.</li>
<li>The last argument is a space-separated list of one or more files that you want to include in your JAR file. This can contain the <code>*</code> wildcard symbol as well as name of directories.</li>
</ul>

<p>When I run the code in <em>Code Snippet 11</em> I see:</p>

<pre><code class="language-bash">W:\path_to_my_source_code&gt;jar cvf testjar1.jar Calculator.class
added manifest
adding: Calculator.class(in = 656) (out= 426)(deflated 35%)
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>Output Create JAR File</em></p>

<p>In <em>Code Snippet 12</em> we see how a manifest file is added, together with the <code>.class</code> file. We also see how the <code>.class</code> file is compressed. The manifest file is not required as such, and you can use the <code>M</code> option to indicate you do not want a manifest file. So now we have a JAR file, what do we do with it?</p>

<p>To begin with, delete everything you have in the <code>CLASSPATH</code> location, and also in where <code>@CLASSPATH</code> (<em>Code Snippet 10</em>) points to. Then copy the <code>.jar</code> file created in <em>Code Snippet 12</em> to the <code>CLASSPATH</code> location. The JAR file contains the <code>Calculator</code> class from <em>Code Snippet 2</em>, and we may expect to be able to call it as in <em>Code Snippet 3</em>. However, when we execute the code we get the same error as in <em>Figure 1</em>: <code>... Failed to find class ...</code>. So apparently we cannot call a method in a JAR file the same way as in a <code>.class</code> file. That makes sense when we see how to call an application in a JAR file from the command line:</p>

<pre><code class="language-bash">java -jar &lt;jar_file&gt;
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>Calling a JAR File Application</em></p>

<p>In <em>Code Snippet 13</em> we see how we tell the Java launcher, (<code>java</code>), that the application is packaged in a <code>.jar</code> file by using the <code>-jar</code> flag, and we also point to the actual file. To do this in SQL Server, we need to give the full path to the JAR file in the <code>@CLASSPATH</code> variable:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'Calculator.adder'
, @params = N'@CLASSPATH nvarchar(256), @x int, @y int'
, @CLASSPATH = N'C:/javacodepath/testjar1.jar'
, @x = @p1
, @y = @p2 
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>Calling a Method in a JAR File</em></p>

<p>We see in <em>Code Snippet 14</em> how we use the <code>@CLASSPATH</code> parameter to indicate what JAR file our method is in,  and when we run the code, we get the expected result back.</p>

<p>So what about the scenario when we have a couple of <code>.class</code> files and one (or more) is part of a package, like what we discussed related to <em>Code Snippet 5</em>? We have:</p>

<ul>
<li>The <code>Calculator</code> class as in <em>Code Snippet 2</em>, compiled to <code>Calculator.class</code>.</li>
<li>The <code>Calculator</code> class in a package <code>mycalculator</code>, compiled to a separate <code>Calculator.class</code>.</li>
</ul>

<p>Remember what we said above how packages need their own directories, and the <code>.class</code> file(s) being inside that directory. I have a directory layout like so:</p>

<p><img src="/images/posts/sql_2k19_java_misc_jar_directories.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Project Directory Layout</em></p>

<p>We see in <em>Figure 5</em> how I have a &ldquo;top&rdquo; level directory <code>calcproj</code>, and in that directory, I have the <code>.class</code> file for the <code>Calculator</code> class we see in <em>Code Snippet 2</em>. Under the <code>calcproj</code> directory I have a directory named as the package in <em>Code Snippet 5</em>, and inside that directory is the <code>.class</code> file for the <em>Code Snippet 5</em> <code>Calculator</code> class. To create the <code>.jar</code> file I run some code like so:</p>

<pre><code class="language-bash">W:\&lt;path&gt;\calcproj&gt;jar cvfM testjar1.jar *
adding: Calculator.class(in = 656) (out= 426)(deflated 35%)
adding: mycalculator/(in = 0) (out= 0)(stored 0%)
adding: mycalculator/Calculator.class(in = 475) (out= 334) (deflated 29%)
</code></pre>

<p><strong>Code Snippet 15:</strong> <em>Create JAR File</em></p>

<p>There are a couple of things to look at in <em>Code Snippet 15</em>:</p>

<ul>
<li>I use a wildcard in the <code>jar</code> command, and this recursively adds <code>.class</code> files and directories to the <code>.jar</code> file.</li>
<li>In the output we see how the <code>mycalculator</code> directory gets added together with the <code>Calculator.class</code> file.</li>
</ul>

<p>To check that what we see in <em>Code Snippet 15</em> is correct we can inspect a <code>.jar</code> file by executing: <code>jar tf testjar1.jar</code>:</p>

<pre><code class="language-bash">W:\&lt;path&gt;\calcproj&gt;jar tf testjar1.jar
Calculator.class
mycalculator/
mycalculator/Calculator.class
</code></pre>

<p><strong>Code Snippet 16:</strong> <em>Inspecting JAR File</em></p>

<p>What we see in <em>Code Snippet 16</em> verifies the output in <em>Code Snippet 15</em>.</p>

<p>After copying the <code>.jar</code> file to the location in <em>Code Snippet 14</em> (and overwriting the file there) we can execute as usual (including the path to the <code>.jar</code> file), and see how it all works:</p>

<ul>
<li><code>@script=N'Calculator.adder</code></li>
<li><code>@script=N'mycalculator.Calculator.adder</code></li>
</ul>

<p>Before I summarize this post, a couple of points about JAR files:</p>

<ul>
<li>The location of the JAR file can be wherever Java can get to it.</li>
<li>The directory where the JAR file needs the usual permissions: <code>READ</code> for the <code>ALL APPLICATION PACKAGES</code> group.</li>
<li>The JAR file does not need to be named <code>.jar</code>, it can have any extension or even no extension at all.</li>
<li>If the JAR file is not found where <code>@CLASSPATH</code> parameter says it is, Java fallbacks to the <code>CLASSPATH</code> and tries to find the class and method as per the <code>@script</code> parameter. It does NOT try and find the JAR file.</li>
</ul>

<h2 id="summary">Summary</h2>

<p>In this blog post we discussed:</p>

<ul>
<li>Java packages</li>
<li>Classpaths</li>
<li>JAR files</li>
</ul>

<h4 id="java-packages-1">Java Packages</h4>

<p>We use Java packages to avoid naming conflicts and to keep &ldquo;like-minded&rdquo; things together. The compiled <code>.class</code> file needs to be in a subdirectory with the same name as the package, and we call it from SQL Server as so: <code>@script = N'packagename.classname.methodname</code>. If the package is like <code>p1.p2</code>, then we need two directories <code>p1/p2</code> and call it like: <code>@script = N'p1.p2.classname.methodname'</code>.</p>

<h4 id="classpaths">Classpaths</h4>

<p>In <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a> we discussed the <code>CLASSPATH</code> environment variable and said it indicated to Java where our applications are located. In this post, we saw how we could have our applications at other locations and indicate where they are by using the <code>@CLASSPATH</code> parameter. When using the <code>@CLASSPATH</code> parameter, we need to ensure that the path has the right permissions: <code>READ</code> for the <code>ALL APPLICATION PACKAGES</code> group.</p>

<p>If we cannot find the method (<code>ClassName.method</code>) in the location indicated by the <code>@CLASSPATH</code> parameter, we fall back to the location of the <code>CLASSPATH</code> environment variable.</p>

<h4 id="jar-files-1">JAR Files</h4>

<p>We use JAR files to typically aggregate many Java class files and associated metadata and resources into one file for distribution. When we use JAR files from SQL Server, we need to explicitly set the path to the JAR file (including the filename) via the <code>@CLASSPATH</code> parameter.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server ML Services - Multiple Input Data Sets]]></title>
    <link href="http://nielsberglund.com/2018/12/24/sql-server-ml-services---multiple-input-data-sets/" rel="alternate" type="text/html"/>
    <updated>2018-12-24T06:51:12+02:00</updated>
    <id>http://nielsberglund.com/2018/12/24/sql-server-ml-services---multiple-input-data-sets/</id>
    <content type="html"><![CDATA[<p>This post came about due to a question on the <a href="https://social.msdn.microsoft.com/Forums/en-US/home?forum=MicrosoftR">Microsoft Machine Learning Server</a> forum. The <a href="https://social.msdn.microsoft.com/Forums/en-US/70e3577c-58f1-40af-9f0b-546c8b181cfa/sql-server-r-services-are-more-input-datasets-planned-or-under-development?forum=MicrosoftR">question</a> was if there are any plans by Microsoft to support more the one input dataset (<code>@input_data_1</code>) in <code>sp_execute_external_script</code>. My immediate reaction was that if you want more than one dataset, you can always connect from the script back into the database, and retrieve data.</p>

<p>However, the poster was well aware of that, but due to certain reasons he did not want to do it that way - he wanted to push in the data, fair enough. When I read this, I seemed to remember something from a while ago, where, instead of retrieving data from inside the script, they pushed in the data, serialized it as an output parameter and then used the binary representation as in input parameter (yeah - this sounds confusing, but bear with me). I did some research (read Googling), and found <a href="https://stackoverflow.com/questions/42918990/how-to-pass-two-sql-tables-as-input-parameter-for-r-codes-in-sql-server">this</a> StackOverflow question, and answer. So for future questions, and for me to remember, I decided to write a blog post about it.</p>

<p></p>

<blockquote>
<p><strong>DISCLAIMER:</strong> <em>I want to make it perfectly clear that the method outlined in this post is NOT my idea, but - as mentioned above - comes from the StackOverflow <a href="https://stackoverflow.com/questions/42918990/how-to-pass-two-sql-tables-as-input-parameter-for-r-codes-in-sql-server">answer</a> by <a href="https://twitter.com/@brandonmoretz">Brandon Moretz</a>.</em></p>
</blockquote>

<h2 id="recap">Recap</h2>

<p>We start with a recap about how we pass/retrieve data for external scripts. In <a href="/2018/03/07/microsoft-sql-server-r-services---sp_execute_external_script---i/">Microsoft SQL Server R Services - sp_execute_external_script - I</a> we discussed, among other things, the <code>sp_execute_external_script</code>&rsquo;s <code>@input_data_1</code> parameter and how it specifies the input data used by the external script in the form of a Transact-SQL query:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
            @language = N'R',
            @script = N'
                iris_dataset &lt;- InputDataSet
                setosa &lt;- iris_dataset[iris_dataset$Species == &quot;setosa&quot;,]
                meanSepWidth &lt;- mean(setosa$SepalWidth)
                cat(paste(&quot;Seposa sepal mean width: &quot;, meanSepWidth))',
            @input_data_1 = N'SELECT * FROM dbo.tb_irisdata_full'
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Using @input_data_1 with straight SELECT</em></p>

<p>In <em>Code Snippet 1</em> we see how the <code>@input_data_1</code> parameter contains a <code>SELECT</code> statement against a table, and the statement executes during the <code>sp_execute_external_script</code> execution. The dataset generated by the query is referred to in the script as <code>InputDataSet</code>. The query has to be based on a <code>SELECT</code>, but it does not have to be against a table, it can be against a view or a user-defined function as well.</p>

<blockquote>
<p><strong>NOTE:</strong> The code in <em>Code Snippet 1</em> above, and <em>Code Snippet 2</em> below, uses tables and data from the <a href="/2018/03/07/microsoft-sql-server-r-services---sp_execute_external_script---i/">Microsoft SQL Server R Services - sp_execute_external_script - I</a> post.</p>
</blockquote>

<p>That is all well and good, but what if you want to push in multiple datasets? Seeing that the parameter we use to define the data ends with a <code>1</code>, it would be logical to think there are <code>@input_data_2</code>, <code>3</code>, and so on - but no, that is not the case. To process more than one dataset in the external script, we have to pull the dataset(s) from inside the script. To do this, we can use <code>ODBC</code> (in R it is the <code>RODBC</code> package, in Python <code>pyodbc</code>) or the highly optimized Microsoft <a href="https://docs.microsoft.com/en-us/machine-learning-server/r-reference/revoscaler/revoscaler"><strong><code>RevoScaleR</code></strong></a> package (in Python <strong><code>revoscalepy</code></strong>). In the following example I use <code>RevoScaleR</code>:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
        @language = N'R',
        @script = N'
          # set up connection string       
          sqlConnString &lt;- &quot;Driver=SQL Server;server=win10-dev;
                            database=IrisTestDb;uid=&lt;some_uid&gt;;pwd=&lt;some_pwd&gt;&quot;
          
          # define the data
          mydata &lt;- RxSqlServerData(table = NULL, 
                                    sqlQuery = &quot;SELECT * 
                                    FROM dbo.tb_irisdata_uneven&quot;, 
                                    connectionString = sqlConnString)

          # open the dataset
          rxOpen(mydata)

          # read the data
          iris_uneven &lt;- rxReadNext(mydata)
          versicolor &lt;- iris_uneven[iris_uneven$Species == &quot;versicolor&quot;,]
          meanSepWidthVersi &lt;- mean(versicolor$SepalWidth)
          
          # get the data from the input data set
          iris_dataset &lt;- InputDataSet
          setosa &lt;- iris_dataset[iris_dataset$Species == &quot;setosa&quot;,]
          meanSepWidth &lt;- mean(setosa$SepalWidth)

          # output the data
          cat(paste(&quot;Seposa sepal mean width:&quot;, meanSepWidth, &quot;.&quot;, 
                     &quot;Versicolor sepal mean width:&quot;, meanSepWidthVersi))',
        @input_data_1 = N'SELECT * FROM dbo.tb_irisdata_even';
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Multiple Data Sets using RevoScaleR</em></p>

<p>The code in <em>Code Snippet 2</em> shows how we read in data within the script by the use of <code>RevoScaleR</code> functionality together with the data from the <code>@input_data_1</code> parameter, and how we subsequently calculate <code>mean</code> on the two datasets.</p>

<p>So above we see how we can use multiple datasets, but what if we for some or another do not want to / cannot connect from the script to the database, as per the <a href="https://social.msdn.microsoft.com/Forums/en-US/70e3577c-58f1-40af-9f0b-546c8b181cfa/sql-server-r-services-are-more-input-datasets-planned-or-under-development?forum=MicrosoftR">question</a> above? Well, that is what we cover in this post.</p>

<h2 id="multiple-datasets">Multiple Datasets</h2>

<p>What we want to do is to push data from three completely different tables into the external script, and then in the script do something. We use <code>@input_data_1</code> to push one of the datasets, and we look at two different ways to push in the data from the other two tables. The tables we read data from are three system tables, this way we do not need to create separate tables and load data etc.:</p>

<ul>
<li><code>sys.tables</code>.</li>
<li><code>sys.databases</code>.</li>
<li><code>sys.columns</code>.</li>
</ul>

<p>What we want to do in the script is to use R/Python functionality to print out the number of rows that we push in from the separate tables, so we start with some code like this:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language = N'R'
, @script = N'
 df &lt;- InputDataSet
 nmbrRows &lt;- nrow(df)
 cat(paste(&quot;Number rows @input_data_1: &quot;, nmbrRows))'
, @input_data_1 = N'SELECT * FROM sys.columns' 
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Pushing Data via @input_data_1 in R</em></p>

<p>There is nothing strange in <em>Code Snippet 3</em>, we see how we do the <code>SELECT</code> in <code>@input_data_1</code>, and how we in the script:</p>

<ul>
<li>Assign the <code>InputDataSet</code> to a variable: <code>df</code>.</li>
<li>Use the R function <code>nrow</code> to get the number of rows in the data frame (<code>df</code>).</li>
<li>Print it out to the console.</li>
</ul>

<p>When we execute the code in <em>Code Snippet 3</em>, the result is like so:</p>

<p><img src="/images/posts/sql_ml_multidata_input1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Input Data 1 Result</em></p>

<p>From <em>Figure 1</em> we see that the dataset contains 1070 rows.</p>

<p>If we want to do this in Python the code looks like this:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language = N'Python'
, @script = N'
df = InputDataSet
nmbrRows = len(df.index)
print(&quot;Number rows @input_data_1: &quot;, nmbrRows)'
, @input_data_1 = N'SELECT * FROM sys.columns' 
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Pushing Data via @input_data_1 in Python</em></p>

<p>Nothing strange in <em>Code Snippet 4</em> either. The one noteworthy thing is the use of <code>len(df.index)</code>, instead of <code>count()</code> or <code>shape</code>. I use <code>len(df.index)</code> as &ldquo;people in the know&rdquo; says it performs better. When we execute the code in <em>Code Snippet 4</em>, we get the same result as we see in <em>Figure 1</em>.</p>

<p>Ok, so the code in the two code snippets above, (3 and 4), is the base code for what we want to do. Now we need to figure out how to push two more datasets into the scripts, and there are two ways to do that:</p>

<ul>
<li>JSON.</li>
<li>Binary serialization.</li>
</ul>

<p>Before we look at those two ways, let us discuss briefly a requirement for us to be able to do what we want, and that is the use of the <code>@params</code> parameter in <code>sp_execute_external_script</code>.</p>

<h4 id="params">@params</h4>

<p>The <code>@params</code> parameter is an optional parameter, and when defined it contains a comma separated list of the definitions of all parameters embedded in the values for the <code>@input_data_1</code> and the <code>@script</code> parameters. The string must be either a Unicode constant or a Unicode variable. Each parameter definition consists of a parameter name and a data type. The parameters defined in the <code>@params</code> list need to be added as named parameters to the stored procedure, and in the case of parameters for the external script; the script references the parameters by name but without the <code>@</code> sign.</p>

<p>So, when we push data into the script, either by using JSON or binary serialization we define a parameter for the data which we then reference in the script.</p>

<blockquote>
<p><strong>NOTE:</strong> If you want to get the inner workings of the <code>@params</code> parameter, have a look at my blog post: <a href="/2018/03/11/microsoft-sql-server-r-services---sp_execute_external_script---ii/">Microsoft SQL Server R Services - sp_execute_external_script - II</a>.</p>
</blockquote>

<p>Enough of this preamble, let us get going.</p>

<h4 id="json">JSON</h4>

<p>With the release of SQL Server 2016, Microsoft added support for JSON text processing. Microsoft added JSON functions to SQL Server, which enable you to analyze and query JSON data, transform JSON to relational format, and export SQL query results as JSON text. A typical query producing JSON text can look like this:</p>

<pre><code class="language-sql">SELECT name, object_id, schema_id 
FROM sys.tables
FOR JSON AUTO; 
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Retrieve JSON Data</em></p>

<p>We see how I use the <code>FOR JSON AUTO</code> syntax in <em>Code Snippet 5</em> to indicate to SQL Server I want JSON formatted data as the resultset. You do not necessarily need to use <code>AUTO</code>, as there are other options. To see more of this look here: <a href="https://docs.microsoft.com/en-us/sql/relational-databases/json/format-query-results-as-json-with-for-json-sql-server?view=sql-server-2016">Format Query Results as JSON with FOR JSON (SQL Server)</a>.</p>

<p>I limit the columns I select to get a more readable output. When I execute and click on the result I see:</p>

<p><img src="/images/posts/sql_ml_multidata_simple_json.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>JSON Result</em></p>

<p>So in <em>Figure 2</em> we see how my <code>SELECT</code> query results in JSON data.</p>

<p>To solve our problem how to push in multiple datasets, we can now use the JSON formatted data together with the <code>@params</code> parameter to define two parameters containing JSON. For example, <code>@sysTables</code> and <code>@sysDatabases</code>:</p>

<pre><code class="language-sql">DECLARE @sysTabs nvarchar(max) = (SELECT * FROM sys.tables FOR JSON AUTO);
DECLARE @sysDbs nvarchar(max) = (SELECT * FROM sys.databases FOR JSON AUTO);

EXEC sp_execute_external_script
@language = N'Python'
, @script = N'
df = InputDataSet
nmbrRows = len(df.index)
print(&quot;Number rows @input_data_1: &quot;, nmbrRows)'
, @input_data_1 = N'SELECT * FROM sys.columns'
, @params = N'@sysTables nvarchar(max), @sysDatabases nvarchar(max)'
, @sysTables = @sysTabs
, @sysDatabases = @sysDbs;
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Python Push JSON</em></p>

<p>In <em>Code Snippet 6</em> we see how we:</p>

<ul>
<li>Declare two variables: <code>@sysTabs</code> and <code>@sysDbs</code>, both of type <code>nvarchar(max)</code>, and how we load data into them.</li>
<li>Declare two parameters in the <code>@params</code> parameter list: <code>@sysTables</code> and <code>@sysDatabases</code>.</li>
<li>Define the two parameters and assign <code>@sysTabs</code> and <code>@sysDbs</code> to them.</li>
</ul>

<p>If we were to execute, all would work - but we have not done anything with the parameters in the script. What we need to do is to parse the incoming JSON text into a data frame somehow. To do this in Python, we use the <code>pandas</code> package as it has various functions for parsing JSON.</p>

<blockquote>
<p><strong>NOTE:</strong> The <code>pandas</code> package makes it easy to work with relational data. To find out more about it, go to <a href="https://pandas.pydata.org/pandas-docs/stable/">here</a>.</p>
</blockquote>

<p>Anyway, the function from the <code>pandas</code> package we use is <code>read_json</code>:</p>

<pre><code class="language-sql">DECLARE @sysTabs nvarchar(max) = (SELECT * FROM sys.tables FOR JSON AUTO);
DECLARE @sysDbs nvarchar(max) = (SELECT * FROM sys.databases FOR JSON AUTO);
EXEC sp_execute_external_script
@language = N'Python'
, @script = N'

import pandas as pd

df= InputDataSet
sysTab = pd.read_json(sysTables)
sysDb = pd.read_json(sysDatabases)

print(&quot;Number rows @input_data_1: &quot;, len(df.index))
print(&quot;Number rows @sysTables: &quot;, len(sysTab.index))
print(&quot;Number rows @sysDatabases: &quot;, len(sysDb.index))'
, @input_data_1 = N'SELECT * FROM sys.columns'
, @params = N'@sysTables nvarchar(max), @sysDatabases nvarchar(max)'
, @sysTables = @sysTabs
, @sysDatabases = @sysDbs;
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Python Parse JSON</em></p>

<p>The code in <em>Code Snippet 7</em> shows how we:</p>

<ul>
<li>Import the <code>pandas</code> package and give it an alias: <code>pd</code>.</li>
<li>Assign the <code>InputDataSet</code> parameter to the <code>df</code> variable as before.</li>
</ul>

<p>We then do the &ldquo;heavy lifting&rdquo; (or rather <code>pandas</code> does), where we transform the JSON text to data frames, by the use of <code>read_json</code>. From the three data frames, we finally print out the number of rows per table. The result when we execute looks like so:</p>

<p><img src="/images/posts/sql_ml_multidata_python_parse_json.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Result Python read_json</em></p>

<p>It all works! Oh, it just so happens that I am on an almost new SQL Server instance, and that is why the number of rows in <code>sys.tables</code> and <code>sys.databases</code> is the same.</p>

<p>If R is your &ldquo;poison of choice&rdquo;, then we can, for example, use the <code>jsonlite</code> package. You can read more about it <a href="https://cran.r-project.org/web/packages/jsonlite/jsonlite.pdf">here</a>. What we use from <code>jsonlite</code> is the <code>fromJSON</code> function:</p>

<pre><code class="language-sql">DECLARE @sysTabs nvarchar(max) = (SELECT * FROM sys.tables FOR JSON AUTO);
DECLARE @sysDbs nvarchar(max) = (SELECT * FROM sys.databases FOR JSON AUTO);
EXEC sp_execute_external_script
  @language = N'R'
, @script = N'
    require(jsonlite)
    library(jsonlite)

    df &lt;- InputDataSet
    sysTab &lt;- as.data.frame(fromJSON(sysTables))
    sysDb &lt;- as.data.frame(fromJSON(sysDatabases))

    cat(paste(&quot;Number rows @input_data_1: &quot;, nrow(df)))
    cat(paste(&quot;\nNumber rows @sysTables: &quot;, nrow(sysTab)))
    cat(paste(&quot;\nNumber rows @sysDatabases: &quot;, nrow(sysDb)))'
, @input_data_1 = N'SELECT * FROM sys.columns'
, @params = N'@sysTables nvarchar(max), @sysDatabases nvarchar(max)'
, @sysTables = @sysTabs
, @sysDatabases = @sysDbs;
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Parse JSON in R</em></p>

<p>The code in <em>Code Snippet 8</em>, looks very similar to what is in <em>Code Snippet 7</em>, and in <em>Code Snippet 8</em> we:</p>

<ul>
<li>Load the <code>jsonlite</code> package.</li>
<li>Assign the <code>InputDataSet</code> parameter to the <code>df</code> variable as before.</li>
<li>Parse the JSON  into data frames using <code>fromJSON</code>.</li>
</ul>

<p>We have now seen how we, both in Python as well as R, can use JSON to push multiple datasets into external scripts, and - as I mentioned above - JSON is one way of doing it. Now onto the next.</p>

<h4 id="binary-serialization">Binary Serialization</h4>

<p>When we use the binary serialization method of pushing multiple datasets, we use R and Python&rsquo;s built-in functionality for serialization and deserialization. In Python, it is with the help of the <code>pickle</code> module, and in R the <code>serialize</code> and <code>unserialize</code> methods.</p>

<p>Initially, binary serialization looks somewhat more complicated than JSON (and it might be), especially since the deserialization happens against a binary parameter serialized with R or Python. In other words, we need to make a roundtrip to R/Python to get the binary representation of the data, so having a helper procedure to do this sounds like a good idea to me:</p>

<pre><code class="language-sql">CREATE PROCEDURE dbo.pr_SerializeHelperR 
                       @query nvarchar(max)
                     , @serializedResult varbinary(max) OUT
AS

EXEC sp_execute_external_script
  @language = N'R'
, @script = N'
  serRes &lt;- serialize(InputDataSet, NULL)'
, @input_data_1 = @query
, @params = N'@serRes varbinary(max) OUT'  
, @serRes = @serializedResult OUT;
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>R Serialization Helper</em></p>

<p>In <em>Code Snippet 9</em> we see an R serialization helper procedure:</p>

<ul>
<li>It takes a query as an input parameter (<code>query</code>).</li>
<li>It has an output parameter which is the serialized result set.</li>
</ul>

<p>The body of the procedure is a call to <code>sp_execute_external_script</code>, where the <code>@query</code> parameter acts as <code>@input_data_1</code>, and we have a defined output parameter <code>@serRes</code>. In the script, we call the R <code>serialize</code> function on the pushed in dataset, and assigns it to the output parameter. The flow:</p>

<ul>
<li>We pass in a query statement.</li>
<li>During the call to <code>sp_execute_external_script</code> the query is run, and the resulting data set passed into the external script.</li>
<li>The external script serializes the dataset and passes it back out as an output parameter.</li>
</ul>

<p>The equivalent Python serialization helper looks like so:</p>

<pre><code class="language-sql">CREATE PROCEDURE dbo.pr_SerializeHelperPy  
                          @query nvarchar(max)
                        , @serializedResult varbinary(max) OUT
AS

EXEC sp_execute_external_script
  @language = N'Python'
, @script = N'
import pickle as p 
serRes = p.dumps(InputDataSet)'
, @input_data_1 = @query
, @params = N'@serRes varbinary(max) OUT'  
, @serRes = @serializedResult OUT;
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Python Serialization Helper</em></p>

<p>We see in <em>Code Snippet 10</em> how we bring in the <code>pickle</code> module, and then serialize the dataset with the <code>dumps</code> function.</p>

<p>The equivalent of <em>Code Snippet 7</em> using the Python serialization helper looks like so:</p>

<pre><code class="language-sql">-- declare the variables for queries as well 
-- as serialized binary representation
DECLARE @sysTabs nvarchar(max) = 
            'SELECT name, object_id, schema_id 
             FROM sys.tables';
DECLARE @sysDbs nvarchar(max) = 
            'SELECT name, database_id, source_database_id 
             FROM sys.databases';
DECLARE @sysTabsBin varbinary(max);
DECLARE @sysDbsBin varbinary(max);

-- get the serialized result of @sysTabs
EXEC dbo.pr_SerializeHelperPy   @query = @sysTabs
                              , @serializedResult = @sysTabsBin OUT

-- get the serialized result of @sysDbs
EXEC dbo.pr_SerializeHelperPy   @query = @sysDbs
                              , @serializedResult = @sysDbsBin OUT

-- do the &quot;real&quot; stuff
EXEC sp_execute_external_script
@language = N'Python'
, @script = N'
import pandas as pd
import pickle as p 

df = InputDataSet
# this deserializes the sys.tables result
sysTab = p.loads(sysTables)
# this deserializes the sys.databases result
sysDb = p.loads(sysDatabases)
print(&quot;Number rows @input_data_1: &quot;, len(df.index))
print(&quot;Number rows @sysTables: &quot;, len(sysTab.index))
print(&quot;Number rows @sysDatabases: &quot;, len(sysDb.index))'
, @input_data_1 = N'SELECT * FROM sys.columns'
, @params = N'@sysTables varbinary(max), @sysDatabases varbinary(max)'
, @sysTables = @sysTabsBin
, @sysDatabases = @sysDbsBin;
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Implementation of Python Serialization</em></p>

<p>In <em>Code Snippet 11</em> we see how we:</p>

<ul>
<li>Declare the variables for the queries as well as the serialized results of the queries.</li>
<li>Execute the helper procedure for the two queries we want the results serialized for.</li>
<li>Have defined two <code>varbinary(max)</code> parameters in the <code>@params</code> parameter of <code>sp_execute_external_script</code>.</li>
<li>Assign the serialized values to those two parameters.</li>
<li>Execute <code>sp_execute_external_script</code> and send in the two serialized results as well as <code>@input_data_1</code>.</li>
<li>In <code>sp_execute_external_script</code> we deserialize the results using <code>pickle.loads</code>, or rather <code>p.loads</code> where <code>p</code> is the alias for <code>pickle</code>.</li>
</ul>

<p>The code for an R implementation looks almost the same except that we call the R <code>unserialize</code> function instead of <code>pickle.loads</code>, as per the code below:</p>

<pre><code class="language-sql">-- declare the variables for the querie as well 
-- as serialized binary representation
DECLARE @sysTabs nvarchar(max) = 'SELECT name, object_id, schema_id FROM sys.tables';
DECLARE @sysTabsBin varbinary(max);

-- get the serialized result of @sysTabs
EXEC dbo.pr_SerializeHelperR   @query = @sysTabs
                              , @serializedResult = @sysTabsBin OUT

-- do the &quot;real&quot; stuff
EXEC sp_execute_external_script
@language = N'R'
, @script = N'

sysTab = unserialize(sysTables)
cat(paste(&quot;Number rows @sysTables: &quot;, nrow(sysTab)))'
, @params = N'@sysTables varbinary(max)'
, @sysTables = @sysTabsBin;
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>Implementation of R Serialization</em></p>

<p>We see in <em>Code Snippet 12</em> how we push in one resultset, and we do not use <code>@input_data_1</code>. Instead, we serialize the resultset from the query with the R helper procedure and then deserialize with the <code>unserialize</code> function.</p>

<h2 id="performance">Performance</h2>

<p>So, we have now seen two ways of pushing in datasets to an external script: JSON and Binary Serialization. Which should you choose - I mean, JSON seems a lot easier? The answer comes down to performance.</p>

<p>To look at performance let us create a database, a table and some data:</p>

<pre><code class="language-sql">DROP DATABASE IF EXISTS MultiDataSetDB;
GO
CREATE DATABASE MultiDataSetDB;
GO

USE MultiDataSetDB
GO 

SET NOCOUNT ON;
GO

DROP TABLE IF EXISTS dbo.tb_Rand1M ;
CREATE TABLE dbo.tb_Rand1M
(
  RowID bigint identity,
  y int NOT NULL, 
  rand1 int NOT NULL, 
  rand2 int NOT NULL, 
  rand3 int NOT NULL, 
  rand4 int NOT NULL, 
  rand5 int NOT NULL,
  CONSTRAINT [pk_Rand1M] PRIMARY KEY (RowID)
)
GO
INSERT INTO dbo.tb_Rand1M(y, rand1, rand2, rand3, rand4, rand5)
SELECT TOP(1000000) CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 25 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 50 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 100 AS INT)
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>Database with Table and Data</em></p>

<p>We see above, in <em>Code Snippet 13</em>, how we create a database, with a table and we load one million records into the table. The data is entirely random and fairly useless, but it serves its purposes.</p>

<blockquote>
<p><strong>NOTE:</strong> If you code along and use the code above, please also create <code>dbo.pr_SerializeHelperPy</code>, which you see in <em>Code Snippet 10</em>, in the <code>MultiDataSetDB</code> database.</p>
</blockquote>

<p>Having created the database and the objects, let us look at the code we use to compare performance. The code below is for JSON:</p>

<pre><code class="language-sql">--this is for JSON
DECLARE @start datetime2 = SYSUTCDATETIME();
DECLARE @rand1M nvarchar(max) = (SELECT TOP(100) * 
                           FROM dbo.tb_Rand1M FOR JSON AUTO);
EXEC sp_execute_external_script
@language = N'Python'
, @script = N'
import pandas as pd

df = pd.read_json(randTab)
print(&quot;Number rows @randTab: &quot;, len(df.index))'
, @params = N'@randTab nvarchar(max)'
, @randTab = @rand1M;
SELECT DATEDIFF(ms, @start, SYSUTCDATETIME()) AS JSONTime; 
GO
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>JSON Performance Code</em></p>

<p>And here is the binary serialization code:</p>

<pre><code class="language-sql">--this is binary
-- declare the variables for queries as well 
--as serialized binary representation
DECLARE @start datetime2 = SYSUTCDATETIME();
DECLARE @rand1M nvarchar(max) = 'SELECT TOP(100) * 
                                FROM dbo.tb_Rand1M';
DECLARE @rand1MBin varbinary(max);
-- get the serialized result of @sysTabs
EXEC dbo.pr_SerializeHelperPy   @query = @rand1M
                              , @serializedResult = @rand1MBin OUT

-- do the &quot;real&quot; stuff
EXEC sp_execute_external_script
@language = N'Python'
, @script = N'
import pandas as pd
import pickle as p 

sysTab = p.loads(randTab)

print(&quot;Number rows @sysTables: &quot;, len(sysTab.index))'
, @params = N'@randTab varbinary(max)'
, @randTab = @rand1MBin
SELECT DATEDIFF(ms, @start, SYSUTCDATETIME()) AS BinaryTime;
GO
</code></pre>

<p><strong>Code Snippet 15:</strong> <em>Binary Serialization Performance Code</em></p>

<p>The code in snippets 14, and 15 is a variant of what we have seen so far. In these two snippets, we only push in one table, and we look at the time it takes to do it. We see how we <code>SELECT</code> from <code>dbo.tb_Rand1M</code>, and initially, we do a <code>TOP(100)</code>.</p>

<p>When I highlight both code snippets and run them a couple of times to not incur &ldquo;startup&rdquo; costs the results are:</p>

<p><img src="/images/posts/sql_ml_multidata_perf1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Performance 100 Rows</em></p>

<p>That&rsquo;s quite interesting; we see in <em>Figure 4</em> how JSON serialization is around twice as fast as binary serialization. Ok, what if we did it on 1000 rows? With 1000 rows JSON is still about twice as fast. However, when we tun it against the full table (one million rows), the results are different:</p>

<p><img src="/images/posts/sql_ml_multidata_perf2.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Performance a Million Rows</em></p>

<p>In <em>Figure 5</em> we see how with a million rows, the binary serialization is about 3.5 times faster than JSON. There are a couple of reasons why binary serialization performs better that JSON with larger datasets:</p>

<ul>
<li>Binary serialization is more compact, less data to transfer.</li>
<li>By using the <code>@input_data_1</code> parameter to push in the data to the serialization we get a better performing transport. Read more about it in my post: <a href="/2017/11/25/microsoft-sql-server-r-services---internals-xiv/">Microsoft SQL Server R Services - Internals XIV</a>.</li>
</ul>

<p>So, for very small datasets, use the JSON method, but for larger datasets, the binary serialization is always preferred. Another thing to keep in mind is if you use both <code>@input_data_1</code> as well as pushing in serialized data (like in all our code snippets where we used three tables), try to use <code>@input_data_1</code> for the biggest dataset. That way you get the better performing transport and also, potentially, parallel execution of the query.</p>

<h2 id="summary">Summary</h2>

<p>We have now seen how we can push in multiple datasets to an external script, without having to connect from the script back to the database. We can use two methods:</p>

<ul>
<li>JSON.</li>
<li>Binary serialization.</li>
</ul>

<p>When we use JSON we utilize SQL Server&rsquo;s JSON capabilities to execute a query and receive the result as JSON formatted text: <code>SELECT ... FROM ... FOR JSON AUTO</code>. In the external script we then deserialize the JSON using:</p>

<ul>
<li>In R the <code>fromJSON</code> function in the <code>jsonlite</code> package.</li>
<li>In Python the <code>read_json</code> function in the <code>pandas</code> package.</li>
</ul>

<p>When we do binary serialization we use R/Python&rsquo;s capabilities to both serialize as well as deserialize data. This means we need to do a roundtrip to R/Python to serialize the data. The tools we use:</p>

<ul>
<li>In R we call <code>serialize</code> and <code>unserialize</code>.</li>
<li>In Python we use functions from the <code>pickle</code> package. To serialize we call <code>dumps</code> and to deserialize we use <code>loads</code>.</li>
</ul>

<p>What method to use (JSON or binary serialization) comes down to, in my mind, performance. For very small datasets JSON is faster, but as soon as the dataset gets bigger binary serialization outperforms JSON by order of magnitude.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Extensibility Framework &amp; Java - Null Values]]></title>
    <link href="http://nielsberglund.com/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/" rel="alternate" type="text/html"/>
    <updated>2018-12-19T17:12:39+02:00</updated>
    <id>http://nielsberglund.com/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/</id>
    <content type="html"><![CDATA[<p>This post is the third post in a series where I look at the Java extension in SQL Server, i.e. the ability to execute Java code from inside SQL Server.</p>

<p>To see what other posts there are in the series, go to <a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a>.</p>

<p>In this post we look at something related to the <a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">data passing post</a>; how to handle null values.</p>

<p></p>

<blockquote>
<p><strong>DISCLAIMER:</strong> <em>This post contains Java code. I am not a Java guy, in fact, the only Java I have ever written is the code in this post and the previous SQL Server 2019 Java posts. So, the code is not elegant in any shape or form, and I am absolutely certain it can be done in a much better way. However, this is not about Java as such, but how you call Java code from SQL Server, and what you need to implement on the Java side.</em></p>
</blockquote>

<p>Before we dive into this post&rsquo;s topic, let us do a recap.</p>

<h2 id="recap">Recap</h2>

<p>In the <a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">SQL Server 2019 Extensibility Framework &amp; Java - Passing Data</a> post, we looked at how we pass data back and forth between SQL Server and Java. In the Java extensions we do not have the <code>InputDataSet</code>, and <code>OutputDataSet</code> variables, so we need to define class member arrays for the columns we send in and pass back out, as well as a variable indicating the number of columns we return:</p>

<ul>
<li><strong><code>inputDataCol</code></strong><strong><em><code>N</code></em></strong>: array variable representing the input columns, where <em>N</em> is the column number (1 based).</li>
<li><strong><code>outputDataCol</code></strong><strong><em><code>N</code></em></strong>: array variable representing the output columns, where <em>N</em> is the column number (1 based).</li>
<li><strong><code>numberofOutputCols</code></strong>: it represents the number of columns returned from Java, and it is always required - regardless if you return columns or not.</li>
</ul>

<p>In addition to these variables we need two variables mapping null values:</p>

<ul>
<li><strong><code>inputNullMap</code></strong>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
<li><strong><code>outputNullMap</code></strong>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
</ul>

<p>All the <code>input</code><em><code>xxx</code></em> variables get populated automatically, whereas you need to populate the <code>output</code><em><code>xxx</code></em> variables in the code.</p>

<p>In the <a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">post</a> we had some example code looking like so:</p>

<pre><code class="language-java">public class DataPassing {
  //input data variables 
  static public int[] inputDataCol1 = new int[1];
  static public int[] inputDataCol2 = new int[1];
  static public int[] inputDataCol3 = new int[1];
  static public boolean[][] inputNullMap = new boolean[1][1];

  //output variables
  static public int[] outputDataCol1;
  static public int[] outputDataCol2;
  static public int[] outputDataCol3;
  static public boolean[][] outputNullMap;

  static public short numberOfOutputCols;

  public static void bar() {
    
    int numRows = inputDataCol1.length;
    numberOfOutputCols = 3;

    outputDataCol1 = new int[numRows];
    outputDataCol2 = new int[numRows];
    outputDataCol3 = new int[numRows];

    for(int x = 0; x &lt; numRows; x++) {
      outputDataCol1[x] = inputDataCol1[x];
      outputDataCol2[x] = inputDataCol2[x];
      outputDataCol3[x] = inputDataCol3[x];
    }

    outputNullMap = new boolean[numberOfOutputCols][numRows];
  }

}
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Example Code</em></p>

<p>The code we see in <em>Code Snippet 1</em> represents a class to which we pass in a data set consisting of three columns. The class passes back a three column data set. What the code does should be pretty self-explanatory, but there are two array variables that we are not doing much with: <code>inputNullMap</code> and <code>outputNullMap</code>, and today we look at them.</p>

<h2 id="demo-data">Demo Data</h2>

<p>In today&rsquo;s post, we use some data from the database, so let us set up the necessary database, a table, and load data into the table:</p>

<pre><code class="language-sql">USE master;
GO
SET NOCOUNT ON;
GO
DROP DATABASE IF EXISTS JavaNullDB;
GO
CREATE DATABASE JavaNullDB;
GO
USE JavaNullDB;
GO
DROP TABLE IF EXISTS dbo.tb_NullRand10
CREATE TABLE dbo.tb_NullRand10(RowID int identity primary key, 
                          x int, y int, col1 nvarchar(50));
GO
INSERT INTO dbo.tb_NullRand10(x, y, col1)
SELECT TOP(10) CAST(ABS(CHECKSUM(NEWID())) % 14 AS int) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS int)
  , N'Hello ' + CAST(CAST(ABS(CHECKSUM(NEWID())) % 25 AS int) AS nvarchar(50))
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO
UPDATE dbo.tb_NullRand10
  SET y = NULL
WHERE RowId = 3
UPDATE dbo.tb_NullRand10
  SET Col1 = NULL
WHERE RowId = 5    
UPDATE dbo.tb_NullRand10
  SET x = NULL
WHERE RowId = 6 
UPDATE dbo.tb_NullRand10
  SET y = NULL
WHERE RowId = 8  
UPDATE dbo.tb_NullRand10
  SET col1 = NULL
WHERE RowId = 9 
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Database Objects</em></p>

<p>We see from <em>Code Snippet 2</em> how we:</p>

<ul>
<li>Create a database: <code>JavaNullDB</code>.</li>
<li>Create a table: <code>dbo.tb_NullRand10</code>.</li>
<li>Insert some data into the table.</li>
</ul>

<p>The data we insert is entirely random, but it gives us something to &ldquo;play around&rdquo; with. Now, when we have a database and some data let us get started.</p>

<h2 id="null-values">Null Values</h2>

<p>So why do we care about null values? Well, the reason is that in SQL Server all data types are nullable, whereas in Java that is not the case. In Java, like in .NET, primitive types (<code>int</code>, etc.) cannot be null, so code like this:</p>

<pre><code class="language-java">public static void foo() {
    int x;
    x = null;
  }
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Null Value</em></p>

<p>When we try and compile the code in <em>Code Snippet 3</em> we get a compile-time error: <code>error: incompatible types: &lt;null&gt; cannot be converted to int</code>. In this case, the compiler saves us, but if we now think about passing in data from SQL Server, we can get into trouble as columns in the dataset can be null. So what do we do?</p>

<p>In the <strong>SQL Server 2019 Java Extension</strong> there are a couple of things helping with null values:</p>

<ul>
<li><strong>Extension components</strong>.</li>
<li><strong>Null maps</strong>.</li>
</ul>

<h4 id="extension-components">Extension Components</h4>

<p>In previous posts, I have briefly mentioned the Java extension components. They are similar to the launchers, and the &ldquo;link&rdquo; dll&rsquo;s for R/Python, and they are involved when passing data to Java as well as receiving data from Java.</p>

<blockquote>
<p><strong>NOTE:</strong> I cover these components in future posts.</p>
</blockquote>

<p>Let us try to get an understanding of what the components do when passing data to Java. We start with doing a simple <code>SELECT RowID, x, y FROM dbo.tb_NullRand10</code>, where <code>x</code> and <code>y</code> are integer columns:</p>

<p><img src="/images/posts/sql_2k19_java_null_null_result1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Select with NULL</em></p>

<p>In <em>Figure 1</em>, (highlighted in red), we see, which we also mentioned above, how primitive types are nullable in SQL. However, let us say we have some Java code looking like so:</p>

<pre><code class="language-java">public class NullValues {
  static public int[] inputDataCol1 = new int[1];
  static public int[] inputDataCol2 = new int[1];
  static public int[] inputDataCol3 = new int[1];
  static public boolean[][] inputNullMap = new boolean[1][1];

  static public short numberOfOutputCols;

  public static void foo() {
    
    for(int x = 0; x &lt; inputDataCol1.length; x++) {
    System.out.printf(&quot;%d\t\t%d\t\t%d\n&quot;, 
                        inputDataCol1[x],
                        inputDataCol2[x], 
                        inputDataCol3[x]);
    }
  }
}
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Java Code</em></p>

<p>In the code in <em>Code Snippet 4</em> we see how we expect a three-column dataset passed in, which we then print out in the <code>foo</code> method. The immediate problem is that, as we see in <em>Figure 1</em>, the dataset consists of null values, so what happens if we execute some SQL code looking like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'NullValues.foo'
, @input_data_1 = N'SELECT RowID, x, y FROM dbo.tb_NullRand10';
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>SQL Pushing in Null Values</em></p>

<p>When we run the code in <em>Code Snippet 5</em> the result looks as follows:</p>

<p><img src="/images/posts/sql_2k19_java_null_null_result2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Dataset with Null Values in Java</em></p>

<p>Ok, we did not crash and burn, that is good! However, what about the rows with the null values, <code>RowId</code>&rsquo;s 3, 6, and 8? Notice in <em>Figure 2</em> how the values printed out are set to 0 where we in <em>Figure 1</em> saw nulls. So <em>something</em> has &ldquo;automagically&rdquo; converted the nulls to 0&rsquo;s. That <em>something</em> is one of the Java extension components, and from a high level it works something like so (this is somewhat of guesswork from me):</p>

<ol>
<li>SQL Server pushes the data into the component.</li>
<li>The component loops through the data and replaces null values with the data type&rsquo;s default value.</li>
<li>The component calls into the relevant class and method and copies the data into the column variables.<br /></li>
</ol>

<p>That is nice, but now we receive 0&rsquo;s instead for nulls, what about if we want to handle null differently than 0. Think about how SQL Server handles null in some of its functions; columns with null values are ignored. With nulls replaced with 0&rsquo;s, what do we do?</p>

<h4 id="null-maps">Null Maps</h4>

<p>Null maps solve the problem of null values coming back with the default value for the type. Remember what we said in the previous post and in the <em>Recap</em> above that null maps are two-dimensional arrays indicating whether a value is null or not, and that we have two null maps:</p>

<ul>
<li><strong><code>inputNullMap</code></strong>: for data passed in.</li>
<li><strong><code>outputNullMap</code></strong>: for data returned.</li>
</ul>

<p><strong>inputNullMap</strong></p>

<p>For input data we use the <code>inputNullMap</code>, or rather one of the Java extension components populates the map, and we use it in our code like so:</p>

<pre><code class="language-java">public static void bar() {
    
  int numRows = inputDataCol1.length;
  int numCols = inputNullMap.length;
  for(int x = 0; x &lt; numRows; x++) {
    for(int y = 0; y &lt; numCols; y++) {
      System.out.printf(&quot;Null map value at row: %d, column: %d, value: %b\n&quot;, 
                      x, y, inputNullMap[y][x]);  
    }
  }
}
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Input Data Null Map</em></p>

<p>We see in <em>Code Snippet 6</em> how we have a new method: <code>bar</code>, where we loop through the <code>inputNullMap</code> array. For each row, we loop the columns and print out the boolean value indicating if a column value is null or not.</p>

<p>Let us change the code in <em>Code Snippet 5</em> to call into the <code>bar</code> method:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'NullValues.bar'
, @input_data_1 = N'SELECT RowID, x, y 
                    FROM dbo.tb_NullRand10
                    WHERE RowID IN(3, 6, 8)';
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>SQL Pushing in Null Values - II</em></p>

<p>Apart from calling into the <code>bar</code> method in <em>Code Snippet 7</em> the other difference from <em>Code Snippet 5</em> is that we only push in rows which have null values, (to keep it short). The result we see after we execute looks like this:</p>

<p><img src="/images/posts/sql_2k19_java_null_null_map1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Null Map Output</em></p>

<p>We see, highlighted in yellow in <em>Figure 3</em>, how the null map indicates which columns have null values.</p>

<p>So how do we use this? Let us assume we have code which takes a dataset and multiplies two columns together (<code>x</code> and <code>y</code>) and returns the <code>RowID</code> and the result back to the caller:</p>

<pre><code class="language-java">public class NullValues {
  static public int[] inputDataCol1 = new int[1];
  static public int[] inputDataCol2 = new int[1];
  static public int[] inputDataCol3 = new int[1];
  static public boolean[][] inputNullMap = new boolean[1][1];

  static public int[] outputDataCol1;
  static public int[] outputDataCol2;
  static public boolean[][] outputNullMap;

  static public short numberOfOutputCols;

  public static void multiplier() {
    
    int numRows = inputDataCol1.length;
    
    outputDataCol1 = new int[numRows];
    outputDataCol2 = new int[numRows];
    
    for(int i = 0; i &lt; numRows; i++) {
       outputDataCol1[i] = inputDataCol1[i];
       outputDataCol2[i] = inputDataCol2[i] * inputDataCol3[i];

    }
    
    numberOfOutputCols = 2;
    outputNullMap = new boolean[numberOfOutputCols][numRows];

  }
  ...
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Multiplier</em></p>

<p>We see in <em>Code Snippet 8</em> how we:</p>

<ul>
<li>Declare two output column arrays for the two column output dataset.</li>
<li>Declare an output null map.</li>
<li>In the <code>multiplier</code> method initialize the output column arrays.</li>
</ul>

<p>We then loop through the input data and assigns the <code>RowID</code> (<code>inputDataCol1</code>) to <code>outputDataCol1</code>, and sets the value of <code>outputDataCol2</code> to be <code>inputDataCol2 * inputDataCol3</code>. If we at this stage compiled, moved the <code>.class</code> file to the <code>CLASSPATH</code> location, and executed as per <em>Code Snippet 5</em> (obviously edit the <code>@script</code> variable to be: <code>@script = N'NullValues.multiplier'</code>), this is the result:</p>

<p><img src="/images/posts/sql_2k19_java_null_multiplier1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Multiplier Output</em></p>

<p>What we see in <em>Figure 4</em> is how the rows with original null values came back with a result of 0 as, as we know from above, the Java components replace null with 0. However, this may not be what we want; instead we want the original null value columns not to be part of the result at all. So, to not include the original null value columns we use the input null map:</p>

<pre><code class="language-java">import java.util.ArrayList;
import java.util.stream.*;
import java.util.*;

public class NullValues {
  //static variables as in Code Snippet 8 ...

  public static void multiplier2() {
    
    int numRows = inputDataCol1.length;
    int numCols = inputNullMap.length;

    List&lt;Integer&gt; rowId = new ArrayList&lt;Integer&gt;();
    List&lt;Integer&gt; result = new ArrayList&lt;Integer&gt;();

    Boolean colIsNull;
    
    for(int i = 0; i &lt; numRows; i++) {
      colIsNull = false;
      for(int y = 0; y &lt; numCols; y++) {
        if(inputNullMap[y][i] == true) {
          colIsNull = true;
          break;
        }
      }
      if(!colIsNull) {
        rowId.add(inputDataCol1[i]);
        result.add(inputDataCol2[i] * inputDataCol3[i]);
      }
    }

    int numOutRows = rowId.size();
    outputDataCol1 = new int[numOutRows];
    outputDataCol2 = new int[numOutRows];

    for(int i = 0; i &lt; numOutRows; i++) {
      outputDataCol1[i] = rowId.get(i);
      outputDataCol2[i] = result.get(i);
    }
    numberOfOutputCols = 2;
    outputNullMap = new boolean[numberOfOutputCols][numOutRows];
  }
 
  // the other methods ...
} 
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Using Input Null Map for Output Data</em></p>

<p>So in <em>Code Snippet 9</em> we now see a couple of different things compared with <em>Code Snippet 8</em>:</p>

<ul>
<li>As we do not know upfront how many rows the method returns, we use Java <code>List</code>&rsquo;s to load output data into. The two lists are for the <code>RowID</code> and the result output columns.</li>
<li>We use the <code>inputNullMap</code> to check if any column in the row we process is null. If so we break immediately out, and we go to next row.</li>
<li>If the columns for the row is not null, we add the <code>RowID</code> column (<code>inputDataCol1</code>) to the row id list: <code>rowId</code>.</li>
<li>If none of the columns in the row are null we do the multiplication of <code>x</code> and <code>y</code>, (<code>inputDataCol2</code>, <code>inputDataCol3</code>), and add the result to the <code>result</code> list.</li>
<li>We initialize the two output data column arrays with the size of the <code>rowId</code> list.</li>
<li>Finally we loop over the data in the two lists and add it to the two output data column arrays.</li>
</ul>

<p>When we now execute the code in <em>Code Snippet 5</em>, (after compiling etc.), and having changed the @script parameter to: <code>@script = N'NullValues.multiplier2'</code>, the result is as so:</p>

<p><img src="/images/posts/sql_2k19_java_null_multiplier2.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Null Rows Not Included</em></p>

<p>We see in <em>Figure 5</em> how the rows with null values are excluded from the dataset, how cool is that?! However, what about if we want null values back, like the result of this code:</p>

<pre><code class="language-sql">DECLARE @x int = NULL;
DECLARE @y int = 21;

SELECT @x * @y;
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Multiplication with NULL</em></p>

<p>When we run the code in <em>Code Snippet 10</em> we get a result of <code>NULL</code>, and that is expected in SQL Server: any operations against a <code>NULL</code> value, yields <code>NULL</code>. To &ldquo;mimic&rdquo; that behaviour we use <code>outputNullMap</code>.</p>

<p><strong>outputNullMap</strong></p>

<p>The <code>outputNullMap</code> variable is like <code>inputNullMap</code> but the opposite. What do I mean with that: remember how the <code>inputNullMap</code> variable gets populated by the Java extension components, and read by the code we write. For the <code>outputNullMap,</code> our code populates the variable and the Java extension components read it.</p>

<p>To see how this works we incorporate some of the code in <em>Code Snippet 9</em>, with the code in <em>Code Snippet 8</em>, and we create a new method <code>multiplier3</code>:</p>

<pre><code class="language-java">public static void multiplier3() {
    
  int numRows = inputDataCol1.length;
  int numCols = inputNullMap.length;

  outputDataCol1 = new int[numRows];
  outputDataCol2 = new int[numRows];
  
  Boolean colIsNull;

  numberOfOutputCols = 2;
  outputNullMap = new boolean[numberOfOutputCols][numRows];

  for(int i = 0; i &lt; numRows; i++) {
    colIsNull = false;
    for(int y = 0; y &lt; numCols; y++) {
      if(inputNullMap[y][i] == true) {
        colIsNull = true;
        break;
      }
    }
    outputDataCol1[i] = inputDataCol1[i];
    outputDataCol2[i] = inputDataCol2[i] * inputDataCol3[i];
    outputNullMap[0][i] = false;
    outputNullMap[1][i] = colIsNull;
  }
}
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Output Null Values</em></p>

<p>Some things to notice in <em>Code Snippet 11</em>:</p>

<ul>
<li>We initialise the output column arrays to the same size as the input data. We no longer need to dynamically size the output arrays, as they will contain all data passed in.</li>
<li>When we loop the input rows, we also loop the <code>inputNullMap</code> and set the <code>colIsNull</code> variable to whatever the <code>inputNullMap</code> value is.</li>
<li>Regardless if <code>colIsNull</code> is <code>true</code> or <code>false</code> we do the multiplication. I know, we could skip this if we have a null, but I am lazy.</li>
<li>We set the value for the first column in <code>outputNullMap</code> to always be false as it represents the primary key of the data.</li>
<li>The <code>outputNullMap</code> value for the second column (the result) gets the value of <code>colIsNull</code>. In other words, if a column in the row is null, we set the result column in the <code>outputNullMap</code> to be null.</li>
</ul>

<p>After compiling etc., we execute the code in <em>Code Snippet 5</em>, pointing in the <code>@script</code> parameter to our new method. The result looks like so:</p>

<p><img src="/images/posts/sql_2k19_java_null_multiplier3.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Null Passed Back</em></p>

<p>In <em>Figure 6</em> we see null values in the result set, as we would expect in an SQL Server environment. The reason we see null values (instead of 0&rsquo;s) is that the Java extension components, as I mentioned before, reads the data returned together with the null map, and creates a SQL Server result set, including null values.</p>

<h4 id="string-and-null-maps">String and Null Maps</h4>

<p>Ok, we have now seen how we use the <code>inputNullMap</code> and <code>outputNullMap</code> to handle null values, and we know we need to use the null maps for primitive types. However, what about strings, as a string is not a primitive type, and is allowed to be null in Java. Let us have a look.</p>

<blockquote>
<p><strong>NOTICE:</strong> When we push string data from SQL Server, the SQL data type has to be either <code>nchar</code> or <code>nvarchar</code>.</p>
</blockquote>

<p>We create a new class, (in its own source file), in which we expect a two column data set passed in, and a two column data set returned. We also create a method which looks more or less the same as the <code>bar</code> method in <em>Code Snippet 6</em>:</p>

<pre><code class="language-java">public class NullStringValues {
  static public int[] inputDataCol1 = new int[1];
  static public String[] inputDataCol2 = new String[1];
  static public boolean[][] inputNullMap = new boolean[1][1];

  static public int[] outputDataCol1;
  static public String[] outputDataCol2;
  static public boolean[][] outputNullMap;
  
  static public short numberOfOutputCols;

  public static void bar() {
    
    
    int numRows = inputDataCol1.length;
    int numCols = inputNullMap.length;
    for(int x = 0; x &lt; numRows; x++) {
      System.out.printf(&quot;RowID: %d\t, StringValue: %s\t, 
                         NullMapValueStringCol: %b\n&quot;, 
                        inputDataCol1[x],
                        inputDataCol2[x], 
                        inputNullMap[1][x]);

   }
  }
}
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>String and Input Null Map</em></p>

<p>The code in <em>Code Snippet 12</em> should not come as a surprise. We see how we:</p>

<ul>
<li>Have two input column arrays of which one is a <code>String</code> array.</li>
<li>Also have two output column arrays of which one is a <code>String</code> array.</li>
<li>In the <code>bar</code> method loops the rows, and prints out the column values as well as the value of the <code>inputNullMap</code> for the string column.</li>
</ul>

<p>We compile the code in <em>Code Snippet 12</em> and move the <code>.class</code> file to the <code>CLASSPATH</code> location. We then use SQL code like this:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'NullStringValues.bar'
, @input_data_1 = N'SELECT RowID, Col1 
                    FROM dbo.tb_NullRand10 
                    WHERE RowID IN(4, 5, 8, 9, 10)';
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>SQL Code for String Input</em></p>

<p>The code in <em>Code Snippet 13</em> points to the <code>bar</code> method in the <code>NullStringValues</code> class, and it pushes in five rows of which two have a null value in the string column (<code>Col1</code>). The result when we execute the code in <em>Code Snippet 13</em> is:</p>

<p><img src="/images/posts/sql_2k19_java_null_string_null_input.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>String Nulls Passed In</em></p>

<p>We see in <em>Figure 7</em> how we receive null values for the rows where the column was null. However, we also see that the null map value for those columns is set to true. So even for strings, it looks like the null map is used, at least for input data.</p>

<p>So, what about output data. Let us create a new method which populates the output column arrays with some random data:</p>

<pre><code class="language-java"> public static void foo() {
  
    int numOutRows = 3;
    outputDataCol1 = new int[numOutRows];
    outputDataCol2 = new String[numOutRows];

    numberOfOutputCols = 2;
    
    outputNullMap = new boolean[numberOfOutputCols][numOutRows];

    for(int n = 0; n &lt; numOutRows; n++) {
      outputDataCol1[n] = n+1;
      outputNullMap[0][n] = false;
      if(n == 1) {
        outputDataCol2[n] = null;
        outputNullMap[1][n] = true;
      }
      else {
        outputDataCol2[n] = &quot;Hello&quot;;
        outputNullMap[1][n] = false;
      }
      
    }
  }
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>Java Code for String Null Output</em></p>

<p>We see in <em>Code Snippet 14</em> how we output three rows with two columns, where the second column is a string. In the second row, the string column is set to <code>null</code>, and the <code>outputNullMap</code> is also set to <code>null</code> for that column. After compilation, etc. we use following SQL code:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'NullStringValues.foo';
</code></pre>

<p><strong>Code Snippet 15:</strong> <em>SQL Code for String Output</em></p>

<p>The code runs fine when we execute, and the result looks like so:</p>

<p><img src="/images/posts/sql_2k19_java_null_string_null_output.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>String Nulls Passed Out</em></p>

<p>As expected we get a null value back in the outlined row in <em>Figure 8</em>. This does not tell us though whether the <code>outputNullMap</code> is required for strings. Let us comment out all code inside the <code>for</code> loop that references the <code>outputNullMap</code>. Keep the initialization of the null map as is (<code>outputNullMap = new boolean[numberOfOutputCols][numOutRows];</code>), compile, etc. Remember that for output data sets with no nulls we only need to initialise the <code>outputNullMap</code>. The theory is that for non-primitive data types we do not need to set the null map.</p>

<p>When we now execute the code in <em>Code Snippet 15</em> we get an error:</p>

<p><img src="/images/posts/sql_2k19_java_null_string_null_output_error1.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>String Nulls without Output Null Map</em></p>

<p>Ok, so the theory above does not seem correct based on the exception we see in <em>Figure 9</em>. So what about if the column is null, but the <code>outputNullMap</code> says <code>false</code> (not null):</p>

<pre><code class="language-java">if(n == 1) {
  outputDataCol2[n] = null;
  outputNullMap[1][n] = false;
}
</code></pre>

<p><strong>Code Snippet 16:</strong> <em>String Value Null Null Map False</em></p>

<p>When we execute after having compiled the code after the change in <em>Code Snippet 16</em>, we get the same exception as we see in <em>Figure 9</em>. Also, if we assign a value to the string column, but set the <code>outputNullMap</code> to be <code>true</code>:</p>

<pre><code class="language-java">if(n == 1) {
  outputDataCol2[n] = &quot;xxx&quot;;
  outputNullMap[1][n] = true;
}
</code></pre>

<p><strong>Code Snippet 17:</strong> <em>String Value Null Null Map False</em></p>

<p>When we execute the code in <em>Code Snippet 17</em> we get:</p>

<p><img src="/images/posts/sql_2k19_java_null_string_null_output_error2.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>String not Null with Output Null Map True</em></p>

<p>Hmm, that is interesting! We see in <em>Figure 10</em> how we do receive data back, but also an exception, and notice that the exception has a different <code>HRESULT</code> than the exception in <em>Figure 9</em>.</p>

<blockquote>
<p><strong>NOTICE:</strong> I do believe that Microsoft can do a better job with the errors returned from the Java extensions, as they at the moment are not very descriptive.</p>
</blockquote>

<p>We do not receive all rows, the row where the actual string value is not null but the null map says it is, is the last row. Oh, and the value for the column is what the null map say - <code>null</code>.</p>

<p>We now know that the null maps are needed for strings as well as primitive types. It also seems like the Java extensions are doing some validation when data returns from Java.</p>

<h2 id="summary">Summary</h2>

<p>Wow, this turned out to be quite a long blog post! So what have we found out:</p>

<p>When we deal with data passing in Java code we need two null maps:</p>

<ul>
<li><code>static public boolean[][] inputNullMap</code>: for input data.</li>
<li><code>static public boolean[][] outputNullMap</code>: for output data.</li>
</ul>

<p>You initialize the <code>inputNullMap</code>, and the Java extensions components populate the map, based on the data pushed in from SQL Server. You initialise and populate the <code>outputNullMap</code>, and the Java extensions components read the map and create the resultset returned to SQL Server. Even for string null values you need to populate the <code>outputNullMap</code>.</p>

<p>In your code, you can read the <code>inputNullMap</code> to decide how to process the data pushed in.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 50 &amp; Year End!]]></title>
    <link href="http://nielsberglund.com/2018/12/16/interesting-stuff---week-50--year-end/" rel="alternate" type="text/html"/>
    <updated>2018-12-16T18:45:34+02:00</updated>
    <id>http://nielsberglund.com/2018/12/16/interesting-stuff---week-50--year-end/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<blockquote>
<p><strong>NOTE:</strong> <em>I started with these roundup posts in the beginning of 2017 as a way to force myself to write, and - so far, so good. It is now coming up on Christmas and New Year, and I will take a break with these posts and come back in the beginning of next year.</em></p>
</blockquote>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/netflix-play-api">Netflix Play API - An Evolutionary Architecture</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation which deeps dive into how Netflix used a set of three core foundational principles to iteratively develop their architecture. This led to a list of practices to create an Evolutionary Architecture.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/net-mocking-framework">Mocking .NET without Hurting Its Feelings</a>. An [InfoQ] presentation, discussing two main types of mocking frameworks: constrained frameworks (like RhinoMocks and Moq) and unconstrained frameworks (such as Typemock Isolator and Telerik JustMock).</li>
<li><a href="https://mattwarren.org/2018/12/13/Exploring-the-.NET-Core-Runtime/">Exploring the .NET Core Runtime (in which I set myself a challenge)</a>. This is another awesome blog post by <a href="https://twitter.com/matthewwarren">Matthew</a>. In this post, he looks at the inner workings of <strong>.NET Core</strong>. I have said it before, and I say it again; if you have the vaguest interest in .NET/CLR Matthews [blog] is a must!</li>
<li><a href="https://natemcmaster.com/blog/2017/12/21/netcore-primitives/">Deep-dive into .NET Core primitives: deps.json, runtimeconfig.json, and dll&rsquo;s</a>. The post by Matthew above lead me to this post by <a href="https://twitter.com/natemcmaster">Nate</a>. The post is a deep dive into the &ldquo;plumbing&rdquo; of <strong>.NET Core</strong>. A must read!</li>
<li><a href="https://www.infoq.com/articles/netcore-devops">.NET Core and DevOps</a>. An <a href="https://www.infoq.com/">InfoQ</a> article which discusses how .NET Core was designed with DevOps in mind, and how .NET Core projects can benefit from the build automation and application monitoring intrinsic to the platform. The article also shows how the command-line accessibility of .NET Core makes DevOps easier to implement.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/">Azure Functions now supported as a step in Azure Data Factory pipelines</a>. A blog post which shows how easy it is to set up Azure Functions to run as steps in an Azure Data Factory pipeline.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.infoq.com/articles/machine-learning-learn-devops">What Machine Learning Can Learn From DevOps</a>. This <a href="https://www.infoq.com/">InfoQ</a> article makes a strong case that Machine Learning can gain many benefits from DevOps: culture change to support experimentation, continuous evaluation, sharing, abstraction layers, observability, and working in products and services.</li>
<li><a href="https://blog.revolutionanalytics.com/2018/12/azurecontainers.html">How to deploy a predictive service to Kubernetes with R and the AzureContainers package</a>. This blog post from the crew at <a href="http://blog.revolutionanalytics.com">Revolution Analytics</a> shows how you can deploy an R fitted model as a Plumber web service in Kubernetes, using Azure Container Registry (ACR) and Azure Kubernetes Service (AKS).</li>
<li><a href="https://blog.revolutionanalytics.com/2018/12/ten-years-of-revolutions.html">Reflections on the 10th anniversary of the Revolutions blog</a>. Oh, and while we are on the subject <a href="http://blog.revolutionanalytics.com">Revolution Analytics</a>, a huge Happy 10:th Anniversary to you!</li>
<li><a href="https://eng.uber.com/billion-data-point-challenge">The Billion Data Point Challenge: Building a Query Engine for High Cardinality Time Series Data</a>. This blog post discusses the challenges Uber had when building their in-house metrics solution, and the query engine for that solution.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/news/2018/12/pinterest-kafka-scaling">Scaling Apache Kafka at Pinterest</a>. Apache Kafka is used at Pinterest for transporting data for real-time streaming applications, logging and visibility metrics for monitoring. The installation runs on over 2000 brokers and handles more than 800 million messages and 1.2 Petabytes per day. This <a href="https://www.infoq.com/">InfoQ</a> article talks about how  Pinterest scales Kafka.</li>
<li><a href="https://www.confluent.io/blog/deep-dive-ksql-deployment-options">Deep Dive into KSQL Deployment Options</a>. This blog post provides first a brief overview of Kafka Streams and its execution model, and then it discusses KSQL deployment options.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/12/10/announcing-sql-server-2019-community-technology-preview-2-2/">Announcing SQL Server 2019 community technology preview 2.2</a>. The title of this blog post says it all. CTP 2.2 of SQL Server 2019 is released. Go and get it!</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/12/10/availability-groups-on-kubernetes-in-sql-server-2019-preview/">Availability Groups on Kubernetes in SQL Server 2019 preview</a>. This post discusses the ability to deploy a SQL Server 2019 container with Availability Groups on Kubernetes.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>I am continuing my Java and SQL Server 2019 journey. I am very close to finishing a post about how to deal with null values in the SQL Server 2019 Java Extension. It is a continuation of these posts:</p>

<ul>
<li><a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>. In this post, I take a look at how to install and enable the SQL Server 2019 Java extensions. The post finishes with some very simple Java code which I execute from SQL Server.</li>
<li><a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">SQL Server 2019 Extensibility Framework &amp; Java - Passing Data</a>. In this post, I look at how we pass data back and forth between SQL Server and Java.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me. Seeing that holidays are coming up - have a Great Holiday Season!!</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 49]]></title>
    <link href="http://nielsberglund.com/2018/12/09/interesting-stuff---week-49/" rel="alternate" type="text/html"/>
    <updated>2018-12-09T07:18:55+02:00</updated>
    <id>http://nielsberglund.com/2018/12/09/interesting-stuff---week-49/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/s/story/lets-take-a-crack-at-understanding-distributed-consensus-dad23d0dc95">How Does Distributed Consensus Work?</a>. This is an excellent blog post discussing distributed computing and distributed consensus. I learned a lot for the post!</li>
<li><a href="https://cloudblogs.microsoft.com/opensource/2018/12/04/announcing-service-fabric-provider-bosh">Announcing the Service Fabric provider for Bosh</a>. This is an announcement from Microsoft how Service Fabric can now me deployed an managed by <a href="https://bosh.io/docs/">Bosh</a>. BOSH is a project that unifies release engineering, deployment, and lifecycle management of small and large-scale cloud software.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/12/04/announcing-net-core-3-preview-1-and-open-sourcing-windows-desktop-frameworks/">Announcing .NET Core 3 Preview 1 and Open Sourcing Windows Desktop Frameworks</a>. A blog post from Microsoft, announcing a preview of .NET Core 3. In .NET Core 3 Microsoft includes Windows Forms, WPF and WinUI, and they open-source them! Who&rsquo;d have thunk!</li>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/12/04/announcing-ml-net-0-8-machine-learning-for-net/">Announcing ML.NET 0.8 – Machine Learning for .NET</a>. Microsoft has just released version .8 of Machine Learning for .NET (ML.NET), and this blog post provides details about some of the new/improved functionality.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://data-artisans.com/blog/5-steps-flink-application-development">5 “baby” steps to develop a Flink application</a>. A short an sweet introduction how to get up and running with Apache Flink.</li>
<li><a href="https://www.confluent.io/blog/kafka-streams-ksql-minimum-privileges">Kafka Streams and KSQL with Minimum Privileges</a>. You should read this blog post if you are working with Kafka. It discusses security, and how to apply security with minimum privileges.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="http://sql-sasquatch.blogspot.com/2018/12/fun-with-sql-server-query-store-query.html">Fun with SQL Server Query Store, Query Plan &lsquo;StatisticsInfo&rsquo; XML nodes, and STATS_STREAM</a>. A very cool blog post by <a href="https://twitter.com/sqL_handLe">Lonny</a> where he has fun with SQL Server query related &ldquo;stuff&rdquo;.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/11/06/the-november-release-of-azure-data-studio-is-now-available/">The November release of Azure Data Studio is now available</a>. The title says it all. The blog post discusses new/improved functionality in the November release of Azure Data Studio. I have found myself quite like ADS, and are doing most of my &ldquo;stuff&rdquo; in ADS instead of in SSMS; they said you can&rsquo;t learn old dogs new tricks - hah!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/2018/12/03/learning-areas-for-sql-server-big-data-clusters/">Learning Areas for SQL Server Big Data Clusters</a>. A teaser from <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a>, where he talks about upcoming training and workshops related to <strong>SQL Server 2019 Big Data Clusters</strong>. He also lists some resources to start to get to understand all the new technologies in <strong>SQL Server 2019 Big Data Clusters</strong>.</li>
<li><a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">SQL Server 2019 Extensibility Framework &amp; Java - Passing Data</a>. I continue my journey with SQL Server 2019 Java extensions. In this post, I look at how we pass data back and forth between SQL Server and Java.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

