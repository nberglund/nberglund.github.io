<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="/atom.xml" rel="self"/>
  <link href="/"/>
  <updated>2018-08-18T05:03:30+02:00</updated>
  <id>/</id>
  <generator>Hugo -- gohugo.io</generator>
  
  <entry>
    <title type="html"><![CDATA[Goodbye Jekyll, Welcome Hugo!]]></title>
    <link href="/2018/08/goodbye-jekyll-welcome-hugo/"/>
    <id>/2018/08/goodbye-jekyll-welcome-hugo/</id>
    <author>
      <name>nielsb</name>
    </author>
    <published>2018-08-18T05:03:30+02:00</published>
    <updated>2018-08-18T05:03:30+02:00</updated>
    <content type="html"><![CDATA[<p>Back in 2013 I wrote a <a href="/2013/10/moving-to-a-new-blog-engine/">post</a> about how I moved from a self hosted <strong>WordPress</strong> blog to a static blog; <a href="http://octopress.org"><strong>OctoPress</strong></a>. OctoPress spoke to my &ldquo;geeky&rdquo; side, as the foundation of it is the static blog-generator <a href="https://jekyllrb.com/"><strong>Jekyll</strong></a>, which in turn is Ruby based. In fact, OctoPress is more or less just some extra Ruby plugins on top of Jekyll, and to generate sites with OctoPress / Ruby you need Ruby installed on you machine.</p>

<p>I thought that by changing the blog platform, I might write more posts just due to the &ldquo;geekiness&rdquo; of the blog engine. Fast forward to the end of 2016, and the number of posts I had written since the switch came to a grand total of three. Those three includes the post where I announced the switch. Yeah, switching increased indeed my productivity - NOT!</p>

<p>So what does this have to do with anything?</p>

<p></p>

<p>Before I get into the reason for this post, let us look at what a static site generator is.</p>

<h2 id="static-site">Static Site</h2>

<p>A static site generator is a framework that takes source files and generates an entirely static website. We deploy the files that make up the site to the web server, and when a user requests a page, the web server returns that page to the user. This opposed to something like WordPress, where WordPress builds the page from a number of templates, gets the content and other site data from the database and sends the complete HTML page back to the user.</p>

<p>The advantages of a static site are that it is usually less complicated than a dynamic site; no templates, no database and so forth. Quite often serving a page to the user is better performing as the page is not dynamically created.</p>

<p>The downside of a static site is that you build it for each time you do a change to a page, adding a post, and so on, and depending on the size of your site (number of pages etc.), the build can take a while. So that is where my problem lies and the reason for this post.</p>

<h2 id="problem">Problem</h2>

<p>I mentioned above how I didn&rsquo;t manage to write any blog posts up to the end of 2016. Since then, however, I have been reasonably productive in my writing - and managed at least one post per week. It helps when you have cool stuff like <strong>SQL Server Machine Learning Services</strong> to write about. While it is cool that I produce posts, what I noticed was that the build time of the site took longer and longer. I did not think much about it until a couple of weeks ago when I had just finished the <a href="/2018/08/sp_execute_external_script-and-sql-compute-context---iii/">sp_execute_external_script and SQL Compute Context - III</a> post and tried to generate the site. It did not work; it just hung, what to do?</p>

<p>I ended up removing all OctoPress plugins and edited all files that referenced the plugins, to be able to run a bare-bones Jekyll generated site. I eventually managed to get it to work again, but this made me look around for other site generators.</p>

<h2 id="hugo">Hugo</h2>

<p>In my looking around for static site generators I came across <a href="https://gohugo.io/">Hugo</a>. Like Jekyll it is an open source static site generator, but it is built on <a href="https://golang.org/"><strong>Go</strong></a> instead of Ruby. One of the differences between Hugo and Jekyll is that when you generate a site with Jekyll you execute Ruby commands, and, as I mentioned above, you need Ruby installed. Hugo, on the other hand, comes as an executable <code>Hugo.exe</code> and you do not need Go installed at all. That Hugo is a self contained executable is a big plus in my book, since I have had versioning issues with Ruby a couple of times.</p>

<p>The most significant difference between Jekyll and Hugo though is speed when generating a site, or at least that is what the Hugo website says: &ldquo;<em>The world’s fastest framework for building websites</em>&rdquo;.</p>

<p>With the above points in mind, I decided to try and convert this blog from Jekyll to Hugo.</p>

<h2 id="blog-conversion">Blog Conversion</h2>

<p>So, I spent a couple of hours a day for around a week converting my posts and pages to Hugo, and it was not that difficult. The biggest issue was how Jekyll refers to posts: <code>{% post_url 2018-08-04-sp-execute-external-script-and-sql-compute-context---iii %}</code> versus <code>/2018/08/sp_execute_external_script-and-sql-compute-context---iii/</code>, and I had a lot of those. I eventually wrote a small <code>C#</code> console application that trawled through the posts and did the conversion. Apart from that everything is pretty straightforward. Obviously there are differences, but nothing earthshattering, as far as I can tell.</p>

<p>What about the speed then? Well, to build the Jekyll site took around 20 seconds, to build the site using Hugo takes around 2 seconds! Based on this, I decided to switch to Hugo, and this is the first post that I publish using Hugo as blog site generator. I hope I have not missed anything and that all is Ok. If not, well then, hopefully, you, my readers, <a href="mailto:niels.it.berglund@gmail.com">tell me</a> if you notice something amiss.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 32]]></title>
    <link href="/2018/08/interesting-stuff---week-32/"/>
    <id>/2018/08/interesting-stuff---week-32/</id>
    <author>
      <name>nielsb</name>
    </author>
    <published>2018-08-12T12:02:02+02:00</published>
    <updated>2018-08-12T12:02:02+02:00</updated>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/the-many-faces-of-consistency.html">The many faces of consistency</a>. A blog post by <a href="https://twitter.com/muratdemirbas">Murat</a> where he dissects a white paper about consistency. The paper talks about two types of consistency: <em>state</em> and <em>operation</em>. Seeing that Murat now does sabbatical work at Microsoft (see below), he compares the two consistency types with what Cosmos DB provides. The post is a must read if you are the least interested in distributed computing and consistency.</li>
</ul>

<h2 id="cloud-big-data">Cloud / Big Data</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/globally-replicated-data-lakes-with-livedata-using-wandisco-on-azure/">Globally replicated data lakes with LiveData using WANdisco on Azure</a>. The post discusses how you can achieve globally replicated Azure Data Lakes. The replication can be both hybrid: on-prem to Azure as well as Azure to Azure.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/08/09/cloud-data-and-ai-services-training-roundup-august-2018/">Cloud data and AI services training roundup August 2018</a>. This post lists some free data and AI training sessions.</li>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/azure-cosmos-db.html">Azure Cosmos DB</a>. This post by <a href="https://twitter.com/muratdemirbas">Murat</a> is about his first impressions of Azure Cosmos DB. Murat has taken a sabbatical and spends a year at Microsoft in the Cosmos DB team. I look forward to more posts by Murat about Cosmos DB, and other Azure related topics.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://charlla.com/howto-docker-on-windows/">HowTo - Docker on Windows</a>. My mate and colleague, <a href="https://twitter.com/charllamprecht">Charl</a> continues his blogging journey. This post is how to run Docker on Windows.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/streaming-data-dominates-over-2000-developers-say-only-batch-is-almost-extinct">Streaming Data Dominates: Over 2000 Developers Say “Only Batch” Is Almost Extinct</a>. A survey by <a href="https://www.lightbend.com/">Lightbend</a>, (formerly known as <a href="https://en.wikipedia.org/wiki/Lightbend">Typesafe</a>), makes it clear that developers now moves more and more towards real-time processing as opposed to batch. That, my friends, is music to my ears!</li>
<li><a href="https://data-artisans.com/blog/apache-flink-1-6-0-whats-new-in-the-latest-apache-flink-release">Apache Flink 1.6.0: What’s new in the latest Apache Flink release</a>. What the title says; this is a post detailing some of the new features in the Flink 1.6.0 release.</li>
<li><a href="https://www.confluent.io/blog/getting-started-apache-kafka-kubernetes/">Getting Started with Apache Kafka and Kubernetes</a>. A blog-post about the work done to enable Kafka to run on Kubernetes. The post points to a white paper: <a href="https://www.confluent.io/resources/recommendations-for-deploying-apache-kafka-on-kubernetes">Run Confluent Platform on Kubernetes Using Best Practices</a> which is really good!</li>
<li><a href="https://www.confluent.io/blog/kafka-streams-action">Kafka Streams in Action</a>. A post about the upcoming book: <a href="https://www.manning.com/books/kafka-streams-in-action">Kafka Streams in Action</a>. Apart from announcing the book, the post also contains the foreword to the book. This book is a must if you are interested in Kafka Streams!</li>
<li><a href="https://databricks.com/blog/2018/08/09/building-a-real-time-attribution-pipeline-with-databricks-delta.html">Building a Real-Time Attribution Pipeline with Databricks Delta</a>. this blog post looks at how to use the Databricks DataFrame API to build Structured Streaming applications and use Databricks Delta to query the streams in near-real-time.</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://dzone.com/articles/model-serving-stream-processing-vs-rpc-rest-with-j">Model Serving: Stream Processing vs. RPC/REST With Java, gRPC, Apache Kafka, TensorFlow</a>. A short and sweet blog-post comparing stream processing applications with a model serving infrastructure, like <strong>TensorFlow Serving</strong>, for serving machine learning models.</li>
<li><a href="http://blog.revolutionanalytics.com/2018/08/ieee-language-rankings-2018.html">IEEE Language Rankings 2018</a>. A post by <a href="https://twitter.com/revodavid">David</a> about the latest IEEE Spectrum language rankings.</li>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/scalable-iot-ml-platform-with-apache-kafka-deep-learning-mqtt">Scalable IoT ML Platform with Apache Kafka + Deep Learning + MQTT</a>. This post describes a hybrid machine learning infrastructure leveraging Apache Kafka as a scalable central nervous system. Very interesting!</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I have started on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it in a week or so.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 31]]></title>
    <link href="/2018/08/interesting-stuff---week-31/"/>
    <id>/2018/08/interesting-stuff---week-31/</id>
    <author>
      <name>nielsb</name>
    </author>
    <published>2018-08-05T08:01:44+02:00</published>
    <updated>2018-08-05T08:01:44+02:00</updated>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/08/02/tiered-compilation-preview-in-net-core-2-1/">Tiered Compilation Preview in .NET Core 2.1</a>. A blog-post about a new feature in .NET Core 2.1: <strong>Tiered Compilation</strong>. Tiered Compilation allows .NET to have multiple compilations for the same method that can be hot-swapped at runtime. This should improve compile times drastically!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://charlla.com/whatsthat-kafka/">Apache Kafka - Whats That</a>. This post about Kafka is by a colleague and a good friend of mine, <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>. In the post, he takes us through a very succinct overview of Kafka. Charl is &ldquo;Mr Kafka&rdquo; at <a href="/Derivco">Derivco</a>, and he knows his &ldquo;stuff&rdquo;. Please be sure to follow his <a href="https://charlla.com/">blog</a> for more about Kafka (Charl, no pressure, hey?!).</li>
<li><a href="https://www.confluent.io/blog/decoupling-systems-with-apache-kafka-schema-registry-and-avro/">Decoupling Systems with Apache Kafka, Schema Registry and Avro</a>. An excellent post on how to decouple the systems you integrate via Kafka by using the <a href="https://www.confluent.io/confluent-schema-registry/">Confluent Schema Registry</a>. An added bonus in this post is that the code is .NET code!</li>
<li><a href="https://data-artisans.com/blog/a-practical-guide-to-broadcast-state-in-apache-flink">A Practical Guide to Broadcast State in Apache Flink</a>. This article discusses <strong>Broadcast State</strong>, a new feature in Apache Flink 1.5. With Broadcast State you can evaluate dynamic patterns on event streams by combining and jointly process two streams of events in a specific way.</li>
<li><a href="https://www.confluent.io/blog/introducing-confluent-platform-5-0/">Introducing Confluent Platform 5.0</a>. As the title says, this post introduces the latest version of Confluent Platform: 5.0. Lots and lots of new interesting features. Go and have a look!</li>
<li><a href="https://www.confluent.io/landing-page/microservices-online-talk-series/">Apache Kafka for Microservices: A Confluent Online Talk Series</a>. This post is a link to a three-part online talk series which introduces fundamental concepts, use cases and best practices for getting started with microservices and Kafka.</li>
</ul>

<h2 id="big-data-cloud">Big Data / Cloud</h2>

<ul>
<li><a href="https://databricks.com/blog/2018/07/31/processing-petabytes-of-data-in-seconds-with-databricks-delta.html">Processing Petabytes of Data in Seconds with Databricks Delta</a>. In my roundups lately, I have covered Databricks Delta quite a bit and discussed how efficient it is processing lots and lots of data. This blog post takes a look under the hood and examines what makes Databricks Delta capable of sifting through petabytes of data within seconds. If you, like me, are interested in knowing how &ldquo;stuff&rdquo; works under the covers, then this post is a must-read!</li>
<li><a href="https://eng.uber.com/databook/">Databook: Turning Big Data into Knowledge with Metadata at Uber</a>. This post is about <strong>Databook</strong>, Uber’s in-house platform that surfaces and manages metadata about the internal locations and owners of specific datasets, and allows Uber to turn data into knowledge.</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="http://blog.revolutionanalytics.com/2018/08/r-python-in-sql-server.html">Video: How to run R and Python in SQL Server from a Jupyter notebook</a>. A short post by <a href="https://twitter.com/revodavid">David</a> linking to a video showing how to run Python and R from inside SQL Server.</li>
<li><a href="https://www.infoq.com/minibooks/emag-real-world-machine-learning">The InfoQ eMag: Real-World Machine Learning: Case Studies, Techniques and Risks</a>. An <a href="https://www.infoq.com/">InfoQ</a> link to an eMag focusing on the current landscape of machine-learning technologies and real-world case studies of applied machine learning.</li>
<li><a href="https://blogs.technet.microsoft.com/machinelearning/2018/07/31/3-steps-to-build-your-first-intelligent-app-conference-buddy/">3 Steps to Build Your First Intelligent App – Conference Buddy</a>. A blog-post which takes us through how to build an application utilising AI.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<ul>
<li><a href="/2018/08/sp_execute_external_script-and-sql-compute-context---iii/">sp_execute_external_script and SQL Compute Context - III</a>. I finally managed to finish and publish the third post in the <a href="/series/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series. In this post we use WinDbg, Process Monitor and WireShark to look in detail what happens in SQL Server when we use RxSqlServerData to pull data.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[sp_execute_external_script and SQL Compute Context - III]]></title>
    <link href="/2018/08/sp_execute_external_script-and-sql-compute-context---iii/"/>
    <id>/2018/08/sp_execute_external_script-and-sql-compute-context---iii/</id>
    <author>
      <name>nielsb</name>
    </author>
    <published>2018-08-04T16:05:46+02:00</published>
    <updated>2018-08-04T16:05:46+02:00</updated>
    <content type="html"><![CDATA[<p>In the <a href="/2018/03/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services - sp_execute_external_script - III</a> post I wrote about <code>sp_execute_external_script</code> (SPEES) and the <strong>SQL Server Compute Context</strong> (SQLCC). Afterwards I realised I had some things wrong, so I wrote a followup post: <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">sp_execute_external_script and SQL Compute Context - I</a> where I tried to correct my mistakes from the <a href="/2018/03/microsoft-sql-server-r-services---sp_execute_external_script---iii/">initial post</a>. That post led to <a href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a> and now we have a mini-series.</p>

<p>To see other posts (including this) in the series, go to <a href="/series/spees_and_sql_compute_context"><strong>sp_execute_external_script and SQL Server Compute Context</strong></a>.</p>

<p>In the previous post in this series, we looked at how data is sent to the SqlSatellite from SQL Server when we are in the SQLCC. This post was meant to look at what goes on inside SQL Server when we execute in SQLCC, but I realised that it would make more sense if, before we look at the internal working when in SQLCC, I covered what happens when pulling data in the local context. So that is what this post is all about.</p>

<p></p>

<p>Before we dive into todays topic let us recap.</p>

<h2 id="recap">Recap</h2>

<p>In <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> we discussed what the SQLCC is and we said that as part of RevoScaleR, you can define where a workload executes. By default, it executes on your local machine, but you can also set it to execute in the context of somewhere else: Hadoop, Spark and also SQL Server. So, in essence, you can run some code on your development machine and have it execute in the environments mentioned above. To use the SQLCC in SQL Server we use <code>RxInSqlServer</code> and <code>rxSetComputeContext</code>:</p>

<pre><code class="language-r"># set up the connection string
sqlServerConnString &lt;- &quot;Driver=SQL Server;
                        server=.; # localhost
                        database=testParallel;
                        uid=some_uid;pwd=some_pwd&quot;

# set up the context
sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                        numTasks = 1)
# set the compute context to be the sql context
rxSetComputeContext(sqlCtx)    
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Set up SQL Server Compute Context</em></p>

<p>The connection string we see in <em>Code Snippet 1</em> indicates the where we execute, not necessarily where the data we work with lives. The <code>numTasks</code> argument defines the maximum number of tasks SQL Server can use. Something interesting when setting <code>numTasks</code> to be greater than 1 is that when we run the code, we run it hosted in a <code>mpiexec.exe</code> process. If we run in SQLCC under <code>numTasks = 1</code> we do not see <code>mpiexec.exe</code>, but we see another <code>bxlserver.exe</code> process. We also said that when we run in SQLCC. <code>sp_execute_external_script</code> executes multiple times.</p>

<p>The most interesting thing that came out of <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> was the performance benefit of using SQLCC when loading large datasets. So in <a href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a>, we tried to see where that performance benefit came from.</p>

<p>In <a href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> we said we had three ways of getting data into an R script:</p>

<ul>
<li>Push using <code>@input_data_1</code>.</li>
<li>Pull using <code>RxSqlServerData</code> in the local context.</li>
<li>Pull using <code>RxSqlServerData</code> and the SQLCC.</li>
</ul>

<p>We saw that to use SQLCC we need to pull the data; we could not push it. We also saw that for large datasets there was a significant performance difference between pulling in the local context and pulling using SQLCC. The interesting point was that pushing data and pulling using SQLCC had the same performance characteristics.</p>

<p>When pulling the data (<code>RxSqlServerData</code>), we use ODBC, and the protocol is TDS. However, when we use the SQLCC, the BXL protocol is also used and that gives us very efficient processing of data which is the reason we see good performance.</p>

<h2 id="housekeeping">Housekeeping</h2>

<p>Before we go any further let us look at the code and the tools we use today. This section is here for those who want to follow along in what we are doing in the post.</p>

<h4 id="helper-tools">Helper Tools</h4>

<p>To help us figure out the things we want, we use:</p>

<ul>
<li><em>Process Monitor</em> - to filter out TCP traffic.</li>
<li><em>WinDbg</em> - to see what happens inside SQL Server. If you need help with setting it up, we covered that in <a href="/2017/03/microsoft-sql-server-r-services---internals-i/">Microsoft SQL Server R Services - Internals I</a>.</li>
<li><em>WireShark</em> - to &ldquo;sniff&rdquo; network packets. We covered setting up <em>WireShark</em> in <a href="/2017/08/microsoft-sql-server-r-services---internals-x/">Internals - X</a>. Please remember that if you run SSMS and SQL Server on the same machine, then you need the <a href="https://nmap.org/npcap/"><strong>Npcap</strong></a> packet sniffer library instead of the default <strong>WinPcap</strong>.</li>
</ul>

<h4 id="code">Code</h4>

<p>This is the database objects we use in this post:</p>

<pre><code class="language-sql">USE master;
GO

SET NOCOUNT ON;
GO

DROP DATABASE IF EXISTS TestParallel;
GO

CREATE DATABASE TestParallel;
GO

USE TestParallel;
GO

DROP TABLE IF EXISTS dbo.tb_Rand_50M
GO
CREATE TABLE dbo.tb_Rand_50M
(
  RowID bigint identity PRIMARY KEY, 
  y int NOT NULL, rand1 int NOT NULL, 
  rand2 int NOT NULL, rand3 int NOT NULL, 
  rand4 int NOT NULL, rand5 int NOT NULL,
);
GO

INSERT INTO dbo.tb_Rand_50M(y, rand1, rand2, rand3, rand4, rand5)
SELECT TOP(50000000) CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 25 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 50 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 100 AS INT)
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO

</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Setup of Database, Table and Data</em></p>

<p>We use more or less the same database and database object as in the <a href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> post:</p>

<ul>
<li>A database: <code>TestParallel</code>.</li>
<li>A table: <code>dbo.tb_Rand_50M</code>. This table contains the data we want to analyse.</li>
</ul>

<p>In addition to creating the database and the table <em>Code Snippet 2</em> also loads 50 million records into the <code>dbo.tb_Rand_50M</code>. Be aware that when you run the code in <em>Code Snippet 2</em> it may take some time to finish due to the loading of the data. Yes, I know - the data is entirely useless, but it is a lot of it, and it helps to illustrate what we want to do.</p>

<p>Not only is the database and database objects similar to what we used in <a href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a>, the code we use is also almost the same:</p>

<pre><code class="language-sql">DECLARE @isCtx bit = 0;
DECLARE @numTasks int = 1;
EXEC sp_execute_external_script
      @language = N'R'
    , @script = N'
      # set up the connection string
      sqlServerConnString &lt;- &quot;Driver=SQL Server;server=.;
                              database=testParallel;
                              uid=&lt;username&gt;;pwd=&lt;userpwd&gt;&quot;
      
      if(useContext == 1) {
        sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                                numTasks = tasks)
        # set the compute context to be the sql context
        rxSetComputeContext(sqlCtx)
      }

      mydata &lt;- RxSqlServerData(sqlQuery = &quot;SELECT y, rand1, rand2, 
                                            rand3, rand4, rand5 
                                            FROM dbo.tb_Rand_50M&quot;,
                                connectionString = sqlServerConnString);
                        
      myModel &lt;- rxLinMod(y ~ rand1 + rand2 + rand3 + rand4 + rand5, 
                      data=mydata)'
    , @params = N'@tasks int, @useContext bit'
    , @tasks = @numTasks
    , @useContext = @isCtx
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Test Code</em></p>

<p>As we see in <em>Code Snippet 3</em> we parameterise the <code>sp_execute_external_script</code> call, and we have parameters for whether to use the SQLCC and also how many tasks to run when executing in the context. The default is to execute in the local context, and when we execute in SQLCC the default for the number of tasks is 1 (<code>numTasks = 1</code>).</p>

<h2 id="local-context-pull-vs-push">Local Context Pull vs Push</h2>

<p>Let us try and see what happens in SQL Server when we execute in the local context, and we pull data (<code>RxSqlServerData</code>) compared to when we push data (<code>@input_data_1</code>).</p>

<p>First, let us try and understand when and how the <code>SELECT</code> statement in <code>Code Snippet 3</code> executes. To check this we use the same technique as we did in <a href="/2017/11/microsoft-sql-server-r-services---internals-xiii/">Microsoft SQL Server R Services - Internals XIII</a>, we execute a <code>SELECT</code> statement which causes a division by zero exception. We capture that exception in <em>WinDbg</em> and we check the call stack.</p>

<p>So, run <em>WinDbg</em> as admin and enable an event-filter, so the debugger breaks at a <code>C++ EH</code> exception. In the <em>Event Filters</em> dialog you can set how that particular exception should be handled. We want it to be enabled, but not handled:</p>

<p><img src="/images/posts/sql_r_services_windbg_eventfilters_enable.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Enable C++ EH Exception</em></p>

<p>When the <em>Event Filter</em> is enabled as in <em>Figure 1</em> we:</p>

<ul>
<li>Attach <em>WinDbg</em> to the <code>sqlservr.exe</code> process (please do not do this on a production machine - <em>#justsaying</em>).</li>
<li>Set a breakpoint at <code>bp sqllang!SpExecuteExternalScript</code>.</li>
<li>Change the <code>SELECT</code> statement in <em>Code Snippet 3</em> slightly, so it generates the division by zero exception mentioned above: <code>SELECT TOP(1) (1 / y - y), y, rand1, ...</code>.</li>
</ul>

<p>After changing the <code>SELECT</code> statement, we execute the code. Continue through the breakpoint at <code>SpExecuteExternalScript</code>, and when the execution breaks at the exception we check the call stack: <code>k</code>:</p>

<pre><code class="language-cpp">KERNELBASE!RaiseException+0x68
...
sqllang!CXStmtQuery::ErsqExecuteQuery+0x5a2
sqllang!CXStmtSelect::XretExecute+0x2f2
sqllang!CMsqlExecContext::ExecuteStmts&lt;1,1&gt;+0x4c5
sqllang!CMsqlExecContext::FExecute+0xaae
sqllang!CSQLSource::Execute+0xa2c
sqllang!process_request+0xe52
sqllang!process_commands_internal+0x289
sqllang!process_messages+0x213
sqldk!SOS_Task::Param::Execute+0x231
...
ntdll!RtlUserThreadStart+0x21
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Call Stack Local Context Pull</em></p>

<p>The call stack we see in <em>Code Snippet 4</em> is somewhat edited in that it shows only the important parts. Let us compare it with the call-stack we see when we push the data, (<code>@input_data_1</code>), and receive a division by zero exception as we did in <a href="/2017/11/microsoft-sql-server-r-services---internals-xiii/">Microsoft SQL Server R Services - Internals XIII</a>:</p>

<pre><code class="language-cpp">KERNELBASE!RaiseException+0x68
...
sqllang!CXStmtQuery::ErsqExecuteQuery+0x49f
sqllang!CXStmtSelect::XretExecute+0x2f2
sqllang!CMsqlExecContext::ExecuteStmts&lt;1,1&gt;+0x4c5
sqllang!CMsqlExecContext::FExecute+0xaae
sqllang!CSQLSource::Execute+0xa2c
sqllang!SpExecuteExternalScript+0x154b
sqllang!CSpecProc::ExecuteSpecial+0x31e
sqllang!CXProc::Execute+0x139
sqllang!CSQLSource::Execute+0xb5b
sqllang!CStmtExecProc::XretLocalExec+0x2d3
sqllang!CStmtExecProc::XretExecExecute+0x4a1
sqllang!CXStmtExecProc::XretExecute+0x38
sqllang!CMsqlExecContext::ExecuteStmts&lt;1,1&gt;+0x4c5
sqllang!CMsqlExecContext::FExecute+0xaae
sqllang!CSQLSource::Execute+0xa2c
sqllang!process_request+0xe52
sqllang!process_commands_internal+0x289
sqllang!process_messages+0x213
sqldk!SOS_Task::Param::Execute+0x231
...
ntdll!RtlUserThreadStart+0x21
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Call Stack Local Context Push</em></p>

<p>The call-stack in <em>Code Snippet 5</em> is also edited and when we compare the two call-stacks, they look somewhat similar.  We see <code>sqllang!CXStmtQuery::ErsqExecuteQuery</code> in both code snippets, and we know from <a href="/2017/11/microsoft-sql-server-r-services---internals-xiii/">Internals - XIII</a> that <code>ErsqExecuteQuery</code> handles execution of SQL statements and also sending data to the SQL Satellite. So that makes sense that we see it in <em>Code Snippet 4</em>. However, what does not make sense is that we do not see <code>SpExecuteExternalScript</code> in the <em>Code Snippet 4</em> call stack (and no, I did not edit it out). What goes on here?</p>

<p>Ok, let us refer back to what we saw in <a href="/2017/10/microsoft-sql-server-r-services---internals-xii/">Internals - XII</a> and  <a href="/2017/11/microsoft-sql-server-r-services---internals-xiii/">Internals - XIII</a>, how SQL Server sends various data packages to the SqlSatellite and then finally calls <code>sqllang!CSatelliteCargoContext::SendChunkEndMessage</code>. So, add a breakpoint, in addition to the <code>SpExecuteExternalScript</code> breakpoint, at <code>sqllang!CSatelliteCargoContext::SendChunkEndMessage</code> and execute the code again. Oh, we see how we hit the <code>SpExecuteExternalScript</code> breakpoint first, followed by <code>SendChunkEndMessage</code>, and finally, we break at the division by zero exception. That is interesting, SQL Server sends the chunk end message and only after that, the query executes.</p>

<p>So what with <code>ErsqExecuteQuery</code>, what does that do? Let us see if we can find out:</p>

<ul>
<li>Change the code not to cause a divide by zero exception: <code>SELECT TOP(1) y, rand1, ...</code>.</li>
<li>Set a breakpoint at <code>ErsqExecuteQuery</code>.</li>
</ul>

<p>When you execute the code you see something like so:</p>

<pre><code class="language-cpp">0:099&gt; g
Breakpoint 0 hit
sqllang!SpExecuteExternalScript:
0:091&gt; g
Breakpoint 2 hit
sqllang!CXStmtQuery::ErsqExecuteQuery:
0:091&gt; g
Breakpoint 1 hit
sqllang!CSatelliteCargoContext::SendChunkEndMessage:
0:091&gt; g
Breakpoint 2 hit
sqllang!CXStmtQuery::ErsqExecuteQuery:
0:078&gt; g
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Breakpoints Hit</em></p>

<p>In <em>Code Snippet 6</em> we see how we first hit the <code>SpExecuteExternalScript</code> breakpoint, followed by <code>ErsqExecuteQuery</code>, (which is expected), then we hit the <code>SendChunkEndMessage</code> followed by a second <code>ErsqExecuteQuery</code>. That is interesting; two <code>ErsqExecuteQuery</code>, and we know from above that the query executes after the second <code>ErsqExecuteQuery</code>. So the question is now how the data flows from SQL Server to the SqlSatellite. We know from [Internals - XIII<a href="/2017/11/microsoft-sql-server-r-services---internals-xiii/">si13</a>, as we mentioned above, that when we push data, SQL Server makes these calls (among many others):</p>

<ul>
<li><code>sqlmin!CQScanUdx::PushNextChildRow</code>.</li>
<li><code>sqllang!CUDXR_ExternalScript::PushRow</code>.</li>
<li><code>sqllang!CSatelliteCargoContext::SendPackets</code>.</li>
</ul>

<p>Let us set a breakpoint at <code>sqllang!CSatelliteCargoContext::SendPackets</code> and see what happens when we execute.</p>

<p>THAT was interesting! We never hit <code>SendPackets</code>. In the recap above we said that when we pull the data we use ODBC and the TDS protocol, and what we see here further ensures that is the case. So what does SQL Server call when it uses TDS to send data? To find out:</p>

<ul>
<li>Disable the breakpoints at <code>SendChunkEndMessage</code> and <code>SendPackets</code>.</li>
<li>Execute the query.</li>
</ul>

<p>When you reach <code>ErsqExecuteQuery</code> the second time, do a <code>wt  -l4</code> (watch and trace 4 levels deep) and continue. Look at the output from the <code>wt</code>:</p>

<pre><code class="language-cpp">sqllang!CXStmtQuery::ErsqExecuteQuery
  ...   
    sqllang!CXStmtQuery::SetupQueryScanAndExpression
      sqlmin!CQuery::CreateExecPlan
        sqldk!CMemProc::CpagesInUse
      ...
      sqlmin!CQueryScan::StartupQuery
  ...      
  sqlmin!CQueryScan::GetRow
    sqlmin!CQScanTopNew::GetRow
      sqlmin!CQScanTableScanNew::GetRow
        sqlmin!CQScanRowsetNew::GetRowWithPrefetch
      sqlmin!CQScanTableScanNew::GetRow
        sqllang!CValOdsRow::SetDataX
        sqllang!CShilohTds::SendRowImpl
      sqlmin!CQScanTableScanNew::GetRow
    sqlmin!CQScanTopNew::GetRow
  sqlmin!CQueryScan::GetRow
...
sqllang!CXStmtQuery::ErsqExecuteQuery
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Output from wt</em></p>

<p>The output shows a lot of data, so in <em>Code Snippet 7</em> I have edited out all but the interesting parts. We see some routines that have to do with setting up and starting the query and retrieving result data:</p>

<ul>
<li><code>sqllang!CXStmtQuery::SetupQueryScanAndExpression</code>.</li>
<li><code>sqlmin!CQueryScan::StartupQuery</code>.</li>
<li><code>sqlmin!CQueryScan::GetRow</code>.</li>
</ul>

<p>That is interesting but what is more interesting is what we see after the <code>GetRow</code> calls: <code>sqllang!CShilohTds::SendRowImpl</code>. SQL Server calls <code>SendRowImpl</code> when it pushes data to the network packet which it then sends to the caller. We can confirm this is the case by using <em>WinDbg</em> and <em>Process Monitor</em>. So, run <em>Process Monitor</em> as admin and set an event filter which captures TCP Receive events for the <code>BxlServer.exe</code> process.</p>

<blockquote>
<p><strong>NOTE:</strong> If you are not sure how to set an event filter in <em>Process Monitor</em>, you can read about it in the <a href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a> post.</p>
</blockquote>

<p>Now, disable all breakpoints in <em>WinDbg</em> and execute the code in <em>Code Snippet 3</em>, (with <code>SELECT TOP(1) ...</code>) and watch the output from <em>Process Monitor</em>:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_procmon_pull_1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Process Monitor Output</em></p>

<p>What we see in <em>Figure 2</em> is similar to what we saw in <a href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a>, when we looked at the TCP packets SQL Server sends to the <code>BxlServer.exe</code> process. The outlined row in <em>Figure 2</em> is the packet with the result of the <code>SELECT</code>. I know that because I tried with multiple <code>TOP</code> values and saw which row changed length between the <code>TOP</code> values. In <a href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> the corresponding row had a length of 1358 as we did a <code>TOP(50)</code>. You may wonder what the three highlighted parts if the figure is and we come back to that in a little bit.</p>

<p>Now let us confirm that the outlined row is the row with the data:</p>

<ul>
<li>Re-enable (set) the breakpoint at <code>sqllang!SpExecuteExternalScript</code>.</li>
<li>Set a breakpoint at <code>sqllang!CShilohTds::SendRowImpl</code>.</li>
</ul>

<p>Execute the code and continue through until you break at <code>SendRowImpl</code>. If you look at the <em>Process Monitor</em> output, the last row you see is a row with a length of 405. That is the fourth row from the end as we see in <em>Figure 2</em>. Now continue the execution, and you see how <em>Process Monitor</em> outputs the three last rows, with a row with a length of 133 as the first one. We have &ldquo;thus&rdquo; proven that <code>SendRowImpl</code> sends the data row to the data packet which then SQL Server sends to the caller. Oh, the reason I use <code>TOP(1)</code> is that there is one call to <code>SendRowImpl</code> for each row, and I do not want to have to press F5 millions of times, or even 50 times.</p>

<h2 id="packets">Packets</h2>

<p>Now, what about the highlighted areas in <em>Figure 2</em>, what are those? First of all, if you look at the length of the packets it seems like each area have packets of the same length: in the yellow area, we see packets with a length of 37, 1245, 67 and 405, which we also see in the green and blue area. In <em>Figure 2</em> we also see that the highlighted areas originate from the same <code>BxlServer.exe</code> process (process id 6572), but different ports: 6799, 6800 and 6801. OK, let us see if we can figure out what this is all about, and to try to get a clearer picture we also want to filter on what <code>BxlServer.exe</code> sends to SQL Server. To see that, we add two new conditions to the existing <em>Process Monitor</em> filter:</p>

<ul>
<li>First condition - <em>Operation</em> (first drop-down) <em>is</em> (second dropdown): &ldquo;TCP Send&rdquo;.</li>
<li>Second condition - <em>Path</em> <em>contains</em>: the definition for the SQL Server port (1443). In my examples it is: <code>win10-dev:ms-sql-s</code>. We set this filter condition, so we only see the ODBC communication.</li>
</ul>

<p>Disable all breakpoints in <em>WinDbg</em> and execute the code and watch the <em>Process Monitor</em> output:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_procmon_pull_2.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Process Monitor Output - II</em></p>

<p>The output we see in <em>Figure 3</em> is quite similar to what we see in <em>Figure 2</em>, apart from that we also see the packets the <code>BxlServer.exe</code> process sends. We see the same structure of three separate sections of packets, and we have packets of the same length in the three sections. Once again we see the data packet being in the last section, once again with a length of 133 (the outlined row). What are these packets and why do we have three sections? Let us see if we can figure out what the packets do with the help of <em>WireShark</em>:</p>

<ul>
<li>Start <em>WireShark</em> as admin.</li>
<li>On the opening screen double click the adapter for <em>Npcap</em> (most likely named &ldquo;Npcap Loopback Adapter&rdquo;).</li>
</ul>

<p>This starts capturing events immediately and to stop the capture you click <strong>Ctrl + E</strong>. What we want to do now is to create a <em>WireShark</em> display filter, so we only see network packets we are interested in. What we are interested in are packets for port 1433, and we set the filter in the text box just underneath the toolbox: <code>tcp.port==1433</code>.  Finally, we click on the right arrow to the right the filter box to apply the filter (outlined in red below):</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_filter.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>WireShark Display Filter</em></p>

<p>TThe assumption at this stage is that the <em>WireShark</em> capture is off. Start the capture (&ldquo;Ctrl+E&rdquo;) and then immediately execute the code. We stop the capture as soon as the code has finished executing:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_output_I.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>WireShark Output</em></p>

<p>In <em>Figure 5</em> we see part of the output from <em>WireShark</em>, and we may wonder why we see more packets than when we looked at the output from <em>Process Monitor</em> and why the packet sixes are not the same. There are a couple of answers to that:</p>

<ul>
<li><em>Process Monitor</em> only show packets that has a body (data part), but more messages are going over the wire (<code>ACK</code> etc.), and <em>WireShark</em> shows all of them.</li>
<li>The packet size question is related to the point above. <em>WireShark</em> shows the bytes going over the wire and that includes headers and so forth, whereas <em>Process Monitor</em> only shows the data size.</li>
<li>The <em>WireShark</em> display filter is set on port 1433 (anything going in and out of SQL Server), whereas in <em>Process Monitor</em> we filtered on traffic to and from the <code>BxlServer.exe</code> process. The section in <em>Figure 5</em> highlighted in blue is an example of this; this is the communication between <em>SQL Server Management Studio</em> and SQL Server.</li>
</ul>

<h4 id="yellow-section">Yellow Section</h4>

<p>Ok, so what does <em>WireShark</em> tell us? In <em>Figure 5</em> I have highlighted in yellow the part that corresponds to the highlighted yellow part in <em>Figure 3</em>. For this exercise, we can ignore the packets whose <em>Protocol</em> is TCP and concentrate on the ones which have TDS as a protocol. Just by looking at the info we see that the packets have to do with login and authentication: <code>TDS7 pre-login message</code> followed by <code>Response</code> followed by more pre-login messages, followed by <code>TLS Exchange</code> and a <code>Response</code>. I am not going into any details about these packets, but if you want all the &ldquo;gory&rdquo; details about TDS you find documentation here: <a href="https://msdn.microsoft.com/en-us/library/dd304523.aspx">[MS-TDS]: Tabular Data Stream Protocol</a>.</p>

<p>We now realise that the packets in the first section in <em>Figure 3</em> has to do with authentication. Seeing that we see packets with the same length in the green and blue sections in <em>Figure 3</em> we can safely assume those packets are also authentication packets, and a quick look at the packets in <em>WireShark</em> confirms that. What about the packets that are different?</p>

<h4 id="green-section">Green Section</h4>

<p>In the green section there are packets with lengths of 386, 138, 23 and 22, and in the blue section, we see packet lengths of 330 and 133. Well, we do know what the 133 packet is - that is the packet with the data SQL Server sends to the <code>BxlServer.exe</code> process. So, let us go back to the <em>WireShark</em> output and see what we can find out:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_output_II.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>WireShark Output II</em></p>

<p>We see in <em>Figure 6</em> the last packets of the green section packets in <em>Figure 3</em>. In <em>Figure 6</em> the first packet corresponds to the 405 packet in the green section in <em>Figure 3</em> which we know is a response package. To see what type of response packet it is, we click on the row in the &ldquo;Packet List&rdquo; pane in <em>WireShark</em> and the &ldquo;Packet Details&rdquo; pane shows the details about the packet:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_env_change.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>WireShark Packet Details</em></p>

<p>Aha, in <em>Figure 7</em> we see that this packet is an <code>EnvChange</code> packet (yellow highlight) which SQL Server use to notify the caller of an environment change (for example, database, language, and so on). In this case, it is a database context change from <code>master</code> (grey highlight) to <code>testParallel</code> (blue highlight). So this packet appears in all three section sin <em>Figure 3</em>. Now let us look at the 386, 138, 23, and 22 packets, which in <em>WireShark</em> have lengths of 856, 360, 130 and 128 and are highlighted in <em>Figure 6</em> with yellow, green, blue and purple respectively.</p>

<p><strong>WireShark Packet 856</strong>:</p>

<p>When we click on the packet in the &ldquo;Packet List&rdquo; pane, the &ldquo;Packet Details&rdquo; looks like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_prepare.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>WireShark Packet Details II</em></p>

<p>The packet details we see in <em>Figure 8</em> tells us that this is a &ldquo;Remote Procedure Call&rdquo; (which we also can see in <em>Figure 6</em>), and the call is to <code>sp_prepare</code> (highlighted in blue). The procedure <code>sp_prepare</code> is generally used for performance reasons and prepares a parameterized Transact-SQL statement and returns a statement handle for execution. The &ldquo;Packet Bytes&rdquo; pane shows us a hex dump the statement:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_packet_bytes.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>WireShark Packet Bytes</em></p>

<p><strong>WireShark Packet 360</strong>:</p>

<p>The 360 packet (green) is a response packet, and when we look at the packet we see:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_col_metadata.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>WireShark Packet Details and Bytes</em></p>

<p>We see in <em>Figure 10</em> that the response to <code>sp_prepare</code> is column metadata for the query.</p>

<p><strong>WireShark Packet 130 and 128</strong>:</p>

<p>Finally, the last two packets in the green section is a call to <code>sp_unprepare</code> (packet 130) and an &ldquo;End of message&rdquo; response (packet 128) to that.</p>

<h4 id="blue-section">Blue Section</h4>

<p>On to the blue section in <em>Figure 3</em>. We already said that there are only two packets that are different to the ones in the yellow and green sections:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_output_III.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>WireShark Output III</em></p>

<p>The packet highlighted in yellow in <em>Figure 11</em> is the packet with a length of 330 in <em>Figure 3</em>, and the 350 packet (highlighted in green) is the <em>Figure 3</em> 133 packet.</p>

<p><strong>WireShark Packet 744</strong>:</p>

<p>The 744 packet looks like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_sql_batch.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>WireShark Output III</em></p>

<p>In <em>Figure 12</em> we see how the packet is a SQL batch (yellow highlight), and it contains the actual query (green and blue highlights). So this is the query that SQL Server executes.</p>

<p><strong>WireShark Packet 350</strong>:</p>

<p>As we have mentioned a few times, the 350 packet contains the result of the query executed from the 744 packet, and the packet details and bytes look like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_query_result.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>WireShark Output Query Result</em></p>

<p>With a cursory glance at <em>Figure 13</em> we may think there is no difference between what we see in <em>Figure 10</em> - which shows the response to the <code>sp_prepare</code> call - and what is in <em>Figure 13</em>. However, looking a bit closer we see that this packet contains the result of the query where we see highlighted in green and blue the two first column values, 2 and 6 respectively. The column values are not present in <em>Figure 10</em>.</p>

<h2 id="sections">Sections</h2>

<p>By now we sort of understand what goes on when we pull the data in the local context. However, why do we have these different sections, and why do we see multiple authentications? I have no answer why we see multiple authentications; maybe I can get an answer from someone &ldquo;in the know&rdquo;. As for the multiple sections; I am not 100% clear about that - but it seems to have something to do with what RevoScaleR function that accesses the data, remember from <a href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> how loading of the data happens not in the <code>RxSqlServerData</code> call, but in the call which uses the data. So why I say that the sections are related to this is that if we, instead of doing a <code>rxLinMod</code> call, did something like <code>ds &lt;-</code>rxImport(mydata)` then we only see the green and blue sections. Hopefully, I can get some more information about the sections as well which I can update this post with.</p>

<h2 id="summary">Summary</h2>

<p>So this post discusses the internal workings in SQL Server when we pull data in the local context. We talked about how SQL Server first sends the script data to the SQL Satellite through the usual <code>sqllang!CXStmtQuery::ErsqExecuteQuery</code> and <code>sqllang!CSatelliteCargoContext::SendChunkEndMessage</code>. After <code>SendChunkEndMessage</code> <code>ErsqExecuteQuery</code> is called again and at that stage the statement query executes.</p>

<p>The data transfer between the SqlSatellite (hosted by <code>BxlServer.exe</code>) and SQL Server happens through TDS packets, and we saw how authentication packages are sent multiple times, followed by <code>sp_prepare</code>. Finally, SQL Server executes the query and returns the result to the SqlSatellite.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 30]]></title>
    <link href="/2018/07/interesting-stuff---week-30/"/>
    <id>/2018/07/interesting-stuff---week-30/</id>
    <author>
      <name>nielsb</name>
    </author>
    <published>2018-07-29T19:11:58+02:00</published>
    <updated>2018-07-29T19:11:58+02:00</updated>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/chaos-engineering-autonous-vehicles">Properties of Chaos</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation about how and why chaos engineering is being applied to autonomous vehicle safety, and how to advance chaos engineering practices to explore beyond basic properties like system availability and extend into verifying system correctness.</li>
</ul>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="https://blog.pulumi.com/program-the-cloud-with-12-pulumi-pearls">Program the Cloud with 12 Pulumi Pearls</a>. In the <a href="/2018/06/interesting-stuff---week-25/">week 25</a> roundup I wrote about <a href="https://twitter.com/funcOfJoe">Joe Duffy</a> and his <a href="http://pulumi.com/">Pulumi</a>. This blog post by <a href="https://twitter.com/funcOfJoe">Joe</a> demonstrates some fun ways you can program the cloud using Pulumi. Very interesting!</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/sqlserverstorageengine/2018/07/23/pearl-abyss-massive-scale-using-azure-sql-database/">Pearl Abyss: Massive Scale using Azure SQL Database</a>. A blog post, showing off how you can achieve a massive scale of SQL Server by using Azure SQL Database. 7,500 cores, 46 instances 500,000 queries per second! Can I have two of those, please!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.youtube.com/watch?v=ExEWJVjj-RA">Demo: Build a Streaming Application with KSQL</a>. A YouTube video, illustrating how to build a streaming application using KSQL.</li>
<li><a href="https://www.confluent.io/blog/ansible-playbooks-for-confluent-platform/">Ansible Playbooks for Confluent Platform</a>. This blog post covers how <a href="https://www.confluent.io/">Confluent</a> wants to make it super easy to set up <a href="https://www.confluent.io/product/confluent-platform/">Confluent Platform</a>. In this post, they introduce their first set of Ansible playbooks. It certainly seems easy!</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="http://blog.revolutionanalytics.com/2018/07/a-quick-tour-of-ai-services-in-azure.html">A quick tour of AI services in Azure</a>. A post by <a href="https://twitter.com/revodavid">David</a> at <a href="http://blog.revolutionanalytics.com">Revolution Analytics</a> talking about a video giving a quick overview of some of the services available in Azure to build AI-enabled applications. Watch the video if you are interested in AI and Azure.</li>
<li><a href="https://blogs.msdn.microsoft.com/mlserver/2018/07/26/dockerizing-r-and-python-web-services/">Dockerizing R and Python Web Services</a>.  A post looking into how to build a Docker image containing <strong>Machine Learning Server 9.3</strong>using Dockerfiles and how-to-perform Both R and Python operations.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I am close to finish the third post in the <a href="/series/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series. I hope to have it out by the end of this upcoming week.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 29]]></title>
    <link href="/2018/07/interesting-stuff---week-29/"/>
    <id>/2018/07/interesting-stuff---week-29/</id>
    <author>
      <name>nielsb</name>
    </author>
    <published>2018-07-22T16:06:58+02:00</published>
    <updated>2018-07-22T16:06:58+02:00</updated>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending. Unfortunately there was not much that caught my eye this week, so this is a short post.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2018/07/zuul-push-messaging">Scaling Push Messaging for Millions of Devices @Netflix - Susheel Aroskar at QCon NY</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about Zuul Push, which is a high performance asynchronous service based on Apache Netty, a non-blocking I/O (NIO)-based network application framework. The technology handles more than 5.5 million connected clients at peak.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/apache-kafka-vs-enterprise-service-bus-esb-friends-enemies-or-frenemies/">Apache Kafka vs. Enterprise Service Bus (ESB)—Friends, Enemies, or Frenemies?</a>. A blog post showing why so many enterprises leverage the open source ecosystem of Apache Kafka for successful integration of different legacy and modern applications, and how this differs but also complements existing integration solutions like ESB or ETL tools.</li>
<li><a href="https://databricks.com/blog/2018/07/19/simplify-streaming-stock-data-analysis-using-databricks-delta.html">Simplify Streaming Stock Data Analysis Using Databricks Delta</a>. A post about how Databricks Delta helps solving many of the pain points of building a streaming system to analyze stock data in real-time.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.usejournal.com/python-vs-and-r-for-data-science-833b48ccc91d">Python vs (and) R for Data Science</a>. This post acts as a guide for those wishing to choose between Python and R Programming languages for Data Science.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I am <del>busy</del> struggling with writing a couple of posts in the <a href="/series/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series, as well as the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I do hope to have at least one out in a weeks time (or so).</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 28]]></title>
    <link href="/2018/07/interesting-stuff---week-28/"/>
    <id>/2018/07/interesting-stuff---week-28/</id>
    <author>
      <name>nielsb</name>
    </author>
    <published>2018-07-15T06:15:28+02:00</published>
    <updated>2018-07-15T06:15:28+02:00</updated>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/07/09/system-io-pipelines-high-performance-io-in-net/">System.IO.Pipelines: High performance IO in .NET</a>. A blog post which announces <code>System.IO.Pipelines</code> which is a new library that is designed to make it easier to do high performance IO in .NET. I wish it had been around when we wrote socket code, way back when.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/netflix-techblog/evolution-of-application-data-caching-from-ram-to-ssd-a33d6fa7a690">Evolution of Application Data Caching : From RAM to SSD</a>. A blog post about Netflix and how they move from purely RAM based caches to a hybrid between RAM and SSD. Very, very interesting!</li>
<li><a href="https://medium.com/netflix-techblog/auto-scaling-production-services-on-titus-1f3cd49f5cd7">Auto Scaling Production Services on Titus</a>. This blog post, also from Netflix, discusses auto-scaling on their container management system <a href="https://medium.com/netflix-techblog/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436">Titus</a>. It is interesting to read how the interaction happens between the AWS Auto Scaling engine and Titus.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/kafka-1-0-on-hdinsight-lights-up-real-time-analytics-scenarios/">Kafka 1.0 on HDInsight lights up real-time analytics scenarios</a>. This Microsoft blog post discusses the advantages that Kafka 1.0 on Azure HDInsight provides for data scientists and data analysts.</li>
<li><a href="/2018/07/install-confluent-platform-kafka-on-windows/">Install Confluent Platform (Kafka) on Windows</a>. This post from yours truly discusses we can install Kafka, in the guise of Confluent Platform, on Windows Subsystem for Linux (WSL). Useful if you, like me, is a Windows dude (or dudette) and you want to run Kafka locally on your development box.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="http://blog.revolutionanalytics.com/2018/07/r-351-update-now-available-.html">R 3.5.1 update now available</a>. This post by <a href="https://twitter.com/revodavid">David</a> talks about the new version of R: 3.5.1.</li>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/07/09/announcing-ml-net-0-3/">Announcing ML.NET 0.3</a>. At Build 2018 Microsoft released ML.NET 0.1, a cross-platform, open source machine learning framework for .NET developers, and I posted about it in the <a href="/2018/05/interesting-stuff---week-19/">week 19 roundup</a>. A month or two later they released ML.NET 0.2 which I covered in the roundup for <a href="/2018/06/interesting-stuff---week-23/">week 23</a>. It is now time for ML.NET 0.3 with quite a few new enhancements. What interests me is to see what &ldquo;cool&rdquo; new features application developers dreams up with this.</li>
<li><a href="https://www.infoq.com/presentations/r-framework-ai-apps">R for AI developers</a>. So, <a href="https://twitter.com/revodavid">David</a> is at it again. This time he did a presentation at QCon.ai where he makes a case for R as a platform for developing models for intelligent applications. The presentation is a must-see!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Install Confluent Platform (Kafka) on Windows]]></title>
    <link href="/2018/07/install-confluent-platform-kafka-on-windows/"/>
    <id>/2018/07/install-confluent-platform-kafka-on-windows/</id>
    <author>
      <name>nielsb</name>
    </author>
    <published>2018-07-10T18:43:48+02:00</published>
    <updated>2018-07-10T18:43:48+02:00</updated>
    <content type="html"><![CDATA[<p>You who follows my blog and have read my weekly roundups you know that I am quite (that is an understatement) interested in Apache Kafka and I am curious to find out what &ldquo;cool&rdquo; things one can do with it. For that, I want to be able to test &ldquo;stuff&rdquo; quickly. When I test and try out new things, I usually do it on my development box which contains everything I need: <strong>SQL Server</strong>, <strong>RabbitMQ</strong>, <strong>RStudio</strong>, <strong>Microsoft Machine Learning Server</strong>, <strong>Visual Studio</strong> and the list goes on.</p>

<p>So seeing that I have most of my &ldquo;tools of the trade&rdquo; on my machine I obviously also would like to have Kafka on the box. Herein lies a problem, I am a Windows dude and Kafka, and Windows do not gel. Yes, some people are running Kafka on Windows, but it is a chore. OK, so what to do? Sure, I could potentially run Kafka on a virtual machine, or in a Docker image, but it is not as transparent as I would like it to be (yeah, I am lazy).</p>

<p>Hmm, Microsoft did introduce the ability to run Linux binary executables (in ELF format) natively on Windows 10 in Windows 10 version 1607. The feature is called <strong>Windows Subsystem for Linux</strong> (WSL), and since I am now running version 1803, maybe I should try and install Kafka in <em>WSL</em>.</p>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>To install and run the <strong>Confluent Platform</strong>, (which contains Kafka), on <em>WSL</em> there are some pre-reqs:</p>

<ul>
<li><em>WSL</em> installed (fairly obvious).</li>
<li>Java 1.7 or later.</li>
</ul>

<h2 id="windows-subsystem-for-linux">Windows Subsystem for Linux</h2>

<p><em>WSL</em> is primarily aimed at developers, and it allows you to run Linux environments directly on Windows in a native format and without the overhead of a virtual machine. Let us retake a look at that statement: <em>run Linux environments directly on Windows in a native format</em>. Yes native format, <em>WSL</em> is not a UNIX-like environment like Cygwin, which wraps non-Windows functionality in Win32 system calls but it serves Linux programs as special, isolated minimal processes (<em>pico-processes</em>) attached to kernel-mode <em>pico-providers</em>. If you want to read all the &ldquo;gory&rdquo; details about <em>WSL</em>: <a href="https://blogs.msdn.microsoft.com/wsl/2016/04/22/windows-subsystem-for-linux-overview/">Windows Subsystem for Linux Overview</a> gives you an excellent introduction.</p>

<p>Installing <em>WSL</em> is very easy; you first enable <em>WSL</em> either via a Powershell command: <code>Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux</code> or by switching it on from &ldquo;Turn Windows features on or off&rdquo; via &ldquo;Control Panel | Programs and Features&rdquo;:</p>

<p><img src="/images/posts/inst_kafka_wsl_enable_wsl.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Enable WSL</em></p>

<p>You should restart the machine after enabling <em>WSL</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> So I have enabled <em>WSL</em> a few times now, and some of the times I have not had to restart.</p>
</blockquote>

<p>When <em>WSL</em> is enabled you need to download and install a distro from the Windows Store. When <em>WSL</em> first was introduced the only distro available was Ubuntu, since then quite a few have been added, and now the distro list looks like so:</p>

<ul>
<li>Ubuntu</li>
<li>OpenSUSE</li>
<li>SLES</li>
<li>Kali Linux</li>
<li>Debian GNU/Linux</li>
</ul>

<p>I started with <em>WSL</em> when Ubuntu was the only distro available, so I have &ldquo;stuck&rdquo; with it, but I do not think the distros are that different. To continue, you choose a distro and let it install. Finally, you start the command shell for the distro from the Windows &ldquo;Start&rdquo; menu:</p>

<p><img src="/images/posts/inst_kafka_wsl_start_distro.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Start the Distro</em></p>

<p>When you start up the distro for the first time the setup finishes, and you are prompted to enter a root password. Now is probably a good time to run <code>sudo apt-get update -y &amp;&amp; sudo apt-get upgrade -y</code> where <code>sudo apt-get update -y</code> updates the list of all current program packages in the repositories to determine which packages are candidates for upgrading. The command <code>sudo apt-get upgrade -y</code> upgrades all current program packages in the operating system.</p>

<blockquote>
<p><strong>NOTE:</strong> The above commands are for Ubuntu, so if you have another distro installed check the commands for that particular distro.</p>
</blockquote>

<h4 id="java">Java</h4>

<p>When your distro is up and running, you can now install Java. When reading the documentation about <a href="https://docs.confluent.io/current/installation/versions-interoperability.html#java">supported Java versions</a>, you see that <strong>Confluent Platform</strong> 4.1 is the last version with support for Java 1.7. The <strong>Confluent Platform</strong> version I use is the latest preview (version 5.x), so I install 1.8. Oh, and do not try with 1.9 -  it does not work.</p>

<p>The <a href="https://docs.confluent.io/current/installation/versions-interoperability.html#java">docs</a> mention the JDK, but I have found that the JRE works as well (since I am not writing any Java code) and I use the open source version of Java - OpenJDK. So to install you run the following from the bash shell:</p>

<pre><code class="language-bash">&gt; sudo apt install openjdk-8-jre-headless 
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Install JRE</em></p>

<p>As <em>WSL</em> has no GUI, I choose to install the headless version of the JRE as we see in <em>Code Snippet 1</em>. Finally, to check that it installed correctly I do <code>&gt; java -version</code> and the result is like so:</p>

<pre><code class="language-bash">openjdk version &quot;1.8.0_171&quot;
OpenJDK Runtime Environment (build 1.8.0_171-8u171-b11-0ubuntu0.18.04.1-b11)
OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode)
</code></pre>

<p><strong>Code Snippet 2:</strong> Java Version Output*.</p>

<p>In <em>Code Snippet 2</em> everything looks OK, so we can now go ahead with the main attraction.</p>

<h2 id="confluent-platform">Confluent Platform</h2>

<p>By now you may ask yourself what is this thing <strong>Confluent Platform</strong>? Well, <a href="https://confluent.io">Confluent</a> is a company founded by the guys (and girls) that originally built Kafka back at LinkedIn. The company is now focusing on building a streaming platform to help other companies get easy access to enterprise data as real-time streams.</p>

<p>The <strong>Confluent Platform</strong> improves Apache Kafka by expanding its integration capabilities, adding tools to optimise and manage Kafka clusters, and methods to ensure the streams are secure. <strong>Confluent Platform</strong> makes Kafka easier to build and easier to operate. The <strong>Confluent Platform</strong> comes in two flavours:</p>

<ul>
<li><a href="https://www.confluent.io/product/confluent-open-source/">Confluent Open Source</a> is freely downloadable.</li>
<li><a href="https://www.confluent.io/product/confluent-enterprise/">Confluent Enterprise</a> is available on a subscription basis.</li>
</ul>

<p>Back in April Confluent started releasing preview versions of the <strong>Confluent Platform</strong> with the latest and the greatest and that is what I am using. At the time I write this the June preview has just been released, and that is what I am installing here.</p>

<h4 id="installation">Installation</h4>

<p>Before we can install, we need to download the install media which you do from <a href="https://www.confluent.io/preview-release">here</a>. When clicking the &ldquo;Download &hellip;&rdquo; button, a form &ldquo;pops up&rdquo; where you choose your download format and enter your details. I usually choose <code>tar.gz</code>, and that is what I base the following instructions on. Download the file to your PC and then in the bash shell:</p>

<ul>
<li>Create a directory where to extract the files to.</li>
<li><code>cd</code> to the download directory:</li>
</ul>

<p><img src="/images/posts/inst_kafka_wsl_mkdir.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Make Kafka Directory</em></p>

<p>In <em>Figure 3</em> we see how I create the <code>/opt/kafka</code> directory, and how I <code>cd</code> to the Windows directory where my downloaded files are. One of the cool things with <em>WSL</em> is that the local Windows drives gets automatically mounted under the <code>/mnt</code> folder. I can now extract the files:</p>

<blockquote>
<p><strong>NOTE:</strong> The only reason I chose to create the <code>kafka</code> directory under <code>/opt</code> is that traditionally <code>/opt</code> is for third-party applications.</p>
</blockquote>

<p><img src="/images/posts/inst_kafka_wsl_untar.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Extract Files</em></p>

<p>So I <code>tar</code> the files, and we see in <em>Figure 4</em> how the files are extracted. To extract the files takes a couple of minutes and when done we can drill down into the extracted directories and files:</p>

<p><img src="/images/posts/inst_kafka_wsl_kafka_dirs_files.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Directory and File Structure</em></p>

<p>In <em>Figure 5</em> we see how directories and files ended up under a <code>confluent-version...</code> directory (outlined in white) and when we <code>ls</code> into that directory we see sub-directories (also outlined in white), and amongst them a <code>bin</code> directory.</p>

<p>When we drill down into the <code>bin</code> directory and list the content we see a file named <code>confluent</code>. This is an executable file, and we use this file to start and stop all the Confluent components. The <code>bin</code> directory also contains executable files to start and stop individual components, such as <code>kafka-server-start</code>, <code>kafka-server-stop</code> and <code>zookeeper-server-start</code>, and so forth.</p>

<p>Right, enough of this - let us see if we can get the show on the road and spin up all components:</p>

<p><img src="/images/posts/inst_kafka_wsl_start_kafka.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Start Confluent</em></p>

<p>To start all the Confluent components, we use the command <code>sudo ./confluent start (from the</code>bin` directory) and in <em>Figure 6</em> we see how the various components startup, awesome!</p>

<h4 id="control-center">Control Center</h4>

<p>Part of the <strong>Confluent Platform</strong> installation (Enterprise version) is the <em>Control Center</em>. The <em>Control Center</em> (I copied the text from the <a href="https://www.confluent.io/confluent-control-center/">Control Center</a> site) &ldquo;gives the administrator monitoring and management capabilities, delivering automated, curated dashboards so that Kafka experts can easily understand what is happening without tweaking dashboards&rdquo;. So let us see if we can connect with the <em>Control Center</em>. If we connect from the same machine as we installed <strong>Confluent Platform</strong> on, the address is <code>http://localhost:9021</code>:</p>

<p><img src="/images/posts/inst_kafka_wsl_control_center.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Confluent Control Center</em></p>

<p>Cool, <em>Control Center</em> seems to be up and running, let us now use it to create a <em>Topic</em> so we can do a final test.</p>

<h4 id="topics">Topics</h4>

<p>When you send messages to a Kafka broker, you typically send it to a &ldquo;Topic&rdquo;, which is like a collection point in the broker for &ldquo;like-minded&rdquo; messages. If you are a database dude like me, you can see it as a table in a database where you keep records of the same type.</p>

<p>Typically you create multiple topics in you Kafka cluster to cater for multiple message types, and <em>Control Center</em> can help you with that. In <em>Figure 7</em> we see at the bottom left corner, outlined in red, &ldquo;Topics&rdquo;. Click on that, and you see existing default topics. Click on, in the far right corner, the &ldquo;Create topic&rdquo; button and you see something like so:</p>

<p><img src="/images/posts/inst_kafka_wsl_create_topic.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Topics</em></p>

<p>In the topic name box enter &ldquo;testing&rdquo; and then click &ldquo;Create with defaults&rdquo; and we are back seeing the existing topics as well as the newly created &ldquo;testing&rdquo; topic:</p>

<p><img src="/images/posts/inst_kafka_wsl_topic_created.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>New Topic</em></p>

<p>When we have a topic, we can now see whether we can publish and consume messages.</p>

<h2 id="test-send-receive">Test Send &amp; Receive</h2>

<p><strong>Confluent Platform</strong> is now up and running, and you can now start doing &ldquo;cool&rdquo; stuff. However, to make sure everything works let us use the built-in command line clients to send and receive some test messages.</p>

<p>What we do is that in the open bash shell we <code>cd</code> to the <code>/opt/kafka/confluent-xxx/bin/</code> directory. We use the command line producer <code>kafka-console-producer</code> to send messages:</p>

<pre><code class="language-bash">sudo ./kafka-console-producer --broker-list localhost:9092 --topic testing
&gt;Hello World!
&gt;Life Is Awesome!
&gt;We Have Installed Kafka on Windows!
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Publishing Messages</em></p>

<p>We see in <em>Code Snippet 3</em> how we target the local broker on port 9092, and the topic we send to is the &ldquo;testing&rdquo; topic we created above. After hitting enter, we create one message after each other (hit enter in between).</p>

<p>To consume messages we open a second bash shell and <code>cd</code> into the <code>/bin</code> directory as before, and to receive messages we use the <code>kafka-console-consumer</code> command line client:</p>

<pre><code class="language-bash">sudo ./kafka-console-consumer --bootstrap-server localhost:9092 --topic testing --from-beginning
Hello World!
Life Is Awesome!
We Have Installed Kafka on Windows!
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Consume Messages</em></p>

<p>When running the consumer we see in <em>Code Snippet 4</em> how we receive the messages we just sent. If we were to go back to the publisher and create some more messages we immediately see them in the consumer bash shell. It works! So now we can start creating streaming applications using proper <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">clients</a>. If you write .NET applications, I suggest you look at the <a href="https://github.com/confluentinc/confluent-kafka-dotnet">Confluent client</a> which is very feature rich.</p>

<p>When we are done with the <strong>Confluent Platform</strong>, we stop it from the <code>/bin</code> directory:</p>

<p><img src="/images/posts/inst_kafka_wsl_stop_kafka.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Stopping Kafka</em></p>

<p>We stop Kafka by calling <code>sudo ./confluent stop</code> and then as <em>Figure 10</em> shows, all components shut down in an orderly fashion.</p>

<p>We have installed <strong>Confluent Platform</strong> on <em>WSL</em>, started it, published and consumed some messages and stopped it. All is good! Or is it?</p>

<h2 id="issue">ISSUE</h2>

<p>So what happens when you try to start the platform again:</p>

<p><img src="/images/posts/inst_kafka_wsl_start_error.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Error at Start Up</em></p>

<p>That is not good! We get an error when we try to start the platform after a shutdown. What we see here is a &ldquo;half&rdquo; known issue which is most prevalent on Windows machines, and it has to do with Kafka trying to clean up old log files. If you drill down in the Kafka log files you see an error looking something like this: <code>FATAL Shutdown broker because all log dirs in &lt;path_to_logs&gt; have failed (kafka.log.LogManager)</code>.</p>

<p>At the moment I do not have a solution for it other than that before each startup run something like so: <code>sudo rm -fr /tmp/confl*</code> which removes all Kafka related log directories. This is obviously not a solution in a production environment or a &ldquo;proper&rdquo; test/dev environment but for us just wanting to do some &ldquo;quick and dirty&rdquo; testing on <em>WSL</em> it is sufficient.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we discussed a little bit what <em>WSL</em> is and how we can install <strong>Confluent Platform</strong> on <em>WSL</em>. We looked at we can test the installation by creating a topic and then publish and consume messages using the command line publish and consume clients.</p>

<p>Having <strong>Confluent Platform</strong> installed we can now use a <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">client of choice</a> to start doing &ldquo;cool&rdquo; stuff. Keep an eye on my blog for future <strong>Confluent Platform</strong> and Kafka posts!</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 27]]></title>
    <link href="/2018/07/interesting-stuff---week-27/"/>
    <id>/2018/07/interesting-stuff---week-27/</id>
    <author>
      <name>nielsb</name>
    </author>
    <published>2018-07-08T12:11:28+02:00</published>
    <updated>2018-07-08T12:11:28+02:00</updated>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/ip-filtering-for-event-hubs-and-service-bus/">IP filtering for Event Hubs and Service Bus</a>. A frequently asked for feature in Azure Event Hubs is the ability to restrict access to the Event Hubs to certain well-known sites, alternatively rejecting traffic from specific IP addresses. The Azure team has now announced a public preview of IP filtering.</li>
<li><a href="https://databricks.com/blog/2018/07/02/build-a-mobile-gaming-events-data-pipeline-with-databricks-delta.html">Build a Mobile Gaming Events Data Pipeline with Databricks Delta</a>. This blog post shows how to build a data pipeline in AWS using the <a href="https://databricks.com/product/unified-analytics-platform">Databricks Unified Analytics Platform</a>. Even though they in the blog post use AWS it should be possible to do the same on Azure since Databricks is now available there as well.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="http://mattwarren.org/2018/07/05/.NET-JIT-and-CLR-Joined-at-the-Hip/">.NET JIT and CLR - Joined at the Hip</a>. This is another excellent blog post by <a href="https://twitter.com/matthewwarren">Matthew</a>, looking at how the .NET JIT compiler works together with CLR.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2018/07/boner-events-first-microservices">QCon NY: Jonas Bonér on Designing Events-First Microservices</a>. An <a href="https://www.infoq.com/">InfoQ</a> article summarising a QCon talk by <a href="http://jonasboner.com/">Jonas Bonér</a> of <a href="https://akka.io/">Akka</a> fame. The main point Jonas makes is that events-first domain-driven design (DDD) and event streaming are critical in a microservices based architecture. Oh and by th way, if you are interested in event-driven systems and microservices go and download Jonas mini-book <a href="https://www.lightbend.com/blog/reactive-microsystems-the-evolution-of-microservices-at-scale-free-oreilly-report-by-jonas-boner">Reactive Microsystems - The Evolution Of Microservices At Scale</a>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/june-preview-release-confluent-plaform/">June Preview Release: Packing Confluent Platform with the Features You Requested!</a>. A blog post by the Confluent team announcing the latest preview release of Confluent Platform. This release packs quite a few new features, and I am especially interested in the KSQL support for nested data as well as the ability to join two streams together.</li>
<li><a href="https://www.infoq.com/news/2018/07/event-sourcing-kafka-streams">Experiences from Building an Event-Sourced System with Kafka Streams</a>. An article from <a href="https://www.infoq.com/">InfoQ</a> about how engineers at <a href="https://www.wix.com/">Wix</a> built an event sourced system using <a href="https://kafka.apache.org/documentation/streams/">Kafka Streams</a>.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0">A Feature Selection Tool for Machine Learning in Python</a>. This blog post is about a Python tool that helps with feature selection in a dataset.</li>
<li><a href="https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420">A Complete Machine Learning Project Walk-Through in Python: Part One</a>. First post in a series walking through a complete Python machine learning solution.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<ul>
<li><a href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a>. I finally published part 2 of the <a href="/series/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series. In the post, I tried to figure out why the performance is so much better when executing in a <strong>SQL Server Compute Context</strong> in <strong>SQL Server ML Services</strong> compared to executing in the local context (it is SQL Server after all). Even though I &ldquo;sort of&rdquo; figured it out, a few questions arose and hopefully I can answer those questions in a future post.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[sp_execute_external_script and SQL Compute Context - II]]></title>
    <link href="/2018/07/sp_execute_external_script-and-sql-compute-context---ii/"/>
    <id>/2018/07/sp_execute_external_script-and-sql-compute-context---ii/</id>
    <author>
      <name>nielsb</name>
    </author>
    <published>2018-07-07T10:54:21+02:00</published>
    <updated>2018-07-07T10:54:21+02:00</updated>
    <content type="html"><![CDATA[<p>I wrote the post <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">sp_execute_external_script and SQL Compute Context - I</a> about how the <strong>SQL Server Compute Context</strong> (SQLCC) works with <code>sp_execute_external_script</code> (SPEES), as I wanted to correct some mistakes I did in the <a href="/2018/03/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services - sp_execute_external_script - III</a> post. I initially thought one post would be enough, but quite soon I realised I was too optimistic, and at least one more post would be needed, if not more. So this is the first followup post about SPEES and SQLCC.</p>

<p>To see other posts (including this) in the series, go to <a href="/series/spees_and_sql_compute_context"><strong>sp_execute_external_script and SQL Server Compute Context</strong></a>.</p>

<p>One of the reasons for me realising that one post is not enough is that while I wrote and executed code for the first <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">post</a>, I noticed some fairly significant performance differences using SQLCC compared to not using SQLCC (SQLCC performed better :)). So that is part of what we look at in this post.</p>

<p></p>

<h2 id="recap">Recap</h2>

<p>In quite a few posts about <strong>SQL Server Machine Learning Services</strong> we have discussed how, as part of the functionality in RevoScaleR, you can define where a workload executes. By default, it executes on your local machine, but you can also set it to execute in the context of somewhere else: Hadoop, Spark and also SQL Server. So, in essence, you can run some code on your development machine and have it execute in the environments mentioned above.</p>

<p>In the <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> post we saw that even when we executed from inside SQL Server, the compute context was the local context: <code>RxLocalSeq</code>. If we want to use the SQLCC we used <code>RxInSqlServer</code> and <code>rxSetComputeContext</code>:</p>

<pre><code class="language-r"># set up the connection string
sqlServerConnString &lt;- &quot;Driver=SQL Server;
                        server=.; # localhost
                        database=testParallel;
                        uid=some_uid;pwd=some_pwd&quot;

# set up the context
sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                        numTasks = 1)
# set the compute context to be the sql context
rxSetComputeContext(sqlCtx)    
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Set up SQL Server Compute Context</em></p>

<p>To setup the context we see in <em>Code Snippet 1</em> how we use a connection string pointing to the SQL Server where we want to execute the code. In this case, it is the instance we are on.</p>

<blockquote>
<p><strong>NOTE:</strong> The connection string is for where we want to execute, not necessarily where the data we use resides.</p>
</blockquote>

<p>We also see in <em>Figure 1</em> how <code>RxInSqlServer</code> has the <code>numTasks</code> parameter for you to set the number of tasks (processes) to run for each computation. The parameter defines the maximum number of tasks SQL Server can use. SQL Server can, however, decide to start fewer processes. Finally in <em>Figure 1</em> we call <code>rxSetComputeContext</code> which ensures that any code with functions that support SQLCC, executes under the compute context.</p>

<p>In the <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> post, we saw how when we execute inside of SQL Server via SPEES we by default run in the local context and only by setting the context as in <em>Code Snippet 1</em> we can execute in SQLCC.</p>

<p>An interesting observation when we set the <code>numTasks</code> parameter to a value greater than 1 is that when we run the code, we run it hosted in an <code>mpiexec.exe</code> process:</p>

<p><img src="/images/posts/sql_ml_services_comp_mpi.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Parallel Execution in Compute Context</em></p>

<p>In <em>Figure 1</em> we now see not only the &ldquo;usual&rdquo; RTerm and <code>BxlServer.exe</code> processes but also a new hosting process, outlined in red, <code>mpiexec.exe</code>. Underneath the <code>mpiexec.exe</code> process we see the <code>smpd.exe</code> process (outlined in green) and then four RTerm processes with <code>BxlServer.exe</code> processes which handle the workload. So, <code>mpiexec.exe</code> and <code>smpd.exe</code> are parts of <a href="https://msdn.microsoft.com/en-us/library/bb524831%28v=vs.85%29.aspx?f=255&amp;MSPPError=-2147217396"><strong>Microsoft MPI</strong></a> which is an implementation of MPI which is a communication protocol for programming parallel computers.</p>

<p>All this is somewhat interesting, but the most interesting thing (at least for me) is the performance difference we saw when executing the same code in the local context compared to the SQLCC. When executing with <code>numTasks</code> set to 1 (as it would be under the local context) code that ran in ~40 seconds in the local context took ~30 seconds to run in SQLCC! Once again, we did not run it with multiple tasks in SQLCC, so just be running in SQLCC we received a performance gain of about 30%!</p>

<blockquote>
<p><strong>NOTE:</strong> The performance gain is of course not always 30%, it depends on data volumes.</p>
</blockquote>

<p>So, as I said at the beginning of this post - let us try and figure out why the performance is better using SQLCC.</p>

<h2 id="housekeeping">Housekeeping</h2>

<p>Before we &ldquo;dive&rdquo; into today&rsquo;s topics let us look at the code and the tools we use today. This section is here for those who want to follow along in what we are doing in the post.</p>

<h4 id="helper-tools">Helper Tools</h4>

<p>To help us figure out the things we want, we use <em>Process Monitor</em> to filter TCP traffic.</p>

<h4 id="code">Code</h4>

<p>This is the database objects we use in this post:</p>

<pre><code class="language-sql">USE master;
GO

SET NOCOUNT ON;
GO

DROP DATABASE IF EXISTS TestParallel;
GO

CREATE DATABASE TestParallel;
GO

USE TestParallel;
GO

DROP TABLE IF EXISTS dbo.tb_Rand_50M
GO
CREATE TABLE dbo.tb_Rand_50M
(
  RowID bigint identity PRIMARY KEY, 
  y int NOT NULL, rand1 int NOT NULL, 
  rand2 int NOT NULL, rand3 int NOT NULL, 
  rand4 int NOT NULL, rand5 int NOT NULL,
);
GO

INSERT INTO dbo.tb_Rand_50M(y, rand1, rand2, rand3, rand4, rand5)
SELECT TOP(50000000) CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 25 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 50 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 100 AS INT)
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO

</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Setup of Database, Table and Data</em></p>

<p>We use more or less the same database and database object as in the <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> post:</p>

<ul>
<li>A database: <code>TestParallel</code>.</li>
<li>A table: <code>dbo.tb_Rand_50M</code>. This table contains the data we want to analyse.</li>
</ul>

<p>In addition to creating the database and the table <em>Code Snippet 2</em> also loads 50 million records into the <code>dbo.tb_Rand_50M</code>. Be aware that when you run the code in <em>Code Snippet 2</em> it may take some time to finish due to the loading of the data. Yes, I know - the data is entirely useless, but it is a lot of it, and it helps to illustrate what we want to do.</p>

<p>The code we use is almost like what we used in <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a>:</p>

<pre><code class="language-sql">DECLARE @isCtx bit = 0;
DECLARE @numTasks int = 1;
DECLARE @start datetime2 = SYSUTCDATETIME();
EXEC sp_execute_external_script
      @language = N'R'
    , @script = N'
      # set up the connection string
      sqlServerConnString &lt;- &quot;Driver=SQL Server;server=.;
                              database=testParallel;
                              uid=&lt;username&gt;;pwd=&lt;userpwd&gt;&quot;
      
      if(useContext == 1) {
        sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                                numTasks = tasks)
        # set the compute context to be the sql context
        rxSetComputeContext(sqlCtx)
      }

      mydata &lt;- RxSqlServerData(sqlQuery = &quot;SELECT y, rand1, rand2, 
                                            rand3, rand4, rand5 
                                            FROM dbo.tb_Rand_50M&quot;,
                                connectionString = sqlServerConnString);
                        
      myModel &lt;- rxLinMod(y ~ rand1 + rand2 + rand3 + rand4 + rand5, 
                      data=mydata)

      OutputDataSet &lt;- data.frame(nRows=myModel$nValidObs);'
    , @params = N'@tasks int, @useContext bit'
    , @tasks = @numTasks
    , @useContext = @isCtx
WITH RESULT SETS ((NumberRows int NOT NULL));
SELECT DATEDIFF(ms, @start, SYSUTCDATETIME())
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Test Code</em></p>

<p>As we see in <em>Code Snippet 3</em> we parameterize the <code>sp_execute_external_script</code> call, and we have parameters for whether to use the SQLCC and also how many tasks to run when executing in the context. The default is to execute in the local context, and when executing in SQLCC <code>numTasks</code> is 1.</p>

<h2 id="performance-differences">Performance Differences</h2>

<p>To start with, let us repeat - more or less - what we did in <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> and compare execution times when running in the local context (<code>@isCtx = 0</code>) and when in SQLCC  (<code>@isCtx = 1</code>). In both cases, we execute with the default number of tasks (<code>numTasks = 1</code>).</p>

<blockquote>
<p><strong>NOTE:</strong> Do a couple of executions in the local context as well as in the SQLCC to ensure you get representative numbers for both.</p>
</blockquote>

<p>When I run the code on my SQL Server instance I get the following results:</p>

<ul>
<li>Local context: ~40 seconds</li>
<li>SQLCC: ~24 seconds</li>
</ul>

<p>So, the same workload shows an approximately 40% performance improvement when running in the SQLCC compared to the local context and this is in line with what we saw in <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a>. Why is this, we do the same things:</p>

<ul>
<li>We load data</li>
<li>We apply the <code>rxLinMod</code> function.</li>
<li>We run with the same number of tasks (single threaded).</li>
</ul>

<p>A question I have now is at what stage in the script, the script receives the 50 million rows? Comment out in the code, (<em>Code Snippet 3</em>), the <code>myModel</code> and <code>OutputDataSet</code> lines of code. When you now execute in the local context, you see the execution time is ~ 1 second. When you do the same in the SQLCC the time is about the same. It seems like the actual loading of the data happens not in the <code>RxSqlServerData</code> call, but in the call - in this case - to <code>rxLinMod</code>. Hmm, I wonder what happens if we instead of pulling the data, pushed the data to the <code>rxLinMod</code> call by using <code>@input_data_1</code>:</p>

<pre><code class="language-sql">DECLARE @isCtx bit = 0;
DECLARE @numTasks int = 1;
DECLARE @start datetime2 = SYSUTCDATETIME();
EXEC sp_execute_external_script
      @language = N'R'
    , @script = N'
      # set up the connection string
      sqlServerConnString &lt;- &quot;Driver=SQL Server;server=.;
                              database=testParallel;
                              uid=&lt;username&gt;;pwd=&lt;userpwd&gt;&quot;
      
      if(useContext == 1) {
        sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                                numTasks = tasks)
        # set the compute context to be the sql context
        rxSetComputeContext(sqlCtx)
      }
                      
      myModel &lt;- rxLinMod(y ~ rand1 + rand2 + rand3 + rand4 + rand5, 
                         data=InputDataSet)

      OutputDataSet &lt;- data.frame(nRows=myModel$nValidObs);'
    , @input_data_1 = N'SELECT y, rand1, rand2, rand3, rand4, rand5 
                        FROM dbo.tb_Rand_50M'
    , @params = N'@tasks int, @useContext bit'
    , @tasks = @numTasks
    , @useContext = @isCtx    
WITH RESULT SETS ((NumberRows int NOT NULL));
SELECT DATEDIFF(ms, @start, SYSUTCDATETIME())
GO
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Pushing the Data</em></p>

<p>In <em>Code Snippet 4</em> we see how we push the data through the <code>@input_data_1</code> straight to the <code>rxLinMod</code> call via <code>InputDataSet</code>. The code here does not look any different than from most of the other code used in many of my blog posts. When I execute it in the local context (<code>@isCtx bit = 0)</code> however:</p>

<p><img src="/images/posts/sql_ml_services_compctx_II_push_error.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Error Pushing Data</em></p>

<p>Oh, it looks like we try to push too much data as we see, highlighted in <em>Figure 2</em>, a memory issue. Ok, but this is what the SQLCC is all about - efficiently handling large volumes of data, so let us execute the same code but in the SQLCC (<code>@isCtx bit = 1</code>):</p>

<p><img src="/images/posts/sql_ml_services_compII_input_data_ctx_error.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Push and SQLCC Error</em></p>

<p>Ouch, it seems that to use SQLCC we need to pull data through <code>RxSqlServerData</code>. Never mind, I still want to push large volumes of data, so I change <code>@input_data_1</code> to do a <code>SELECT TOP(30000000) ...</code> (30 million) from the table instead. When I push my 30 million rows in the local context the execution time is around  17 seconds. What are the timings if we execute the code in <em>Code Snippet 3</em> with a <code>TOP (30000000)</code> both in the local context as well as SQLCC and compare execution times:</p>

<ul>
<li>Local context push (<em>Code Snippet 4</em> and <code>@isCtx = 0</code>): ~ 17 seconds.</li>
<li>Local context pull (<em>Code Snippet 3</em> and <code>@isCtx = 0</code>): ~ 23 seconds.</li>
<li>SQLCC pull (<em>Code Snippet 3</em> and <code>@isCtx = 1</code>): ~ 15 seconds.</li>
</ul>

<p>That was interesting, the timings between pushing the data in the local context are almost the same as pulling the data in SQLCC, and the push in the local context is much faster than the pull in the same context. What gives?</p>

<p>All we have done so far points to that the difference in performance comes from loading the data, so the question is what the difference is when loading it from the local context compared to the SQLCC, and is SQLCC always faster. Let us start with the last question first; is SQLCC always faster?</p>

<p>To test this change the <code>TOP</code> clause to <code>TOP(50)</code> and execute <em>Code Snippet 4</em> (pushing the data) and <em>Code Snippet 3</em> pulling the data both in the local context as well as SQLCC and take note of the timings:</p>

<ul>
<li>Local context push (<em>Code Snippet 4</em> and <code>@isCtx = 0</code>): ~ 200 ms.</li>
<li>Local context pull (<em>Code Snippet 3</em> and <code>@isCtx = 0</code>): ~ 260 ms.</li>
<li>SQLCC pull (<em>Code Snippet 3</em> and <code>@isCtx = 1</code>): ~ 1.6 seconds.</li>
</ul>

<p>That was quite a difference and now, all of a sudden, SQLCC is slowest! Why is that? Let us use <em>Process Monitor</em> to try to figure out why this is the case. However, before we do that let us recap a little bit about the internal workings when we execute <em>SPEES</em>.</p>

<h4 id="internals">Internals</h4>

<ul>
<li>The host for an external engine is <code>BxlServer.exe</code>.</li>
<li>When we execute <em>SPEES</em> the SqlSatellite (loaded by the BxlServer) connects to SQL Server over a TCP connection.</li>
<li>Data is sent over the TCP connection from and to SQL Server.</li>
<li>The data sent among other things authentication data, script data (the actual external script) as well as the dataset.</li>
</ul>

<p>The figure below illustrates connections and so forth in a &ldquo;simple&rdquo; case where we push data to the SqlSatellite in the local context:</p>

<p><img src="/images/posts/sql_r_services_ext_scriptIII_single_process.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Process Flow</em></p>

<p>In <em>Figure 4</em> we see what happens when we execute <code>sp_execute_external_script</code> and the numbers in the figure stands for:</p>

<ol>
<li>We call <code>sp_execute_external_script</code> and SQL Server calls into the launchpad service.</li>
<li>The launchpad service creates RTerm processes which in turn creates BxlServer processes. One process becomes the executing process.</li>
<li>A TCP connection from the SqlSatellite in the executing process gets established.</li>
<li>SQL server sends input data to the SqlSatellite.</li>
<li>The <code>BxlServer.exe</code> does the processing.</li>
<li>SQL Server receives data back from the SqlSatellite.</li>
</ol>

<p>The <a href="/series/sql_server_2k16_r_services">SQL Server R Services</a> series covered in &ldquo;excruciating&rdquo; details what data SQL Server sends to the BxlServer. If you want to read up on it I suggest <strong>Internals</strong> <a href="/2017/08/microsoft-sql-server-r-services---internals-x/">X</a>, <a href="/2017/10/microsoft-sql-server-r-services---internals-xi/">XI</a>, <a href="/2017/10/microsoft-sql-server-r-services---internals-xii/">XII</a>, <a href="/2017/11/microsoft-sql-server-r-services---internals-xiv/">XIV</a> and <a href="/2017/12/microsoft-sql-server-r-services---internals-xv/">XV</a>.</p>

<h4 id="investigation-using-performance-monitor">Investigation using Performance Monitor</h4>

<p>To see what happens when we execute our three scenarios (local push, local pull, SQLCC pull) we set up some <em>Process Monitor</em> event filters to capture TCP traffic from SQL Server to the SqlSatellite, where <code>BxlServer.exe</code> is &ldquo;hosting&rdquo; the SqlSatellite. The filters we set up are for &ldquo;Process Name&rdquo; and &ldquo;Operation&rdquo;. We want the process to be <code>BxlServer.exe</code> and the operation &ldquo;TCP Receive&rdquo;.</p>

<p>So, run <em>Process Monitor</em> as admin. To set the filter; under the <em>Filter</em> menu click the Filter menu item, and you see the &ldquo;Process Monitor Filter&rdquo; dialog. To create the filter we enter the conditions we want to match:</p>

<ul>
<li>The <em>Process Name</em> (from the first drop down) should be  <em>is</em> (from the second drop-down): <code>bxlserver.exe</code>.</li>
<li><em>Operation</em> (first drop-down) <em>is</em> (second dropdown): &ldquo;TCP Receive&rdquo;</li>
</ul>

<p>You add and include the conditions included and added, and at this stage, the filter dialog looks something like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_II_procmon_filter.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Filters BxlServer</em></p>

<p>What the filter says is that any &ldquo;TCP Receive&rdquo; events for <code>bxlserver.exe</code> should be monitored and displayed. When you have clicked &ldquo;OK&rdquo; out of the dialog box, we are ready to test this by executing the code for local context push (<em>Code Snippet 4</em>), local context pull (<em>Code Snippet 3</em> and <code>@isCtx = 0</code>) and SQLCC pull (<em>Code Snippet 3</em> and <code>@isCtx = 1</code>). When executing we look at the <em>Process Monitor</em> output, and the output for the local push is like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_II_procmon_push2.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>TCP Receive Local Context Push</em></p>

<p>We see in <em>Figure 6</em> that the output looks quite &ldquo;tidy&rdquo; and by looking at the <code>Path</code> column see a connection between SQL Server and the SqlSatellite on port 13273 (<code>win10-dev:13273</code>). Furthermore, we see:</p>

<ul>
<li>There is one <code>BxlServer.exe</code> process with a process id of 17260.</li>
<li>The data the BxlServer receives are what we covered in the <a href="/series/sql_server_2k16_r_services">SQL Server R Services</a> series.</li>
<li>The 50 rows we pushed to the BxlServer is the outlined (in blue) row with a length of 1392.</li>
</ul>

<p>Ok, so onto the local context pull:</p>

<p><img src="/images/posts/sql_ml_services_compctx_II_procmon_pull_loc_ctx2.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>TCP Receive Local Context Pull</em></p>

<p>Looking at <em>Figure 7</em> we see that there is quite a difference between when we push the data to the SqlSatellite. First, we see (highlighted in red) the usual connection between SQL Server and the SqlSatellite and how SQL Server sends data (authentication and script) to the SqlSatellite. Then, however, we see data going from SQL Server from a &ldquo;strange&rdquo; port: <code>ms-sql-s</code>. That &ldquo;name&rdquo; is <a href="http://www.t1shopper.com/tools/port-number/1433/"><em>IANA</em>&rsquo;s</a> (Internet Assigned Numbers Authority) definition for SQL Server&rsquo;s port 1433. As we know, port 1433 is the default port SQL uses for connections and retrieval of data. So it looks like that when we use pull, we connect to SQL Server over the default port and retrieves the data that way. Thinking about it, it makes sense as the connection is an ODBC connection. All the packets received by the SqlSatellite are the regular ODBC data packets. The actual 50 rows of data are in the packet outlined in blue with a length of 1358. As we use ODBC the protocol used to send the data is TDS.</p>

<p>Oh, TDS - that is probably a reason why the local pull is slower than local push, as the local push uses the <a href="/2017/11/microsoft-sql-server-r-services---internals-xiv/"><strong>Binary eXchange Language</strong></a> protocol (BXL) which is very efficient for transferring data. Another reason why the local pull is slower than the local push, even with small datasets, is that for local pull there is much more happening, as we see in <em>Figure 7</em>.</p>

<p>Right, then what about SQLCC pull:</p>

<p><img src="/images/posts/sql_ml_services_compctx_II_procmon_pull_sqlcc2.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>TCP Receive SQLCC Pull</em></p>

<p>Oh my, that is a lot! As in <em>Figure 7</em> the sections outlined in red is the connection between SQL Server and the SqlSatellite, and in blue it is the &ldquo;ODBC&rdquo; connection. What is noticeable is that there are multiple sections interleaved, as well that there are multiple <code>BxlServer.exe</code> processes involved (process id&rsquo;s 2108, 13360 and 15340). Well, maybe that is not such a surprise as we spoke about it in <a href="/2018/05/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a>.</p>

<p>What is more interesting though is that we receive the dataset both via the ODBC connection outlined in blue (length 1358), as well as the way we do it in the local push context, outlined in purple (length 1392)! That means that SQL sends data using both the TDS protocol as well as the BXL protocol.</p>

<p>By seeing the amount of &ldquo;stuff&rdquo; happening in <em>Figure 8</em> we do realise why the SQLCC pull is not as efficient as local push and local pull (1.6 s vs ~200 ms). Having seen all this, we probably ask ourselves why the SQLCC pull was a lot faster (~15 s) than local pull (~23 s) for a big dataset and somewhat faster than the local push (~17 s)?</p>

<p>Let us execute the code in <em>Code Snippet 3</em> and <em>Code Snippet 4</em> with <code>TOP (30000000)</code> (30 million) and see what <em>Process Monitor</em> tells us. For local push, we see many packets with a size of 65495 which is the maximum size for BXL data package. When we execute the local pull, we see many TDS packets with a size of 4096 followed by many packages with sizes ranging from ~70,000 up to 2.5 Mb. For me, it looks like the local pull is nowhere as efficient as the local push. Finally, the SQLCC pull shows the same behaviour as local pull with many TDS 4096 packages. However, after the TDS packages follows BXL packages where most have the maximum size of 65495.</p>

<blockquote>
<p><strong>NOTE:</strong> I do not know why, in the case of SQLCC, data is first loaded via TDS and then BXL. I also do not know why in the case of local pull we see multiple 4096 packages followed by packages with an arbitrary big size. I see if I can find answers to this, in which case update this post (or write a new).</p>
</blockquote>

<h2 id="summary">Summary</h2>

<p>This post set out to try to find out why SQLCC performs better than local context. I believe we found why this is the reason but not necessarily how it works.</p>

<p>What did we see:</p>

<ul>
<li>Local push performs really, really well up until it does not :). It performs well up until you hit memory restrictions.</li>
<li>Some of the memory issues can be alleviated by using the <code>@r_rowsPerRead</code> parameter (not shown in this post).</li>
<li>When pushing the data (<code>@input_data_1</code>) we cannot use SQLCC.</li>
<li>Both local pull as well as SQLCC uses ODBC connections, and the data transfer protocol is TDS.</li>
<li>When using SQLCC the BXL protocol is also used.</li>
<li>By the use of BXL we get very efficient processing of data, and that is the reasons we see good performance.</li>
</ul>

<p>After writing this post, I have quite a few questions which I will try to answer in a future post.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
  </entry>
</feed>