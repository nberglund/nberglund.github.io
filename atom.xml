<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="http://nielsberglund.com/atom.xml" rel="self"/>
  <link href="http://nielsberglund.com"/>
  <updated>2018-12-08T19:52:11+02:00</updated>
  <id>http://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Extensibility Framework &amp; Java - Passing Data]]></title>
    <link href="http://nielsberglund.com/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/" rel="alternate" type="text/html"/>
    <updated>2018-12-08T19:52:11+02:00</updated>
    <id>http://nielsberglund.com/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/</id>
    <content type="html"><![CDATA[<p>This post is the second post about SQL Server 2019 Extensibility Framework and the Java language extensions. In the first post, <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>, we looked at how to install and enable the Java language extensions, and we also wrote some pretty basic Java code to ensure it all worked.</p>

<p>In this post we look at how we can pass data back and forth between SQL Server and Java.</p>

<p></p>

<h2 id="recap">Recap</h2>

<p>Let us just look back and refresh our memories about what we discussed in the previous <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">post</a>.</p>

<ul>
<li>On Windows you install and enable the Java language extensions when enabling the <em>Machine Learning Services</em> feature during setup or when adding features.</li>
<li>For CTP 2.1 the supported Java version on both Linux and Windows is 1.8.x.</li>
<li>Java does not get automatically installed on Windows when you install and enable the Java language extensions (on Linux it does). So unless 1.8.x is already installed you need to install it.</li>
<li>On Windows the <code>Path</code> environment variable needs to extend to the directory where the <code>jvm.dll</code> is located.</li>
<li>It is good practice to create an environment variable called <code>CLASSPATH</code> which indicates where your compiled Java code exists.</li>
<li>On Windows you need to give <code>Read</code> permissions on the <code>CLASSPATH</code> directory to <code>ALL APPLICATION PACKAGES</code>.</li>
<li>On Linux you need to give <code>Read</code> and <code>Execute</code> permissions on the on the <code>CLASSPATH</code> directory to the <code>mssql_satellite</code> user.</li>
</ul>

<p>So, when the Java extensions are enabled we can write and execute Java code:</p>

<ul>
<li>We execute Java code from SQL Server using <code>sp_execute_external_script</code>.</li>
<li>We define the class and method we call in the <code>@script</code> parameter.</li>
<li>All methods called from SQL Server need to be <code>public static</code>.</li>
<li>The static methods can not have a return type, they need to be <code>public static void</code>.</li>
<li>The methods must be parameterless.</li>
<li>No support for output parameters.</li>
<li>The code needs to contain &ldquo;magic&rdquo; <code>public static</code> class members.</li>
<li>One such member is <code>numberOfOutputCols</code> which is always required. It has to be declared as: <code>public static short numberOfOutputCols;</code>.</li>
<li>Even though the methods must be parameterless we can pass in parameters as class members.</li>
<li>We refer to the class member parameters in SQL with the same name as the Java names but appended with <code>@</code>.</li>
<li>We define the SQL parameters in <code>sp_execute_external_script</code>&rsquo;s <code>@params</code> parameter.</li>
<li>We add the SQL parameters as named parameters in <code>sp_execute_external_script</code>.</li>
</ul>

<p>Based on the points above we saw some Java code looking like so:</p>

<pre><code class="language-java">public class JavaTest1 {
  public static short numberOfOutputCols;
  public static int x;
  public static int y;

  public static void adder() {
    System.out.printf(&quot;The result of adding %d and %d = %d&quot;, x, y, x + y);   
  }

  public static void helloWorld() {
    System.out.print(&quot;Hello World from SQL Java&quot;);
  }

}
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Basic Java Code</em></p>

<p>In <em>Code Snippet 1</em> we see the required <code>numberOfOutputCols</code> class member, together with class members for the two parameters used in the <code>adder</code> method. We also see that we do not pass back the result of the <code>adder</code> method as there is no support for output parameters (in this post we cover how we can pass data back to SQL).</p>

<p>We call the code in <em>Code Snippet 1</em> from SQL like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'JavaTest1.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Execute Java from SQL</em></p>

<p>To call the code in <em>Code Snippet 1</em> I execute what we see in <em>Code Snippet 2</em>:</p>

<ul>
<li>I indicate that I want to call into Java: <code>@language = N'Java'</code>.</li>
<li>The method I want to call is set in the <code>@script</code> parameter.</li>
<li>The Java class members <code>x</code> and <code>y</code> is defined in the <code>@params</code> parameter as SQL parameters <code>@x</code>, and <code>@y</code>.</li>
<li>I have added the <code>@x</code>, and <code>@y</code> parameters as named parameters and assigned them some values.</li>
</ul>

<p>So, that was what we discussed in the previous post. Towards the end of this post, we revisit the code above.</p>

<h2 id="demo-data">Demo Data</h2>

<p>In today&rsquo;s post, we use some data from the database, so let us set up the necessary database, tables, and load data into the tables:</p>

<pre><code class="language-sql">USE master;
GO
SET NOCOUNT ON;
GO
DROP DATABASE IF EXISTS JavaTest;
GO
CREATE DATABASE JavaTest;
GO
USE JavaTest;
GO

DROP TABLE IF EXISTS dbo.tb_Rand100
CREATE TABLE dbo.tb_Rand100(RowID int identity primary key, y int, 
                          rand1 int, rand2 int);

INSERT INTO dbo.tb_Rand100(y, rand1, rand2)
SELECT TOP(100) CAST(ABS(CHECKSUM(NEWID())) % 14 AS int) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS int)
  , CAST(ABS(CHECKSUM(NEWID())) % 25 AS int)
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create Database Objects</em></p>

<p>We see from <em>Code Snippet 3</em> how we:</p>

<ul>
<li>Create a database: <code>JavaTest</code>.</li>
<li>Create a table: <code>dbo.tb_Rand100</code>.</li>
<li>Insert some data into the table.</li>
</ul>

<p>The data we insert is entirely random, but it gives us something to &ldquo;play around&rdquo; with. Now, when we have a database and some data let us get started.</p>

<h2 id="data-passing-in-r-python">Data Passing in R/Python</h2>

<p>We start by looking at how we pass data when we use R/Python in <strong>SQL Server Machine Learning Services</strong>. In <a href="/2018/03/07/microsoft-sql-server-r-services---sp_execute_external_script---i/">Microsoft SQL Server R Services: sp_execute_external_script - I</a> I discussed, among other things, how we use named parameters to refer to data pushed into the external engine from SQL Server as well as data returned to SQL Server.</p>

<p>The parameter&rsquo;s default names are <code>InputDataSet</code> and <code>OutputDataSet</code>, and a simple example looks like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
      @language = N'Python'
    , @script = N'OutputDataSet = InputDataSet'
    , @input_data_1 = N'SELECT RowID, y, rand1 
                        FROM dbo.tb_Rand100';  
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Passing Data In and Out</em></p>

<p>The Python code in <em>Code Snippet 4</em> pushes data into the Python engine via <code>InputDataSet</code>, and <code>OutputDataSet</code> echoes the data back to SQL Server. Why this works is because R/Python are aware of these parameters, thanks to &ldquo;helper&rdquo; components, (<code>SqlSatellite.dll</code> and friends), shipped together and &ldquo;baked&rdquo; into R/Python. In essence, there is a very tight integration with the external runtime which makes this possible.</p>

<h2 id="data-passing-in-java-extensions">Data Passing in Java Extensions</h2>

<p>In Java, there are also helper components, (a topic for future posts), but the integration is not as tight, so when we want to pass data into and out of Java we need to code somewhat more explicit to make data passing possible.</p>

<p>In our Java code, we need to represent the data passed in and out as class member column arrays. You define in your classes, one array per column passed in, and one array per column returned. These column arrays are some of the &ldquo;magic&rdquo; members I mentioned above, and they are the equivalent to <code>InputDataSet</code>, and <code>OutputDataSet</code>.</p>

<p>The components that are part of the Java extension need to know about these members as the components either populate them when pushing data into Java or read from them when returning data from Java. The way that the components know about the members is based on a naming standard.</p>

<h4 id="pushing-data-into-java">Pushing Data into Java</h4>

<p>When pushing data into Java and using that data, we need to define two <code>public static</code> class members:</p>

<ul>
<li><strong><code>inputDataColN</code></strong>: array variable representing the input columns, where <em>N</em> is the column number (1 based).</li>
<li><strong><code>inputNullMap</code></strong>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
</ul>

<p>The calling components populate the variables for input data automatically, and you just need to initialize the arrays with a size greater than 0. Let us look at an example based on the <code>SELECT</code> statement in <em>Code Snippet 4</em>, where we want to push data into Java:</p>

<pre><code class="language-java">public class DataPassing {
  static public int[] inputDataCol1 = new int[1];
  static public int[] inputDataCol2 = new int[1];
  static public int[] inputDataCol3 = new int[1];

  static public boolean[][] inputNullMap = new boolean[1][1];

  static public short numberOfOutputCols;

  public static void foo() {
    
    for(int x = 0; x &lt; inputDataCol1.length; x++) {
      System.out.printf(&quot;Row %d:\t\t%d\t %d\t %d\n&quot;, x+1, 
                          inputDataCol1[x], 
                          inputDataCol2[x], 
                          inputDataCol3[x]);
    }
  }
}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Input Data</em></p>

<p>In <em>Code Snippet 5</em> we see how we:</p>

<ul>
<li>Declare and initialize three <code>int</code> arrays representing the three columns we expect from the <code>SELECT</code> statement.</li>
<li>Declare and initialize a <code>boolean</code> array for null mapping.</li>
<li>Declare the <code>numberOfOutputCols</code> variable which always is required.</li>
</ul>

<p>In the <code>foo</code> method we loop the three input arrays and print them out. We see that we do not have to populate the arrays as something else, (some component), does that for us. To make the code callable from SQL Server we:</p>

<ul>
<li>Copy the code in <em>Code Snippet 5</em> to a file: <code>DataPassing.java</code>.</li>
<li>We compile it like so: <code>javac DataPassing.java</code>, which results in a <code>.class</code> file: <code>DataPassing.class</code></li>
<li>We copy the <code>.class</code> file to the location of <code>CLASSPATH</code>.</li>
</ul>

<p>With the <code>.class</code> file in the <code>CLASSPATH</code> location we call it from SQL Server:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'DataPassing.foo'
, @input_data_1 = N'SELECT TOP(10) RowID, y, rand1 
                    FROM dbo.tb_Rand100';
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Execute &amp; Push Data to Java</em></p>

<p>The code in <em>Code Snippet 6</em> follows what we discussed in <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>, and the summary section above. When we execute it we see this:</p>

<p><img src="/images/posts/sql_2k19_java_intro_exec_input.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Result of Exec</em></p>

<p>In <em>Figure 1</em> we see the result of the <code>printf</code> statement in <em>Code Snippet 5</em>. Printing data to the console is probably not what we want to do in a &ldquo;proper&rdquo; application, so let us see how we can return data from Java back to SQL.</p>

<h4 id="returning-data-from-java">Returning Data from Java</h4>

<p>When returning data from our Java method(s), we use column array class members, similar to the ones we use when passing data into Java:</p>

<ul>
<li><strong><code>outputDataColN</code></strong>: array variable representing the output columns, where <em>N</em> is the column number (1 based).</li>
<li><strong><code>numberofOutputCols</code></strong>: we discussed this variable in the summary section above. It represents the number of columns returned from Java, and it is always required - regardless if you return columns or not.</li>
<li><strong><code>outputNullMap</code></strong>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
</ul>

<p>There are a couple of differences between the variables used for input data and the above output variables:</p>

<ul>
<li>You need to populate the variables yourself.</li>
<li>You initialize the variables right before you use them.</li>
</ul>

<p>So, if we wanted to have a method, <code>bar</code>, &ldquo;echoing&rdquo; back the input dataset, similar to what we see in *Code Snippet, 4 the code looks like so:</p>

<pre><code class="language-java">public class DataPassing {
  //input data variables as per above
  ...

  //output variables
  static public int[] outputDataCol1;
  static public int[] outputDataCol2;
  static public int[] outputDataCol3;
  static public boolean[][] outputNullMap;

  static public short numberOfOutputCols;

  public static void bar() {
    
    int numRows = inputDataCol1.length;
    numberOfOutputCols = 3;

    outputDataCol1 = new int[numRows];
    outputDataCol2 = new int[numRows];
    outputDataCol3 = new int[numRows];

    for(int x = 0; x &lt; numRows; x++) {
      outputDataCol1[x] = inputDataCol1[x];
      outputDataCol2[x] = inputDataCol2[x];
      outputDataCol3[x] = inputDataCol2[x];
    }

    outputNullMap = new boolean[numberOfOutputCols][numRows];
  }

 public static void foo() {...}

</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Output Data</em></p>

<p>In <em>Code Snippet 7</em> we see how I:</p>

<ul>
<li>Declare the output variables as class members.</li>
<li>Set the <code>numberOfOutputCols</code> variable. I do not get any data back without setting this variable,</li>
<li>Instantiate output variables in the method, based on the number of columns and number of rows.</li>
<li>Copy the input dataset into the output variables.</li>
<li>Instantiate the null map variable.</li>
</ul>

<p>After I have compiled the code and moved the <code>.class</code> file to the <code>CLASSPATH</code> location, I execute as in <em>Code Snippet 6</em>, but with one difference: the <code>@script</code> parameter now points to the <code>bar</code> method: <code>@script = N'DataPassing.bar'</code>:</p>

<p><img src="/images/posts/sql_2k19_java_intro_exec_output.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Output Dataset</em></p>

<p>We see in <em>Figure 2</em> how the data is returned to SQL Server and presented as a result set. If you compare what you see in <em>Figure 2</em> with what you see in <em>Figure 1</em>, you see the data is identical.</p>

<p>Cool, we have now seen how we pass datasets in and out of Java and how we use the different class arrays. Actually, that is not entirely true; we have not discussed the null map variables at all, other than saying what they are and that they need to be instantiated. We answer the questions about how and why to use them in the next post.</p>

<h2 id="input-output-parameters">Input &amp; Output Parameters</h2>

<p>In the previous post, (and in the recap above), we said that Java methods we use from SQL Server cannot have parameters and they must be <code>void</code>. In the recap section, we saw how we tried to work around those restrictions by having class member variables, etc.</p>

<p>With what we now know, we can re-write the Java code to something like so:</p>

<pre><code class="language-java">public class Calculator {
  static public int[] inputDataCol1 = new int[1];
  static public int[] inputDataCol2 = new int[1];
  static public boolean[][] inputNullMap = new boolean[1][1];

  static public int[] outputDataCol1;
  static public boolean[][] outputNullMap;

  static public short numberOfOutputCols;

  public static void adder() {
    
    int x = inputDataCol1[0];
    int y = inputDataCol2[0];

    numberOfOutputCols = 1;

    outputDataCol1 = new int[1];
    outputDataCol1[0] = x + y;

    outputNullMap = new boolean[1][1];
  }
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>New Adder</em></p>

<p>In this new code we see in <em>Code Snippet 8</em> we:</p>

<ul>
<li>Receive a one row, two column data set in the class.</li>
<li>Parse out the two columns, as <code>x</code>, and <code>y</code>, from the data coming in.</li>
<li>Return a one row, one column dataset back to SQL Server with the result of <code>x+y</code>.</li>
</ul>

<p>The SQL code to call into this Java code looks like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'Calculator.adder'
, @input_data_1 = N'SELECT 21, 21'; 
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Execute the Adder Method</em></p>

<p>The result returned when we execute the code in <em>Code Snippet 9</em> is as follows:</p>

<p><img src="/images/posts/ql_2k19_java_intro_exec_output_param.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Output Parameter</em></p>

<p>In this section, we saw how we can work around, (somewhat), the restrictions that exist at the moment when calling Java code from SQL Server.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we looked at how we pass data back and forth between SQL Server and Java. In the Java extensions we do not have the <code>InputDataSet</code>, and <code>OutputDataSet</code> variables, so we need to define class member arrays for the columns we send in and pass back out, as well as a variable indicating the number of columns we return:</p>

<ul>
<li>`<strong>inputDataCol*N</strong>*: array variable representing the input columns, where <em>N</em> is the column number (1 based).</li>
<li><code>**outputDataCol*N***</code>: array variable representing the output columns, where <em>N</em> is the column number (1 based).</li>
<li><code>**numberofOutputCols**</code>: it represents the number of columns returned from Java, and it is always required - regardless if you return columns or not.</li>
</ul>

<p>In addition to these variables we need two variables mapping null values:</p>

<ul>
<li><code>**inputNullMap**</code>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
<li><code>**outputNullMap**</code>: two dimensional <code>boolean</code> array variable, indicating whether a column value is <code>null</code>.</li>
</ul>

<p>All the <code>input*xxx*</code> variables get populated automatically, whereas you need to populate the <code>output*xxx*</code> variables in the code.</p>

<p>In the next post we discuss more about the null map variables.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 48]]></title>
    <link href="http://nielsberglund.com/2018/12/02/interesting-stuff---week-48/" rel="alternate" type="text/html"/>
    <updated>2018-12-02T14:43:12+02:00</updated>
    <id>http://nielsberglund.com/2018/12/02/interesting-stuff---week-48/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/considering-azure-functions-for-a-serverless-data-streaming-scenario/">Considering Azure Functions for a serverless data streaming scenario</a>. A blog post which describes a solution of using Azure server-less to create a data pipeline to detect fraudulent transactions. Quite a lot of exciting technologies/solutions!</li>
<li><a href="https://data-artisans.com/blog/getting-started-data-artisans-platform-azure-kubernetes-service">Getting Started with data Artisans Platform on Azure Kubernetes Service</a>. The <a href="https://data-artisans.com/download">dA Platform</a> is a production-ready stream processing infrastructure that includes open-source Apache Flink, and it is purpose-built for the stateful stream processing architecture. This blog post discusses how to get the dA Platform up and running on Azure Kubernetes Service.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://data-artisans.com/blog/apache-flink-1-7-0-release">What’s new in the latest Apache Flink 1.7.0 release</a>. As per the title. This blog post mentions some new functionality in the 1.7.0 release of Apache Flink. The new <code>MATCH_RECOGNIZE</code> functionality looks very juicy!</li>
<li><a href="https://databricks.com/blog/2018/11/30/apache-avro-as-a-built-in-data-source-in-apache-spark-2-4.html">Apache Avro as a Built-in Data Source in Apache Spark 2.4</a>. Starting from the Apache Spark 2.4 release, Spark provides built-in support for reading and writing Avro data. This blog post examines the functionality in detail.</li>
<li><a href="https://www.confluent.io/blog/3-ways-prepare-disaster-recovery-multi-datacenter-apache-kafka-deployments">3 Ways to Prepare for Disaster Recovery in Multi-Datacenter Apache Kafka Deployments</a>. What it says in the title - how we can prepare for disaster recovery in Kafka environments.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>. In the roundup for last <a href="/2018/11/25/interesting-stuff---week-47/">week</a> I wrote how I was working on a blog post about the new Java functionality in SQL Server 2019. This is it. In this post I take a look at how to install and enable the Java extensions. The post finishes with some very simple Java code which I execute from SQL Server. Do not be surprised if more posts comes shortly about Java.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Extensibility Framework &amp; Java - Hello World]]></title>
    <link href="http://nielsberglund.com/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/" rel="alternate" type="text/html"/>
    <updated>2018-12-02T09:36:34+02:00</updated>
    <id>http://nielsberglund.com/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/</id>
    <content type="html"><![CDATA[<p>The SQL Server 2016 release introduced the <strong>SQL Server Extensibility Framework</strong> (EF), which gives us the ability to, from inside SQL Server, execute code in an external language runtime environment. SQL Server 2016 supports R as external runtime, and Microsoft added Python to supported runtimes in the SQL Server 2017 release. The important part about the EF is that the runtime is outside of the core database engine, but we call it from inside SQL Server via the stored procedure <code>sp_execute_external_script</code>. We can push data from SQL Server queries to the external runtime, and consume data, (resultsets, output parameters) from the external runtime back in SQL Server.</p>

<blockquote>
<p><strong>NOTE:</strong> You can read more about the actual implementation of the external runtimes and <code>sp_execute_external_script</code> in my <a href="/sql_server_2k16_r_services/">SQL Server R Services Series</a> posts.</p>
</blockquote>

<p>In SQL Server 2019 Microsoft added the ability to execute custom Java code along the same lines we execute R and Python, and this blog post intends to give an introduction of how to install and enable the Java extension, as well as execute some very basic Java code. In future posts, I drill down how to pass data back and forth between SQL Server and Java.</p>

<p></p>

<p>There may very well be future posts discussing how the internals differ between Java and R/Python, but I want to talk about that a little bit in this post as well, as it has an impact on how we write and call Java code.</p>

<h2 id="r-python-basics">R/Python Basics</h2>

<p>In my <a href="/sql_server_2k16_r_services/">SQL Server R Services</a> series  talked about the components which make up <strong>SQL Server Machine Learning Services</strong>, and we saw how the flow when we execute an external script, looks something like so:</p>

<p><img src="/images/posts/sql_2k19_java_intro_flow1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Components &amp; Flow</em></p>

<p>We see in <em>Figure 1</em> what happens when we execute an R or Python script:</p>

<ul>
<li>Inside SQL Server we execute <code>sp_execute_external_script</code> and we send in the language as a parameter.</li>
<li>A connection happens to the <em>Launchpad</em> service and the service calls into an R or Python launcher (based on the <code>@language</code> parameter).</li>
<li>The launcher launches either the R or Python process.</li>
<li>Through a proprietary link dll, the <em>BxlServer</em> process gets launched.</li>
<li>In the process is the <code>SqlSatellite.dll</code> which handles data transfers.</li>
</ul>

<p>In the case of either R or Python, the execution of the script happens in the <em>BxlServer</em> process. A couple of things to keep in mind for later when we look at what we do when executing Java code:</p>

<ul>
<li>R/Python gets installed when we install SQL Server and choose In-Database machine learning. As R/Python is installed together with SQL Server, SQL gets implicit permissions to the R/Python exe&rsquo;s.</li>
<li>Even though we install open source versions of R/Python, they come together with proprietary dll&rsquo;s and exe&rsquo;s.</li>
<li>When we execute external scripts, we execute just that - scripts, and we pass in the code in the <code>sp_execute_external_script</code> call.</li>
</ul>

<p>Before we look at a how Java compares to the above, let us look at how we install and enable the Java extension.</p>

<h2 id="installing-java-extensions">Installing Java Extensions</h2>

<p>The way we install and enable Java on SQL Server differs depending on if you run SQL Server on Windows or Linux.</p>

<h4 id="windows">Windows</h4>

<p>You enable the ability to execute Java code, the same way as you do with R/Python. At install time, or when you add features, you check the <em>Machine Learning Services (In-Database)</em> checkbox:</p>

<p><img src="/images/posts/sql_2k19_java_intro_install1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Enable In-Database Machine Learning</em></p>

<p>In <em>Figure 2</em> we see how I have in the <em>Feature Selection</em> dialog checked the Machine Learning Services checkbox only. To enable Java, I do not need to choose either R or Python, the &ldquo;top-level&rdquo; <em>Machine Learning Services</em> is enough. Checking that, ensures that the required components, (<em>Launchpad</em>, and so on), gets installed.</p>

<blockquote>
<p><strong>NOTE:</strong> A future post covers the various components.</p>
</blockquote>

<p>When you install/add <em>Machine Learning Services</em> on Windows, you need to be aware that Java not gets installed. See below for more about that.</p>

<h4 id="linux">Linux</h4>

<p>When we install SQL Server on Linux, we do not have a &ldquo;nice&rdquo; UI, but we install via the shell. Similarly, when you want to install <em>Machine Learning Services</em> you do it via the shell. So to install only the Java extension you:</p>

<pre><code class="language-bash">sudo apt-get install mssql-server-extensibility-java
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Install Java Extension on Linux</em></p>

<p>In <em>Code Snippet 1</em> we see how I use the Ubuntu package manager to install the Java extension on an already installed <em>SQL Server on Linux</em> instance. Of course, you can install the instance as well as the Java extension in one go:</p>

<pre><code class="language-bash">sudo apt-get install mssql-server mssql-server-extensibility-java
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Install SQL Server and Java Extension Together</em></p>

<p>A difference between the install on Linux vs the Windows install is that on Linux Java 1.8.x gets installed if it is not already on the box.</p>

<h4 id="enabling-external-scripts">Enabling External Scripts</h4>

<p>On both Windows and Java we need to enable the execution of external scripts after installation:</p>

<pre><code class="language-sql">EXEC sp_configure 'external scripts enabled', 1
RECONFIGURE WITH OVERRIDE
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Enable External Scripts</em></p>

<p>You may remember that when we executed the code in <em>Code Snippet 3</em> in SQL Server 2016, and 2017 we had to restart the SQL Server instance afterwards to make the change &ldquo;stick&rdquo;. In SQL Server 2019 that is no longer necessary, (at least from CTP 2.1 and later), you just run the code and the setting changes!</p>

<h2 id="windows-and-java">Windows and Java</h2>

<p>When we run <code>mssql-server-extensibility-java</code> as in <em>Code Snippet 1</em> (or <em>Code Snippet 2</em>), I mentioned that Java 1.8.x gets installed if it is not already on the box. The installation also takes care of permissions as well as paths to the JVM. On Windows, this is not the case, so in this section, I talk a little bit about what you need to do if you are on Windows. More specifically we discuss Java versions, environment variables, and permissions.</p>

<blockquote>
<p><strong>NOTE:</strong> If you are on Linux have a look below where I mention <code>CLASSPATH.</code></p>
</blockquote>

<h4 id="java-version">Java Version</h4>

<p>As enabling <em>Machine Learning Services</em> does not install Java (on Windows), you need to ensure you already have a compatible version of Java on the box, or that you install one. On Windows, the suggestion for CTP 2.0 was version 1.10.x. However, Oracle no longer supports that Java version, so for CTP 2.1 the recommended version is 1.8.x, which is the same version recommended for SQL Server 2019 on Linux.</p>

<h4 id="environment-variables">Environment Variables</h4>

<p>When you do a typical installation of Java on Windows, you (or the installation program) also set some environment variables, so that applications can find the various Java dependencies:</p>

<ul>
<li>Set the <code>Path</code> environment variable to point to the <code>bin</code> directory of the JDK or JRE installation.</li>
<li>Create a <code>JAVA_HOME</code> environment variable pointing to the JDK/JRE top-level directory.</li>
</ul>

<p>When installing Java 1.8.x to use by <em>Machine Learning Services</em> you do not need the <code>JAVA_HOME</code> variable. However, other applications may need it, so I suggest you set it unless it is already set.</p>

<p>One crucial thing to be aware of is that the <code>Path</code> variable needs to be extended to include the directory where the <code>jvm.dll</code> exists. So, even if the <code>Path</code> includes the JDK/JRE <code>bin</code> directory, you need to edit the path to include the directory where the <code>jvm.dll</code> exists. The easiest way to do it (once again, this is Windows) is to use the UI: <strong>Control Panel-&gt;All Control Panel Items-&gt;System-&gt;Advanced System Setttings-&gt;Environment Variables-&gt;System Variables-&gt;Path-&gt;Edit</strong>:</p>

<p><img src="/images/posts/sql_2k19_java_intro_path.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Set Path</em></p>

<p>We see in <em>Figure 3</em> how I in the <em>Edit environment variable</em> dialog:</p>

<ul>
<li>Click <em>New</em> (outlined in blue).</li>
<li>Add the path in the text box (highlighted in red).</li>
</ul>

<p>I then click <em>Ok</em> in the various screens to exit out of there.</p>

<p>So far in this section, the discussion has been around things related to Java on Windows. The next thing I want to mention is as equally relevant for Windows as for Linux, and it is the Java <code>CLASSPATH</code> environment variable.</p>

<p>Remember what I mentioned above; when we use R or Python in <em>Machine Learning Services</em> we send in the script to execute as a parameter in the call. For Java, we execute against compiled code, and somehow we need to let the external engine know where the code is. That is where the <code>CLASSPATH</code> variable comes in. The <code>CLASSPATH</code> variable is there so that the Java Compiler and Java Runtime can find Java classes referenced in your program. It maintains a list of directories (containing many Java class files) and JAR file (a single-file archive of Java classes).</p>

<p>To create the <code>CLASSPATH</code> variable on Windows I use the UI: <strong>Control Panel-&gt;All Control Panel Items-&gt;System-&gt;Advanced System Setttings-&gt;Environment Variables-&gt;System Variables</strong>, and click <em>New</em> in the <em>Environment Variables</em> dialog:</p>

<p><img src="/images/posts/sql_2k19_java_intro_classpathpath.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Create CLASSPATH Windows</em></p>

<p>In <em>Figure 4</em>, after clicking <em>New</em>:</p>

<ul>
<li>I entered <code>CLASSPATH</code> in the <em>Variable name</em> text box (outlined in blue).</li>
<li>In the <em>Variable value</em> text box, (outlined in red), I entered the actual path where my compiled java code is located.</li>
</ul>

<p>If the <code>CLASSPATH</code> variable already exists, but you want your code in another location than where the variable points to, you can add a new path to the existing. In Windows, you delimit the paths with a semi-colon, and in Linux with a colon.</p>

<p>For completeness let us see how you create/set the <code>CLASSPATH</code> in Linux:</p>

<pre><code class="language-bash">export CLASSPATH=&quot;/path/to/directory:/path/to/directory2:$CLASSPATH&quot;
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Setting CLASSPATH in Linux</em></p>

<p>The code we see in <em>Code Snippet 4</em> adds two new directories to the existing <code>CLASSPATH</code> variable, delimited with colons.</p>

<blockquote>
<p><strong>NOTE:</strong> It is not absolutely necessary to have the <code>CLASSPATH</code> variable set. In a future post we see how we can do without it.</p>
</blockquote>

<h4 id="permissions">Permissions</h4>

<p>The last thing we need to do is to set permissions on the <code>CLASSPATH</code> directory.</p>

<blockquote>
<p><strong>NOTE:</strong> The following is based on what I found using CTP 2.1. This may change in future releases.</p>
</blockquote>

<p>On Windows, right click on the directory(s) in the <code>CLASSPATH</code>, followed by <strong>Properties-&gt;Security</strong>, and you see something like so:</p>

<p><img src="/images/posts/sql_2k19_java_intro_classpathpath_sec1.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Security Tab for CLASSPATH</em></p>

<p>In <em>Figure 5</em> we see how we are in the <em>Security</em> tab, and from here we can <strong>Edit-&gt;Add</strong> permissions, which gives us the ability to add permissions for users/groups:</p>

<p><img src="/images/posts/sql_2k19_java_intro_classpathpath_sec2.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Security Tab for CLASSPATH</em></p>

<p>The group we want to add permissions for is <code>ALL APPLICATION PACKAGES</code>, as we see in <em>Figure 6</em>. Clicking <em>OK</em> allows us to assign the permissions needed:</p>

<p><img src="/images/posts/sql_2k19_java_intro_classpathpath_sec3.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Security Tab for CLASSPATH</em></p>

<p>The required permission needed for <code>ALL APPLICATION PACKAGES</code> is <code>Read</code> as in <em>Figure 7</em>. After we click <em>OK</em> out of the dialogs, we are finally ready to write and execute code on Windows.</p>

<p>On Linux you need to give read and execute permissions on the <code>CLASSPATH</code> directory to the <code>mssql_satellite</code> user.</p>

<h2 id="java-code">Java Code</h2>

<p>There are certain things to think about when we write Java code which we want to call from SQL Server:</p>

<ul>
<li>We have no way, from calling from SQL Server to <code>new</code> &ldquo;up&rdquo; a class to get a reference to the class and call methods on that reference.</li>
<li>All methods called from SQL Server need to be <code>public static</code>. This is similar to SQLCLR. However, static methods can <code>new</code> &ldquo;up&rdquo; a class and call methods on the reference.</li>
<li>The static methods can not have a return type, they need to be <code>public static void</code>.</li>
<li>The methods must be parameterless!</li>
<li>No support for output parameters.</li>
</ul>

<p>The two first points above are probably not that too much of a hindrance, after all, this is the same as with SQLCLR. The last three, however, is more of a pain, and shortly we see how we work around the parameterless restriction.</p>

<p>So, let us write some code, and we start with &ldquo;Hello World&rdquo;:</p>

<pre><code class="language-java">public class JavaTest1{
  
  public static void helloWorld() {
    System.out.print(&quot;Hello World from SQL Java&quot;);
  }
}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Java Hello World - I</em></p>

<p>In <em>Code Snippet 5</em> we see your typical &ldquo;Hello World&rdquo; Java application. Copy the code into a file and name the file <code>JavaTest1.java</code>. Then compile it into a <code>class</code> file:</p>

<pre><code class="language-bash">javac JavaTest1.java
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Compile Java Code</em></p>

<p>After executing the code in <em>Code Snippet 6</em>, copy the <code>class</code> file to the location of your <code>CLASSPATH</code> variable. When the <code>class</code> file is in the <code>CLASSPATH</code> directory, we try and call it from SQL:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
                  @language = N'Java'
                , @script = N'JavaTest1.helloWorld'
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Execute Hello World</em></p>

<p>The only real difference we see in <em>Code Snippet 7</em> when executing Java code, compared to R/Python is that we call into the method we want to execute as opposed to send in the script.</p>

<blockquote>
<p><strong>NOTE:</strong> If you want a refresher about <code>sp_execute_external_script</code>, have a look at these three blog posts:</p>

<ul>
<li><a href="/2018/03/07/microsoft-sql-server-r-services---sp_execute_external_script---i/">Microsoft SQL Server R Services: sp_execute_external_script - I</a></li>
<li><a href="/2018/03/11/microsoft-sql-server-r-services---sp_execute_external_script---ii/">Microsoft SQL Server R Services - sp_execute_external_script - II</a></li>
<li><a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services: sp_execute_external_script - III</a></li>
</ul>
</blockquote>

<p>So, the code in <em>Code Snippet 7</em> looks very straightforward, but when we execute, the result is this:</p>

<p><img src="/images/posts/sql_2k19_java_intro_error1.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Execution Exception</em></p>

<p>Hmm, in <em>Figure 8</em> we see an exception about how we miss a member <code>numberOfOutputCols</code> in the class. What is this? So, in R/Python, there is a much &ldquo;tighter&rdquo; integration between SQL Server and the external runtime than between SQL Server and Java. Also, once again, with Java, we execute already compiled code whereas we in R/Python execute scripts. This all means that we don&rsquo;t have the same control over the runtime in Java, and we need to use &ldquo;magic&rdquo; variables to indicate to Java what we want to do. The <code>numberOfOutputCols</code> variable is such a variable, and the components that call into Java expects to find this variable. In this case, the variable is not there, so, therefore, the exception. However, take a look at the exception. Before the <code>Error: ...</code>, we do see the output from <code>System.out.print</code>, so all is not lost :).</p>

<blockquote>
<p><strong>NOTE:</strong> As I mentioned above, in future blog posts I cover the various components that make the Java integration work.</p>
</blockquote>

<h4 id="numberofoutputcols">numberOfOutputCols</h4>

<p>Above I said that <code>numberOfOutputCols</code> is a &ldquo;magic&rdquo; variable and that there are others as well. In future posts we cover the others, here we look at what <code>numberOfOutputCols</code> does.</p>

<p>First, all the &ldquo;magic&rdquo; variables have to do with passing of data between SQL Server and Java, and <code>numberOfOutputCols</code> indicates the number of columns you return to the calling code. In our code in <em>Code Snippet 5</em> we do not return anything, but the variable is still expected to be there. So, let us change the code slightly:</p>

<pre><code class="language-java">public class JavaTest1{
  public static short numberOfOutputCols;
  public static void helloWorld() {
    System.out.print(&quot;Hello World from SQL Java&quot;);
  }
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Java Hello World - II</em></p>

<p>In <em>Code Snippet 8</em> I added <code>numberOfOutputCols</code> as a class member. I recompile the code and copy the <code>class</code> file to <code>CLASSPATH</code>, and when I execute as in <em>Code Snippet 7</em>, all works! Yes, I have successfully executed my first Java application from inside SQL Server.</p>

<blockquote>
<p><strong>NOTE:</strong> The <code>numberOfOutputCols</code> variables has to be declared as <code>public static</code> and <code>short</code> as data type. Any other data type, and you get an exception.</p>
</blockquote>

<p>That was easy, what about something slightly more complicated, like a method expecting parameters?</p>

<h4 id="method-parameters">Method Parameters</h4>

<p>We want to add some basic calculator capabilities to our <code>JavaTest1</code> class because it is almost a law that after the obligatory &ldquo;Hello World&rdquo; application we need to write an adder method :). A typical Java <code>adder</code> method might look like so:</p>

<pre><code class="language-java">public static int adder(int x, int y) {
  return x + y;
}
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Java Adder</em></p>

<p>The problem with the code above, (if we want to use it from SQL Server), is twofold:</p>

<ul>
<li>It expects parameters to be passed in to the <code>adder</code> method.</li>
<li>It has a return type (<code>int</code>).</li>
</ul>

<p>So how do we turn the code in <em>Code Snippet 9</em> into something we can execute from SQL Server?</p>

<p>We need to figure out what to do with the parameters (<code>x</code> and <code>y</code>), and the return type. The parameters cannot be parameters in the method, but potentially variables in the class. As for the return type, for now, we just print the result:</p>

<pre><code class="language-java">public class JavaTest1 {
  public static short numberOfOutputCols;
  public static int x;
  public static int y;

  public static void adder() {
    System.out.printf(&quot;The result of adding %d and %d = %d&quot;, x, y, x + y);   
  }

  public static void helloWorld() {
    System.out.print(&quot;Hello World from SQL Java&quot;);
  }

}
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Java Adder Using Class Variables</em></p>

<p>The code in <em>Code Snippet 10</em>, looks like it should work, so recompile and place the compiled code in <code>CLASSPATH</code>. We can now execute, but the question is how we pass in the parameters, and how the Java code knows what they are?</p>

<p>In <a href="/2018/03/11/microsoft-sql-server-r-services---sp_execute_external_script---ii/">Microsoft SQL Server R Services - sp_execute_external_script - II</a> I wrote about parameters for scripts when we execute <code>sp_execute_external_script</code>:</p>

<ul>
<li>For parameters in R/Python scripts we declare them in SQL appending <code>@</code> to the R/Python name.</li>
<li>We define the SQL parameters in <code>sp_execute_external_script</code>&rsquo;s <code>@params</code> parameter.</li>
<li>We add the SQL parameters as named parameters in <code>sp_execute_external_script</code>.</li>
</ul>

<p>For Java code we do the same, so the SQL code looks something like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'JavaTest1.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Execute SQL Code with Parameters</em></p>

<p>As we see in <em>Code Snippet 11</em>, I have added the Java <code>x</code> and <code>y</code> class members to the <code>@params</code> parameter as SQL parameters: <code>@x</code> and <code>@y</code>. I then add the <code>@x</code> and <code>@y</code> parameters as named parameters, and assign them values from the declared parameters <code>@p1</code> and <code>@p2</code>. When I execute the code in <em>Code Snippet 11</em>, I see this:</p>

<p><img src="/images/posts/sql_2k19_java_intro_exec_params.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Execution with Parameters</em></p>

<p>As we see in <em>Figure 9</em> it works and, outlined in red, we get the result printed out. Happy Times!</p>

<h2 id="summary">Summary</h2>

<p>In this post we saw how we install and enable the Java extensions for SQL Server 2019:</p>

<ul>
<li>On Windows you install them via the <em>Machine Learning Services</em> feature during setup or when adding features.</li>
<li>In Linux you install by installing the package: <code>mssql-server-extensibility-java</code>.</li>
<li>Unless the <code>external scripts enabled</code> configuration option is already enabled you need to enable it as in <em>Code Snippet 3</em>.</li>
<li>The Java supported version on both Windows and Linux is version 1.8.x (at least for CTP 2.1).</li>
<li>On Linux, when you install the <code>mssql-server-extensibility-java</code> package, the correct Java version gets installed if it is not on the box already.</li>
<li>On Windows, you have to install the correct version manually if it is not installed.</li>
<li>On Windows, the <code>Path</code> system environment variable needs to be extended to include the directory where <code>jvm.dll</code> exists.</li>
<li>It is good practice to create an environment variable called <code>CLASSPATH</code> which indicates where your compiled Java code exists.</li>
<li>On Windows you need to give <code>Read</code> permissions on the <code>CLASSPATH</code> directory to <code>ALL APPLICATION PACKAGES</code>.</li>
<li>On Linux you need to give <code>Read</code> and <code>Execute</code> permissions on the on the <code>CLASSPATH</code> directory to the <code>mssql_satellite</code> user.</li>
</ul>

<p>So, when the Java extensions are enabled we can write and execute Java code:</p>

<ul>
<li>We execute Java code from SQL Server using <code>sp_execute_external_script</code>.</li>
<li>We define the class and method we call in the <code>@script</code> parameter.</li>
<li>All methods called from SQL Server need to be <code>public static</code>.</li>
<li>The static methods can not have a return type, they need to be <code>public static void</code>.</li>
<li>The methods must be parameterless.</li>
<li>No support for output parameters.</li>
<li>The code needs to contain &ldquo;magic&rdquo; <code>public static</code> class members.</li>
<li>One such member is <code>numberOfOutputCols</code> which is always required. It has to be declared as: <code>public static short numberOfOutputCols;</code>.</li>
<li>Even though the methods must be parameterless we can pass in parameters as class members.</li>
<li>We refer to the class member parameters in SQL with the same name as the Java names but appended with <code>@</code>.</li>
<li>We define the SQL parameters in <code>sp_execute_external_script</code>&rsquo;s <code>@params</code> parameter.</li>
<li>We add the SQL parameters as named parameters in <code>sp_execute_external_script</code>.</li>
</ul>

<p>So far, the code we have used is very simple. In the next post we look at how we can return data to the caller, and how we pass data sets around.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 47]]></title>
    <link href="http://nielsberglund.com/2018/11/25/interesting-stuff---week-47/" rel="alternate" type="text/html"/>
    <updated>2018-11-25T13:21:17+02:00</updated>
    <id>http://nielsberglund.com/2018/11/25/interesting-stuff---week-47/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.tbray.org/ongoing/When/201x/2018/11/18/Post-REST">Post-REST</a>. A very interesting post by [Tim Bray] where he looks at <em>What Comes after REST</em>/how REST will evolve. Very interesting!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/using-apache-kafka-drive-cutting-edge-machine-learning">Using Apache Kafka to Drive Cutting-Edge Machine Learning</a>. A very, very &ldquo;cool&rdquo; post, discussing how Kafka and Machine Learning fits together.</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/dean-of-big-data-driving-the-snakes-of-data-science-out-of">Dean of Big Data Driving the Snakes of Data Science Out of Ireland</a>. Well, I don&rsquo;t know about &ldquo;St. Paddy&rdquo; and <a href="https://news.nationalgeographic.com/news/2014/03/140315-saint-patricks-day-2014-snakes-ireland-nation/">snakes in Ireland</a>, but this post tries to do away with some Data Science myths (get rid of the snakes). Really worth reading!</li>
<li><a href="https://databricks.com/blog/2018/11/21/mlflow-v0-8-0-features-improved-experiment-ui-and-deployment-tools.html">MLflow v0.8.0 Features Improved Experiment UI and Deployment Tools</a>. A week or so ago, <a href="https://twitter.com/databricks">databricks</a> released <a href="https://www.mlflow.org/">MLFlow 0.8.0</a>. This blog post describes a couple of new features in that release.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>After I published the <a href="/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/">SQL Server 2019 Big Data Cluster on Azure Kubernetes Service</a> post, I went back and started to look at the new Java functionality in SQL Server 2019. There is a post forthcoming shortly (the famous last words).</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 46]]></title>
    <link href="http://nielsberglund.com/2018/11/18/interesting-stuff---week-46/" rel="alternate" type="text/html"/>
    <updated>2018-11-18T08:22:25+02:00</updated>
    <id>http://nielsberglund.com/2018/11/18/interesting-stuff---week-46/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/11/12/building-c-8-0/">Building C# 8.0</a>. A post about new functionality in the upcoming C# 8 release. Some very interesting new features, and it looks like C# gets more and more features that you normally find in F#.</li>
<li><a href="https://adamsitnik.com/Sample-Perf-Investigation/">Sample performance investigation using BenchmarkDotNet and PerfView</a>. In this blog post <a href="https://twitter.com/SitnikAdam">Adam</a> describes how he approaches sample performance problem using available free .NET tools and best practices for performance engineering.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://muratbuffalo.blogspot.com/2018/11/my-emacs-journey.html">My Emacs journey</a>. This blog post by <a href="https://twitter.com/muratdemirbas">Murat</a>, where he talks about Emacs, brings me back to the early days of .NET, (pre 1.0), where I created an Emacs .NET intellisense extension. Ah, those were the days!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/11/16/overload-control-for-scaling-wechat-microservices/">Overload control for scaling WeChat microservices</a>. In this blog post <a href="https://twitter.com/adriancolyer">Adrian</a> looks at a white paper discussing how <a href="https://www.wechat.com/en/">WeChat</a> handles overload control. Some very impressive numbers and quotes: <em>&ldquo;WeChat’s microservice system accommodates more than 3000 services running on over 20,000 machines in the WeChat business system, and these numbers keep increasing as WeChat is becoming immensely popular… As WeChat is ever actively evolving, its microservice system has been undergoing fast iteration of service updates. For instance, from March to May in 2018, WeChat’s microservice system experienced almost a thousand changes per day on average.&rdquo;</em>. Think about that; 20,000 machines, thousand code changes per day, wow!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://data-artisans.com/blog/stream-processing-introduction-event-time-apache-flink">Stream processing: An Introduction to Event Time in Apache Flink</a>. Apache Flink supports multiple notions of time for stateful stream processing. This post focuses on event time support in Apache Flink.</li>
<li><a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained">Kafka Connect Deep Dive – Converters and Serialization Explained</a>. In this article <a href="https://twitter.com/rmoff">Robin Moffat</a> takes us through how serialization works in Kafka Connectors.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/11/13/creating-a-data-hub-for-your-analytics-with-polybase/">Creating a data hub for your analytics with PolyBase</a>. SQL Server 2019 CTP 2.0 introduces new connectors for PolyBase, and this blog post discusses how the new connectors enable customers to leverage PolyBase for creating a virtual data hub for a wide variety of data sources within the enterprise. Very interesting!<br /></li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/the-ultimate-r-cheatsheet">The Ultimate R Cheatsheet</a>. As the title says, this post points to an impressive cheatsheet for R.</li>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/data-science-for-managers-mindmap">Intro to Data Science for Managers [Mindmap]</a>. Data science is incredibly broad and complex discipline and can be daunting trying to get your head around. So this blog post tries to mind-map the various key data science concepts and techniques in order to make data science easier to grasp.</li>
<li><a href="https://powerbi.microsoft.com/en-us/blog/power-bi-announces-new-ai-capabilities/">Announcing new AI Capabilities for Power BI to make AI Accessible for Everyone</a>. I guess the title of this blog posts says it all. It announces new AI capabilities for Power BI. This is huge, and I can not wait to test it out!</li>
<li><a href="https://www.infoq.com/presentations/uber-big-data-dl-ml">Big Data and Deep Learning: A Tale of Two Systems</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation about how Uber tackles data caching in large-scale deep learning, its ML architecture and discusses how Uber uses Big Data. The presentation concludes by sharing AI use cases.</li>
<li><a href="https://towardsdatascience.com/implementing-facebook-prophet-efficiently-c241305405a3">Implementing Facebook Prophet efficiently</a>. I wrote back in 2017 about Facebook Prophet and how to run it in <a href="/2017/05/20/facebook-prophet-and-microsoft-r-server/">Microsoft R Server</a> as well as <a href="/2017/05/20/facebook-prophet-and-microsoft-r-server/">SQL Server Machine Learning Services</a>. The blog post I link to <a href="https://towardsdatascience.com/implementing-facebook-prophet-efficiently-c241305405a3">here</a> discusses how to optimize Prophet. Very interesting!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 45]]></title>
    <link href="http://nielsberglund.com/2018/11/11/interesting-stuff---week-45/" rel="alternate" type="text/html"/>
    <updated>2018-11-11T08:05:58+02:00</updated>
    <id>http://nielsberglund.com/2018/11/11/interesting-stuff---week-45/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<p>##.NET</p>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/11/07/understanding-the-whys-whats-and-whens-of-valuetask/">Understanding the Whys, Whats, and Whens of ValueTask</a>. The .NET Framework 4 saw the introduction of the <code>System.Threading.Tasks</code> namespace, and with it the <code>Task</code> class. In this post, <a href="https://github.com/stephentoub">Stephen Toub</a>, covers the newer <code>ValueTask</code> and <code>ValueTask&lt;TResult&gt;</code> types.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://data-artisans.com/blog/flink-sql-powerful-querying-of-data-streams">Flink SQL for powerful querying of data streams and data at rest</a>. This post covers Flink SQL, and discusses how we can quickly explore data in streams or data at rest. Very interesting!<br /></li>
<li><a href="https://azure.microsoft.com/en-us/blog/announcing-the-general-availability-of-azure-event-hubs-for-apache-kafka/">Announcing the general availability of Azure Event Hubs for Apache Kafka</a>. This post announces the general availability (GA) of Azure Event Hubs for Apache Kafka. So what does it mean? Well, it means that you can now stream events from applications using the Kafka protocol directly into Azure Event Hubs. Awesome!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/container-runtimes-kubernetes">Who Is Running My Kubernetes Pod? The Past, Present, and Future of Container Runtimes</a>. Container runtime choices are nowadays not only Docker as the &ldquo;Open Container Initiative&rdquo; (OCI) has successfully standardized the concept of a container and container image to guarantee interoperability between runtimes. This <a href="https://www.infoq.com/">InfoQ</a> article looks at the past, present, and future of container engine implementations.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/sqlserverstorageengine/2018/11/07/introducing-scalar-udf-inlining/">Introducing Scalar UDF Inlining</a>. SQL Server 2017 introduced <a href="https://docs.microsoft.com/en-us/sql/relational-databases/performance/intelligent-query-processing?view=sql-server-2017">Intelligent Query Processing</a> which is meant to improve the performance of existing workloads with minimal implementation effort. SQL Server 2019 further expands query processing capabilities, and this blog post discusses Scalar T-SQL UDF Inlining. The inlining of scalar UDF&rsquo;s is a feature to improve the performance of queries that invoke scalar UDFs, where UDF execution is the main bottleneck.</li>
<li><a href="/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/">SQL Server 2019 Big Data Cluster on Azure Kubernetes Service</a>. This is a blog post by &ldquo;yours truly&rdquo;. As the title implies, it discusses how to deploy <strong>SQL Server 2019 Big Data Cluster</strong> to <strong>Azure Kubernetes Service</strong> (AKS).</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Big Data Cluster on Azure Kubernetes Service]]></title>
    <link href="http://nielsberglund.com/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/" rel="alternate" type="text/html"/>
    <updated>2018-11-10T07:58:09+02:00</updated>
    <id>http://nielsberglund.com/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/</id>
    <content type="html"><![CDATA[<p>At the <a href="https://www.microsoft.com/en-us/ignite/agenda"><strong>Microsoft Ignite 2018</strong></a> conference back in September Microsoft released <strong>SQL Server 2019</strong> for public preview, and I wrote two short blog posts about it:</p>

<ul>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a>.</li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a>.</li>
</ul>

<p>What Microsoft also announced was <strong>SQL Server 2019 Big Data Clusters</strong>, which combines the SQL Server database engine, Spark, and HDFS into a unified data platform! Yes, you read that right: SQL Server, Spark, and Hadoop right out of the box. Seeing that both Spark and Hadoop are mainly Linux based, what makes the Big Data Cluster possible is <strong>SQL Server on Linux</strong>. When you deploy a <strong>SQL Server 2019 Big Data Cluster</strong>, you deploy it as containers on <strong>Kubernetes</strong>, where the Kubernetes cluster can be in the cloud, such as <a href="https://azure.microsoft.com/en-us/services/kubernetes-service/"><strong>Azure Kubernetes Service</strong></a>, or on-prem like <a href="https://www.openshift.com/learn/topics/kubernetes/"><strong>Red Hat OpenShift</strong></a> or even on a local dev-box/laptop using <a href="https://kubernetes.io/docs/setup/minikube/"><strong>Minikube</strong></a>.</p>

<p>Initially, this post was about <strong>SQL Server 2019 Big Data Clusters</strong> on Minikube, but after quite a few failed installation attempts I realised I did not have enough memory on my development box, so I decided to try it on <strong>Azure Kubernetes Service</strong> (AKS) instead.</p>

<blockquote>
<p><strong>NOTE:</strong> If you want to run <strong>SQL Server 2019 Big Data Clusters</strong> on Minikube it is suggested that your host machine (Minikube is essentially a VM) has at least 32Gb of memory, and you allocate at least 22Gb to the Minikube VM.</p>
</blockquote>

<p>Since I am a complete novice when it comes to Kubernetes, this post covers both how I set up AKS as well as the deployment of <strong>SQL Server 2019 Big Data Clusters</strong> to AKS, and the post is somewhat a summary of the official <a href="https://docs.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?view=sqlallproducts-allversions">documentation</a>.</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> SQL Server 2019 is in public preview, but the preview does not contain the Big Data Cluster parts. To deploy <strong>SQL Server 2019 Big Data Clusters</strong> you need to be part of the SQL Server 2019 Early Adoption Program, for which you can sign up for <a href="https://sqlservervnexteap.azurewebsites.net/">here</a>.</p>
</blockquote>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>Apart from having an Azure subscription and being enrolled in the SQL Server 2019 EAP, there are a couple of other pre-reqs needed.</p>

<h4 id="python">Python</h4>

<p>If you, like me, are a SQL Server guy, you are probably quite familiar with installing SQL Server instances by mounting an ISO file, and running setup. Well, you can forget all that when you deploy a <strong>SQL Server 2019 Big Data Cluster</strong>. The setup is all done via Python utilities, and various Docker images pulled from a private repository. So, you need Python3. On my box I have Python 3.5, and - according to Microsoft - version 3.7 also works. Make you that you have your Python installation on the path.</p>

<p>When you deploy you use a Python utility: <code>mssqlctl</code>. To download <code>mssqlctl</code>, you need Python&rsquo;s package management system <code>pip</code> installed. During installation you also need a Python HTTP library: <em>Requests</em>. If you do not have it you need to install it:</p>

<pre><code class="language-python">python -m pip install requests
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Installing Python Requests</em></p>

<p>Down below we talk some more about how to download the <code>mssqlctl</code> utility.</p>

<h4 id="azure-cli">Azure CLI</h4>

<p>When working with Azure, you can do it in three ways:</p>

<ul>
<li>Azure Portal</li>
<li>Cloud Shell from within the portal.</li>
<li>Azure CLI.</li>
</ul>

<p>The Azure CLI is Microsoft&rsquo;s cross-platform command-line experience for managing Azure resources, and you install it on your local machine. In this post I mainly use the Azure CLI, so if you want to follow along download it from <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest">here</a>.</p>

<h4 id="kubectl">Kubectl</h4>

<p>The <code>kubernetes-cli</code> (Kubernetes command line tool), gives you an executable <code>kubectl.exe</code> which you use to manage your Kubernetes cluster. Using <code>kubectl</code>, you can inspect cluster resources; create, delete, and update components; etc.</p>

<p>You can install <code>kubectl</code> in different ways, and I installed it from <a href="https://chocolatey.org/packages/kubernetes-cli">Chocolatey</a>: <code>choco install kubernetes-cli</code>.</p>

<h2 id="azure-kubernetes-cluster">Azure Kubernetes Cluster</h2>

<p>Ok, so having &ldquo;sorted&rdquo; out the pre-reqs, let us start with creating an Azure Kubernetes cluster through the Azure CLI.</p>

<h4 id="login">Login</h4>

<p>I start with open Powershell as administrator and from Powershell I log in to Azure:</p>

<pre><code class="language-bash">az login
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Login to Azure</em></p>

<p>When I execute the code in <em>Code Snippet 2</em> a tab opens in my browser, and I see a dialog that asks me to pick an account to log in to Azure with:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Azure Login</em></p>

<p>I choose the account from what I see in <em>Figure 1</em>, and after a little while I see in the browser a success message:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login_success.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Azure Login Success</em></p>

<p>At the same time as the success message in <em>Figure 2</em>, the code in <em>Code Snippet 1</em> returns with information what subscriptions I have access to in Azure:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login_return.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Azure Login Return</em></p>

<p>As we see in <em>Figure 3</em>, I have access to multiple subscriptions, and before we go any further, I set the subscription I target. I look at the <code>id</code> for the subscription I want and:</p>

<pre><code class="language-bash">az account set -s &lt;my_subscription_id&gt;
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Set Azure Context</em></p>

<p>I have now logged in and indicated, as in <em>Code Snippet 3</em>, what subscription to use.</p>

<h4 id="resource-groups">Resource Groups</h4>

<p>Everything we do in Azure is in the context of a resource group. A resource group is a logical group in which Azure resources are deployed and managed, and it exists in a physical location (Azure data center). So when I create a Kubernetes cluster, I need to define what resource group the cluster should belong to. So let us create a resource group:</p>

<pre><code class="language-bash">az group create --name kubernetes-rg --location westeurope
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Create Resource Group</em></p>

<p>In <em>Code Snippet 4</em> we see how I create a group named <code>kubernetes-rg</code>, and I want it in the West Europe region. After I run the code in <em>Code Snippet 4</em>, I get back a JSON blob:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_create_rg.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Azure Create Group Return</em></p>

<p>The JSON blob, as in <em>Figure 4</em>, contains details about my newly created resource group. If I log in to the Azure Portal:</p>

<p><img src="/images/posts/sql_2k19_bdc_portal_resource_group.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Azure Resource Group</em></p>

<p>In the portal, when I click the <em>Resource groups</em> link (outlined in red in <em>Figure 5</em>), I see my newly created resource group outlined in blue.</p>

<h2 id="create-kubernetes-cluster">Create Kubernetes Cluster</h2>

<p>I now have a resource group, and I go on to create the Kubernetes cluster.</p>

<blockquote>
<p><strong>NOTE:</strong> You do not need to create a new resource group as such. When you create the Kubernetes cluster, you can create it in an existing group.</p>
</blockquote>

<p>To create the Kubernetes cluster I continue to use the Azure CLI, and I use the <code>az aks create</code> command. The command has quite a few parameters, which you can read about <a href="https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create">here</a>, but I use only a few of them:</p>

<ul>
<li><code>--name</code>: name of the cluster.</li>
<li><code>--resource-group</code>: name of the resource group.</li>
<li><code>--disable-rbac</code>: disables Kubernetes Role-Based Access Control.</li>
<li><code>--generate-ssh-keys</code>: if no SSH keys exist, generate both public and private key files.</li>
<li><code>--node-vm-size</code>: the size of the VM&rsquo;s used for the nodes in the Kubernetes cluster. For a <strong>SQL Server 2019 Big Data Cluster</strong> you need a VM with at least 32Gb of memory. You can see a list of VM sizes and their features in the portal <a href="https://portal.azure.com/#create/microsoft.aks">here</a>. I use &ldquo;Standard E4s_v3&rdquo;.</li>
<li><code>--node-count</code>: number of nodes in the Kubernetes node pool. I use 3.</li>
<li><code>--kubernetes-version</code>: the version of Kubernetes to use for creating the cluster. The <strong>SQL Server 2019 Big Data Cluster</strong> requires at minimum the Kubernetes v1.10 version.</li>
</ul>

<p>Before I create the cluster, let us talk a little bit about <code>--node-vm-size</code> and <code>--node-count</code> as they are somewhat related to each other. In addition to defining how much memory a VM has the <code>--node-vm-size</code> also defines the number of virtual CPU&rsquo;s (VCPUS) for the VM. The number of VCPUS controls how many data disks the VM has, (normally it is 2 disks per VCPU). The number of disks per VM is important as the <strong>SQL Server 2019 Big Data Cluster</strong> mounts quite a lot of storage on individual disks and with too few disks the mount operations fail. To get more disks you either increase the VM size or the node count, and that is the relation between <code>--node-vm-size</code> and <code>--node-count</code>.</p>

<p>For a &ldquo;default&rdquo; <strong>SQL Server 2019 Big Data Cluster</strong> deployment around 20 disks are required. So if I choose the &ldquo;Standard E4s_v3&rdquo; VM as vm size, I want at least 3 nodes. With this in mind the code to create a Kubernetes cluster looks like so:</p>

<pre><code class="language-bash">az aks create --name sqlkubecluster \
--resource-group kubernetes-rg \
--disable-rbac \
--generate-ssh-keys \
--node-vm-size Standard_E4s_v3 \
--node-count 3 \
--kubernetes-version 1.10.8
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Kubernetes Cluster</em></p>

<p>In <em>Code Snippet 5</em> we see how:</p>

<ul>
<li>I want to create a cluster with the name <code>sqlkubecluster</code>.</li>
<li>I want the cluster in the <code>kubernetes-rg</code> resource group.</li>
<li>I do not want to use Kubernetes Role-Based Access Control.</li>
<li>I want SSH keys created.</li>
<li>I want the VM&rsquo;s to be &ldquo;Standard E4s_v3&rdquo;,</li>
<li>I want 3 nodes.</li>
<li>Finally I want the Kubernetes version to be 1.10.8.</li>
</ul>

<p>When I execute the code I see something like so:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_create_cluster.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Run Create Kubernetes Cluster</em></p>

<p>What we see in <em>Figure 6</em> runs for several minutes and when it completes I receive a JSON blob with information about the cluster.</p>

<p>I mentioned above how <code>kubectl</code> is used to manage your Kubernetes cluster, and now when the cluster is created, we need to configure <code>kubectl</code> to connect to the cluster. To do this, you use the <code>az aks get-credentials</code> command like so:</p>

<pre><code class="language-bash">az aks get-credentials --resource-group=kubernetes-rg --name sqlkubecluster
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Get Credentials</em></p>

<p>We see in <em>Code Snippet 6</em> how I pass in the name of the resource group and cluster as parameters and when I execute:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_credentials.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Get Credentials</em></p>

<p>The <code>config</code> file we see outlined in red in <em>Figure 7</em> holds, among other things, the keys for the Kubernetes cluster. To ensure that I can connect to the cluster I call <code>kubectl get nodes</code>, and I see some information about the cluster nodes.</p>

<h4 id="dashboard-namespaces">Dashboard &amp; Namespaces</h4>

<p>To monitor and manage a Kubernetes cluster you do not have to rely on <code>kubectl</code> solely, as you can also use the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Kubernetes Dashboard</a>. In Azure you access the Dashboard by the <code>az aks browse</code> command, and - as with <code>get-credentials</code> - you pass in the names of the resource group and cluster: <code>az aks browse --resource-group kubernetes-rg --name sqlkubecluster</code>, and a new tab opens in your browser:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_dashboard.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Kubernetes Dashboard</em></p>

<p>In <em>Figure 8</em> we see the dashboard right after I created the VM and the Kubernetes cluster. Notice <em>namespaces</em>, outlined in red. <em>Namespcaces</em> help different projects, teams, or customers to share a Kubernetes cluster, and when you deploy to Kubernetes, you deploy into a namespace. To see what <em>namespaces</em> exist in a cluster you execute: <code>kubectl get namespaces</code>. When I do it at this stage I see:</p>

<p><img src="/images/posts/sql_2k19_bdc_kubectl_namespaces.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Kubernetes Namespaces</em></p>

<p>When we look at <em>Figure 9</em> we see following <em>namespaces</em>:</p>

<ul>
<li><code>default</code>: A default, (duh), namespace to hold the default set of Pods, Services, and Deployments used by the cluster.</li>
<li><code>kube-public</code>: A namespace readable by everyone for public <a href="https://unofficial-kubernetes.readthedocs.io/en/latest/tasks/configure-pod-container/configmap/">ConfigMap&rsquo;s</a>.</li>
<li><code>kube-system</code>: A namespace for objects created by the Kubernetes system.</li>
</ul>

<p>So, coming back to Dashboard: when we want to monitor a deployment with Dashboard, we monitor a specific namespace. Enough of this, let us deploy!</p>

<h2 id="deploy-sql-server-2019-big-data-cluster">Deploy SQL Server 2019 Big Data Cluster</h2>

<p>I mentioned above that when we deploy a <strong>SQL Server 2019 Big Data Cluster</strong> we deploy using a Python utility: <code>mssqlctl</code>. So what we need to do is to download and install the utility:</p>

<pre><code class="language-bash">pip3 install \
   --index-url https://private-repo.microsoft.com/python/ctp-2.0 \
     mssqlctl
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Download and Install mssqlctl</em></p>

<p>We download and install <code>mssqlctl</code> from a Microsoft repository as we see in <em>Code Snippet 7</em>. After download the source is located in <code>&lt;python_path&gt;\lib\site-packages</code> and the executable - <code>mssqlctl.exe</code> - is at: <code>&lt;python_path&gt;\Scripts</code>.</p>

<h4 id="environment-variables">Environment Variables</h4>

<p>When you deploy the <strong>SQL Server 2019 Big Data Cluster</strong> using <code>mssqlctl</code> you customise the cluster configuration via environment variables read by <code>mssqlctl</code>. To see all available environment variables you go <a href="https://docs.microsoft.com/en-us/sql/big-data-cluster/deployment-guidance?view=sqlallproducts-allversions">here</a>. Below I list the ones I use:</p>

<ul>
<li>SET ACCEPT_EULA=Y - to accept the SQL Server license agreement.</li>
<li>SET CLUSTER_PLATFORM=aks - the Kubernetes platform you deploy to: Azure - <code>aks</code>, Kubernetes - <code>kubernetes</code>, Minikube - <code>minikube</code>.</li>
<li>SET CONTROLLER_USERNAME=admin - the user name for the cluster administrator. You can set this to anything.</li>
<li>SET CONTROLLER_PASSWORD=<some_secret_password> - the password for the cluster administrator.</li>
<li>SET KNOX_PASSWORD=<some_secret_password> - the password for the Knox user. <a href="https://knox.apache.org/">Knox</a> is an application gateway for interacting with the REST API&rsquo;s and UI&rsquo;s of Apache Hadoop deployments.</li>
<li>SET MSSQL_SA_PASSWORD=<some_secret_password> - the <code>sa</code> password for the master SQL instance. It needs to meet password complexity requirements.</li>
<li>SET DOCKER_REGISTRY=private-repo.microsoft.com - the registry for the images being pulled.</li>
<li>SET DOCKER_REPOSITORY=mssql-private-preview - the repository within the registry.</li>
<li>SET DOCKER_USERNAME=<docker_username> - user name to access the images. You get this when you sign up for the <a href="https://sqlservervnexteap.azurewebsites.net/">EAP</a>.</li>
<li>SET DOCKER_PASSWORD=<some_secret_password> - the password for the above user. You get this when you sign up for the <a href="https://sqlservervnexteap.azurewebsites.net/">EAP</a>.</li>
<li>SET DOCKER_EMAIL=<email_for_the_docker_user> - the email associated with the registry. You get this when you sign up for the <a href="https://sqlservervnexteap.azurewebsites.net/">EAP</a>.</li>
<li>SET DOCKER_PRIVATE_REGISTRY=1 - this has to be set to 1.</li>
</ul>

<p>Before you deploy the environment variables, need to be set, and if you are on Windows, you need to do it from a command prompt (not Powershell). Instead of having to enter these variables individually, I have a <code>bat</code> file I run before deploying: <code>set_env_variables_aks.bat</code>.</p>

<h4 id="create-cluster">Create Cluster</h4>

<p>After I have set the variables I create the cluster with the <code>mssqlctl</code> command, and I do it from the command prompt (not Powershell):</p>

<pre><code class="language-bash">mssqlctl create cluster sqlbigdata1 -v
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Create Big Data Cluster</em></p>

<p>Looking at <em>Code Snippet 8</em> we see how I call <code>mssqlctl</code> to create a <strong>SQL Server 2019 Big Data Cluster</strong>, and I want to create it in a namespace called <code>sqlbigdata1</code>. I use the <code>-v</code> flag (as in verbose) to get debug output. When I execute the code I see something like so:</p>

<p><img src="/images/posts/sql_2k19_bdc_create_cluster.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Create Big Data Cluster</em></p>

<p>What we see in <em>Figure 10</em> is how we have started to create the main controller and its pod. We also see a note (outlined in red) saying that it can take quite a while to create the cluster. To monitor the process you can use Dashboard:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_dashboard2.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Kubernetes Dashboard</em></p>

<p>In <em>Figure 11</em> we see an overview of the <code>sqlbigdata1</code> namespace. You may see errors in the dashboard, but you can ignore them initially. In addition to Dashboard to monitor progress you can use <code>kubectl</code> commands, for example: <code>kubectl get pods -n sqlbigdata1</code>:</p>

<p><img src="/images/posts/sql_2k19_bdc_kubectl1.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Pods being Created</em></p>

<p>The <strong>SQL Server 2019 Big Data Cluster</strong> exposes its own dashboard; the <em>Cluster Administration Portal</em>, which we can use to monitor the deployment as well. The portal becomes available as soon as the controller is up, and in a running state. The portal is exposed at the endpoint for the <code>service-proxy-lb</code> (proxy load balancer) service. To find the IP address, you call: <code>kubectl get svc service-proxy-lb -n &lt;name of your cluster&gt;</code>:</p>

<p><img src="/images/posts/sql_2k19_bdc_cluster_admin_ip.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Endpoint for Cluster Admin</em></p>

<p>In <em>Figure 13</em> we see how <code>svc service-proxy-lb</code> has an external IP of <code>13.94.174.28</code>, and it exposes two ports: <code>30777</code> and <code>31826</code>. The port for the portal is <code>30777</code>, and when I browse there, I first need to log in with the <code>CONTROLLER_USERNAME</code> (admin in my case) and <code>CONTROLLER_PASSWORD</code>. After login, I come to the <em>Overview</em> page. I then click on the <em>Deployment</em> link outlined in red:</p>

<p><img src="/images/posts/sql_2k19_bdc_cluster_admin2.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Deployment Progress</em></p>

<p>What we see in <em>Figure 14</em> is the progress of the <strong>SQL Server 2019 Big Data Cluster</strong> deployment, and we see that it is still in progress: yellow triangle by the namespace, (outlined in blue).</p>

<p>Eventually, the deployment finishes, and we know that either: by seeing that the triangle in <em>Figure 14</em> is now a green circle, or by the output from command line:</p>

<pre><code class="language-bash">2018-11-07 09:04:52.0147 UTC | INFO | Data pool is ready...
2018-11-07 09:04:52.0148 UTC | INFO | Storage pool is ready...
...
2018-11-07 09:06:55.0073 UTC | INFO | Compute pool is ready...
...
2018-11-07 09:07:36.0155 UTC | INFO | Cluster state: Ready
2018-11-07 09:07:36.0155 UTC | INFO | Cluster deployed successfully.
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Cluster Deployed Successfully</em></p>

<p>We see in <em>Code Snippet 9</em> how <code>mssqlctl</code> reports that the various pools are ready, followed by successful cluster deployment.</p>

<h4 id="connection-endpoints">Connection Endpoints</h4>

<p>So far, so good - but what do we do now? We know that a <strong>SQL Server 2019 Big Data Cluster</strong> consists both of a SQL Server master instance, as well as Hadoop/Spark, but where do we find them?</p>

<p>As with the portal, the endpoints are service load balancers endpoints. The service load balancer for the SQL Server master instance is: <code>service-master-pool-lb</code> and for Hadoop/Spark it is: <code>service-security-lb</code>. To retrieve the endpoints I call:</p>

<pre><code class="language-bash"># SQL Server master instance
kubectl get service service-master-pool-lb \
                 -o=custom-columns=&quot;IP:.status.loadBalancer.\
                 ingress[0].ip,PORT:.spec.ports[0].port&quot; \
                 -n sqlbigdata1

# Hadoop/Spark
kubectl get service service-security-lb \
                 -o=custom-columns=&quot;IP:.status.loadBalancer.\
                 ingress[0].ip,PORT:.spec.ports[0].port&quot; \
                 -n sqlbigdata1
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Get Endpoints</em></p>

<p>In <em>Code Snippet 10</em> we see how I customize what I want to be returned from the <code>get service</code> calls to only to return IP addresses and ports. With these endpoints, I can now connect to my <strong>SQL Server 2019 Big Data Cluster</strong>. Once again, to connect to the SQL Server master instance (databases), you use the <code>service-master-pool-lb</code> endpoint, and to connect to Hadoop/Spark, the <code>service-security-lb</code> endpoint is what you use.</p>

<p>The user names and passwords are:</p>

<ul>
<li>SQL Server master instance: <code>sa</code> as user name, and <code>MSSQL_SA_PASSWORD</code> as password.</li>
<li>Hadoop / Spark: <code>root</code> as user name, and <code>KNOX_PASSWORD</code> as password.</li>
</ul>

<p>In a future post I look at what we can do with <strong>SQL Server 2019 Big Data Cluster</strong>.</p>

<h2 id="summary">Summary</h2>

<p>In this post we looked at how to install <strong>SQL Server 2019 Big Data Cluster</strong> on <em>Azure Kubernetes Service</em> (AKS). We saw how to:</p>

<ul>
<li>Create a new Azure resource group using Azure CLI.</li>
<li>Create a Kubernetes cluster in that resource group.</li>
</ul>

<p>We discussed the size requirements for the VM&rsquo;s in the cluster, and mentioned they needed at least 32Gb of RAM. We also need quite a few disks to mount storage on, so the node count is important.</p>

<p>The actual deployment of a <strong>SQL Server 2019 Big Data Cluster</strong> is done using a Python utility <code>mssqlctl</code>. During the deployment process we can monitor the progress via:</p>

<ul>
<li><code>kubectl</code> commands.</li>
<li>the Kubernetes dashboard.</li>
<li>the <strong>SQL Server 2019 Big Data Cluster</strong>&rsquo;s <em>Cluster Administration Portal</em>.</li>
</ul>

<p>Access to the various services in a <strong>SQL Server 2019 Big Data Cluster</strong> is through service load balancers and their external IP addresses and ports:</p>

<ul>
<li>Cluster Administration Portal: <code>service-proxy-lb</code>.</li>
<li>SQL Server master instance: <code>service-master-pool-lb</code>.</li>
<li>Hadoop/Spark: <code>service-security-lb</code>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 44]]></title>
    <link href="http://nielsberglund.com/2018/11/04/interesting-stuff---week-44/" rel="alternate" type="text/html"/>
    <updated>2018-11-04T18:54:29+02:00</updated>
    <id>http://nielsberglund.com/2018/11/04/interesting-stuff---week-44/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/local-testing-with-live-data-means-faster-development-with-azure-stream-analytics-2/">Local testing with live data means faster development with Azure Stream Analytics</a>. An Azure Stream Analytics (ASA) announcement of a new feature which lets you test your ASA queries locally while using live data streams from cloud sources such as Azure Event Hubs, IoT Hub or Blob storage!</li>
<li><a href="https://www.confluent.io/blog/atm-fraud-detection-apache-kafka-ksql">ATM Fraud Detection with Apache Kafka and KSQL</a>. A blog post by <a href="https://twitter.com/rmoff">Robin Moffat</a> discussing how, by using Kafka and KSQL, you can take a stream of inbound ATM transactions and easily set up an application to detect transactions that look fraudulent. Awesome!</li>
<li><a href="https://blog.acolyer.org/2018/10/29/noria-dynamic-partially-stateful-data-flow-for-high-performance-web-applications/">Noria: dynamic, partially-stateful data-flow for high-performance web applications</a>. <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white-paper about Noria: a new streaming data-flow system designed to act as a fast storage backend for read-heavy web applications. It acts like a database but precomputes and caches relational query results so that reads are blazingly fast.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/11/02/the-fuzzylog-a-partially-ordered-shared-log/">The FuzzyLog: a partially ordered shared log</a>. Another white-paper dissected by <a href="https://twitter.com/adriancolyer">Adrian</a>. This time it is about FuzzyLog and the FuzzyLog abstraction which extends the shared log approach to partial orders, allowing applications to scale linearly without sacrificing transactional guarantees, and switch seamlessly between these guarantees when the network partitions and heals.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>I am still trying to get to grips with <strong>SQL Server 2019 Big Data Cluster</strong>, and now I better sort it out as I on Wednesday (November 7), is supposed to give a presentation about <a href="https://www.meetup.com/Azure-Transformation-Labs/events/255690875/?rv=ea1_v2&amp;_xtd=gatlbWFpbF9jbGlja9oAJGRjZmM2OTBjLWEwNTgtNDRjZi1iOWQyLWI5YzMyZTM1YmIzMg">Building a SQL 2019 Big Data Cluster on Azure Kubernetes Service</a>. If you are in Durban, please come by and say Hi!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 43]]></title>
    <link href="http://nielsberglund.com/2018/10/28/interesting-stuff---week-43/" rel="alternate" type="text/html"/>
    <updated>2018-10-28T07:44:55+02:00</updated>
    <id>http://nielsberglund.com/2018/10/28/interesting-stuff---week-43/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/announcing-automated-ml-capability-in-azure-machine-learning/">Announcing automated ML capability in Azure Machine Learning</a>. Somehow I must have missed this post announcing <strong>Azure Automated Machine Learning</strong>. What is it? Well, it is a way for the <a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">Azure Machine Learning Service</a> to automatically pick an algorithm for you, and generate a model from it. It sounds really interesting, and this is something I need to take a look at.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><p><a href="https://www.confluent.io/blog/kafka-summit-san-francisco-2018-roundup">That’s a Wrap! Kafka Summit San Francisco 2018 Roundup</a>. The San Francisco <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2018/">Kafka Summit</a> ran October 16 - 17, and this blog post is a summary of the conference. It also has links to some interesting sessions, and out of those, these are my three favorites:</p>

<ul>
<li><a href="https://www.confluent.io/kafka-summit-sf18/zen-and-the-art-of-streaming-joins">Zen and the Art of Streaming Joins—The What, When and Why</a>.</li>
<li><a href="https://www.confluent.io/kafka-summit-sf18/kafka-security-101-and-real-world-tips">Kafka Security 101 and Real-World Tips</a>.</li>
<li><a href="https://www.confluent.io/kafka-summit-sf18/breaking-down-a-sql-monolith">Breaking Down a SQL Monolith with Change Tracking, Kafka and KStreams/KSQL</a>.</li>
</ul></li>

<li><p><a href="https://data-artisans.com/blog/stateful-stream-processing-apache-flink-state-backends">Stateful Stream Processing: Apache Flink State Backends</a>. This post explores stateful stream processing and more precisely the different state backends available in Apache Flink. It presents the 3 state backends of Apache Flink, their limitations and when to use each of them depending on case-specific requirements.</p></li>

<li><p><a href="https://www.confluent.io/blog/apache-kafka-kubernetes-could-you-should-you">Apache Kafka on Kubernetes – Could you? Should you?</a>. This is a post discussing whether you should run Kafka on Kubernetes or not. At <a href="/derivco">work</a>, we are in the process of rolling out our first Kafka deployments, (not on Kubernetes), and this post definitely gives us &ldquo;food for thought&rdquo;.</p></li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Yeah, I kind of ask myself that question as well (what am I doing): at the moment I have a hard time getting any blog posts out, as I am busy at work as well as trying to get to grips with <strong>SQL Server 2019 Big Data Clusters</strong>. I hope to be able to publish something in a week (or twos) time.</p>

<p>In the meantime, if you are interested in <strong>SQL Server 2019</strong>, go and have a read at these two posts:</p>

<ul>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a>.</li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 42]]></title>
    <link href="http://nielsberglund.com/2018/10/21/interesting-stuff---week-42/" rel="alternate" type="text/html"/>
    <updated>2018-10-21T19:19:43+02:00</updated>
    <id>http://nielsberglund.com/2018/10/21/interesting-stuff---week-42/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/15/guidance-for-library-authors/">Guidance for library authors</a>. This blog post announces the publication of the <strong>.NET Library Guidance</strong>. It’s brand new set of articles for .NET developers who want to create high-quality libraries for .NET.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/16/automating-release-notes-with-azure-functions/">Automating Release Notes with Azure Functions</a>. This post is a walk through how Azure Functions and Azure Blob Storage can help generate release notes.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://eng.uber.com/uber-big-data-platform/">Uber’s Big Data Platform: 100+ Petabytes with Minute Latency</a>. This article looks in-depth at Uber&rsquo;s Hadoop platform and how the platform allows for analysis of over 100 petabytes of data with minimal latency.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/how-to-choose-a-machine-learning-model-some-guidelines">How to Choose a Machine Learning Model – Some Guidelines</a>. This blog post explores some broad guidelines for selecting machine learning models.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/event-driven-2-0">Event Driven 2.0</a>. In this article <a href="https://twitter.com/benstopford">Ben Stopford</a> discusses the next generation of event-driven architecture.</li>
<li><a href="https://www.youtube.com/watch?v=HeNegOzjnJY&amp;feature=em-uploademail">Jay Kreps | Kafka Summit SF 2018 Keynote (Kafka and Event-Oriented Architecture)</a>. The San Francisco <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2018/">Kafka Summit 2018</a> was held October 16 - 17, and this video is Jay Kreps keynote.</li>
<li><a href="https://www.youtube.com/watch?v=v2RJQELoM6Y&amp;feature=em-uploademail">Martin Kleppmann | Kafka Summit SF 2018 Keynote (Is Kafka a Database?)</a>. Another <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2018/">Kafka Summit 2018</a> keynote video. This is <a href="https://twitter.com/martinkl">Martin Kleppmann</a> comparing Kafka to databases.<br /></li>
<li><a href="https://www.infoq.com/articles/apache-kafka-best-practices-to-optimize-your-deployment">Apache Kafka: Ten Best Practices to Optimize Your Deployment</a>. An <a href="https://www.infoq.com/">InfoQ</a> article discussing the latest Kafka best practices for developers to manage the data streaming platform more effectively. Best practices include log configuration, proper hardware usage, Zookeeper configuration, replication factor, and partition count.</li>
<li><a href="https://data-artisans.com/blog/watermarks-in-apache-flink-made-easy">Watermarks in Apache Flink Made Easy</a>. This blog post looks at how watermarks work in Flink.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 41]]></title>
    <link href="http://nielsberglund.com/2018/10/14/interesting-stuff---week-41/" rel="alternate" type="text/html"/>
    <updated>2018-10-14T08:03:37+02:00</updated>
    <id>http://nielsberglund.com/2018/10/14/interesting-stuff---week-41/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/a-fast-serverless-big-data-pipeline-powered-by-a-single-azure-function/">A fast, serverless, big data pipeline powered by a single Azure Function</a>. This is a blog post about how to use Azure Serverless functions to build highly performant data pipelines. At work, we are looking at implementing something similar.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><p><a href="https://muratbuffalo.blogspot.com/2018/10/everything-is-broken.html">Everything is broken</a>. This is a very cool post by <a href="https://twitter.com/muratdemirbas">Murat</a> where he lists some very relevant quotes/statements from a recent <a href="https://www.meetup.com/Everything-Is-Broken/events/251899676/">Everything Is Broken</a> meetup he attended. Some of the quotes I particularly liked:</p>

<ul>
<li><em>Without observability you don&rsquo;t have chaos engineering, you have a chaos.</em></li>
<li><em>You don&rsquo;t know what you don&rsquo;t know, so dashboards are very limited utility. Dashboards are only for anticipated cases: every dashboard is an artifact of past failures. There are too many dashboards, and they are too slow.</em></li>

<li><p><em>prerequisites for chaos engineering:</em></p>

<ol>
<li><em>monitoring &amp; observability</em></li>
<li><em>on-call &amp; incident management</em></li>
<li><em>know the cost of your downtime per hour (British Airlines&rsquo;s 1 day outage costed $150 millon)</em></li>
</ol></li>

<li><p><em>How to choose a chaos experiment?</em></p>

<ul>
<li><em>identify top 5 critical systems</em></li>
<li><em>choose 1 system</em></li>
<li><em>whiteboard the system</em></li>
<li><em>select attack: resource/state/network</em></li>
<li><em>determine scope</em></li>
</ul></li>
</ul></li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/08/announcing-ml-net-0-6-machine-learning-net/">Announcing ML.NET 0.6 (Machine Learning .NET)</a>. Microsoft just released ML.NET 0.6, and this post highlights some of the new enhancements.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/@simon.aubury/machine-learning-kafka-ksql-stream-processing-bug-me-when-ive-left-the-heater-on-bd47540cd1e8">Machine learning &amp; Kafka KSQL stream processing — bug me when I’ve left the heater on</a>. I like this post as it combines two of my favorite topics: Streaming and Machine Learning. So anyway, the post is about how you can, by using Kafka and Machine Learning, monitor household power usage and alert when something out of the ordinary occurs.</li>
<li><a href="https://data-artisans.com/blog/an-introduction-to-acid-guarantees-and-transaction-processing">An introduction to ACID guarantees and transaction processing</a>. A while ago dataArtisans introduced serializable, distributed ACID transactions directly on data streams in Flink. This post here talks about the foundations of the capability.</li>
<li><a href="https://www.confluent.io/blog/ksql-recipes-available-now-stream-processing-cookbook">KSQL Recipes Available Now in the Stream Processing Cookbook</a>. A post which introduces the &ldquo;KSQL Cookbook&rdquo;: a collection of &ldquo;recipes&rdquo; designed to help people build event-driven, real-time systems.</li>
<li><a href="https://data-artisans.com/blog/how-apache-flink-manages-kafka-consumer-offsets">How Apache Flink manages Kafka consumer offsets</a>. This post explains with a step-by-step example of how Apache Flink works with Apache Kafka to ensure that records from Kafka topics are processed with exactly-once guarantees.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 40]]></title>
    <link href="http://nielsberglund.com/2018/10/07/interesting-stuff---week-40/" rel="alternate" type="text/html"/>
    <updated>2018-10-07T09:34:44+02:00</updated>
    <id>http://nielsberglund.com/2018/10/07/interesting-stuff---week-40/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/10/04/update-on-net-core-3-0-and-net-framework-4-8/">Update on .NET Core 3.0 and .NET Framework 4.8</a>. A blog post from the .NET engineering team, where they talk about the future of the .NET Framework and .NET Core. I wonder if this post was prompted by speculations recently about the future of the .NET Framework, where there were questions whether the .NET Framework 4.8 would be the last version, and all development would be concentrated on .NET Core.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/enabling-real-time-data-warehousing-with-azure-sql-data-warehouse/">Enabling real-time data warehousing with Azure SQL Data Warehouse</a>. This post is an announcement how <a href="https://www.striim.com/">Striim</a> now fully supports SQL Data Warehouse as a target for Striim for Azure. Striim is a system which enables continuous non-intrusive performant ingestion of enterprise data from a variety of sources in real time.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/event-streaming-new-big-thing-finance">Is Event Streaming the New Big Thing for Finance?</a>. An excellent blog post by <a href="https://twitter.com/benstopford">Ben Stopford</a> where he discusses the use of event streaming in the financial sector.</li>
<li><a href="https://www.confluent.io/blog/troubleshooting-ksql-part-2">Troubleshooting KSQL – Part 2: What’s Happening Under the Covers?</a>. The second post by <a href="https://twitter.com/rmoff">Robin Moffat</a> about debugging of KSQL. In this post - Robin, as the title says, goes under the covers to figure out what happens with KSQL queries.</li>
<li><a href="https://data-artisans.com/blog/6-things-to-consider-when-defining-your-apache-flink-cluster-size">6 things to consider when defining your Apache Flink cluster size</a>.  This post discusses how to plan and calculate a Flink cluster size. In other words; how to define the number of resources you need to run a specific Flink job.</li>
</ul>

<h2 id="ms-ignite">MS Ignite</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/2018/10/02/syllabuck-ignite-2018-conference/">Syllabuck: Ignite 2018 Conference</a>. A great list of MS Ignite sessions that <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a> found interesting! Now I know what to do in my spare time!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/10/03/customized-regression-model-for-airbnb-dynamic-pricing/">Customized regression model for Airbnb dynamic pricing</a>. This post by <a href="https://twitter.com/adriancolyer">Adrian</a> is about a white-paper which details the methods that Airbnb use to suggest prices to listing hosts.</li>
<li><a href="https://towardsdatascience.com/cleaning-and-preparing-data-in-python-494a9d51a878">Cleaning and Preparing Data in Python</a>. A post which lists Python methods and functions that helps to clean and prepare data.</li>
<li><a href="https://www.microsoft.com/en-us/research/blog/the-microsoft-infer-net-machine-learning-framework-goes-open-source/">The Microsoft Infer.NET machine learning framework goes open source</a>. A blog post from Microsoft Research, in which they announce the open-sourcing of <a href="https://dotnet.github.io/infer/">Infer.NET</a>. Is anyone else but me somewhat confused about the various data science frameworks that Microsoft has?</li>
<li><a href="https://towardsdatascience.com/how-to-build-a-simple-recommender-system-in-python-375093c3fb7d">How to build a Simple Recommender System in Python</a>. A blog post which discusses what a recommender system is and how you can use Python to build one.</li>
</ul>

<h2 id="what-is-niels-doing-wind">What Is Niels Doing (WIND)</h2>

<p>That is a good question! As you know, I wrote two blog posts about SQL Server 2019:</p>

<ul>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a></li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a></li>
</ul>

<p>My plan was to relatively quickly follow up those two posts with a third post how to run <strong>SQL Server Machine Learning Services</strong> on <strong>SQL Server 2019 on Linux</strong>, and do it inside a Docker container. After having spent some time trying to get it to work, (with no luck), I gave up and contacted a couple of persons in MS asking for help. The response was that, right now in <strong>SQL Server 2019 on Linux CTP 2.0</strong>, you cannot do it - bummer! The functionality will be in a future release.</p>

<p>I am now reworking the post I had started on to cover <strong>SQL Server Machine Learning Services</strong> in an <strong>Ubuntu</strong> based <strong>SQL Server 2019 on Linux</strong>. I should be able to publish something within a week or two.</p>

<p>I am also working on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series (still). Right now I have no idea when I can publish it - Sorry!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 39]]></title>
    <link href="http://nielsberglund.com/2018/09/30/interesting-stuff---week-39/" rel="alternate" type="text/html"/>
    <updated>2018-09-30T13:13:43+02:00</updated>
    <id>http://nielsberglund.com/2018/09/30/interesting-stuff---week-39/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://adamsitnik.com/ETW-Profiler/">Profiling .NET Code with BenchmarkDotNet</a>. If you want to benchmark your .NET code, you probably use <a href="https://benchmarkdotnet.org/">BenchMarkDotNet</a> (if you do not, you should). The man behind BenchMarkDotNet is <a href="https://twitter.com/SitnikAdam">Adam Sitnik</a>, and in the linked blog post he announces how you, soon, can use the EtwProfiler to profile benchmarked code! Very cool!</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/09/26/the-design-and-implementation-of-modern-column-oriented-database-systems/">The design and implementation of modern column-oriented database systems</a>. In this post, <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white paper about column-oriented databases. Having worked a little bit with SQL Server&rsquo;s column store indexes, it is very cool to get the &ldquo;lowdown&rdquo; on the design behind it.</li>
</ul>

<h2 id="azure-cloud">Azure Cloud</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/azure-databricks-delta-in-preview-9-regions-added-and-other-exciting-announcements/">Azure Databricks – Delta in preview, 9 regions added, and other exciting announcements</a>. A blog post announcing that Azure Databricks Delta is available in preview. This is very interesting since I have been &ldquo;chomping at the bits&rdquo;, to do some tests with Databricks Delta.</li>
<li><a href="https://azure.microsoft.com/en-us/blog/spark-debugging-and-diagnosis-toolset-for-azure-hdinsight/">Spark Debugging and Diagnosis Toolset for Azure HDInsight</a>. This post is another announcement from Microsoft. This time it is how <strong>Spark Diagnosis Toolset for HDInsight</strong> is now available in preview. The toolset allows you to identify low parallelization, to detect data skew and run data skew analysis, and quite a lot more.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-presence-detection-apache-kafka-aws">Real-Time Presence Detection at Scale with Apache Kafka on AWS</a>. This post discusses how <a href="https://www.zenreach.com/">Zenreach</a> has implemented a framework for real-time presence detection, using Kafka Streams.</li>
<li><a href="https://data-artisans.com/blog/state-ttl-for-apache-flink-how-to-limit-the-lifetime-of-state">State TTL for Apache Flink: How to Limit the Lifetime of State</a>. Instead of me summarising the post, I shamelessly copy the opening paragraph: <em>A common requirement for many stateful streaming applications is the ability to control how long application state can be accessed (e.g., due to legal regulations like GDPR) and when to discard it. This blog post introduces the state time-to-live (TTL) feature that was added to Apache Flink with the 1.6.0 release</em>. It is very, very interesting. I need to start to play around with Flink!</li>
<li><a href="https://www.confluent.io/blog/troubleshooting-ksql-part-1">Troubleshooting KSQL – Part 1: Why Isn’t My KSQL Query Returning Data?</a>. The obligatory Kafka link. The post is the first in a series how to troubleshoot KSQL. This and future posts in the series is, and, will be required reading for our Kafka team!</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/azure-data-studio-for-sql-server/">Azure Data Studio for SQL Server</a>. A post by <a href="https://twitter.com/vickyharp">Vicky Harp</a>. Vicky is Principal Program Manager Lead at Microsoft for SQL Server tooling, and in the post, she introduces <strong>Azure Data Studio</strong> (the artist formerly known as SQL Operations Studio). Azure Data Studio is a new cross-platform desktop environment for both on-premises and cloud data platforms on Windows, MacOS, and Linux.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/24/sql-server-2019-preview-combines-sql-server-and-apache-spark-to-create-a-unified-data-platform/">SQL Server 2019 preview combines SQL Server and Apache Spark to create a unified data platform</a>. An announcement by Microsoft how SQL Server 2019 comes with support for both Spark as well as Hadoop File System (HDFS). We do live in exciting times!</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/introducing-microsoft-sql-server-2019-big-data-clusters/">Introducing Microsoft SQL Server 2019 Big Data Clusters</a>. This post builds on top of the post above. It discusses how we can create big data clusters utilising the support in SQL Server 2019 of Spark and HDFS.</li>
<li><a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a>. A post by yours truly. I do a quick look at what is new in SQL Server 2019, and I especially look at the Java language extension.</li>
<li><a href="/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/">SQL Server 2019 for Linux in Docker on Windows</a>. Another post my myself. Since SQL Server 2019 for Linux now have support for SQL Server Machine Learning Services, I want to have a look at how it works. For that I obviously need it installed and I decided to install it as a Docker for Windows container. The post walks through what I did to get it installed. The post also discusses Azure Data Studio briefly.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 for Linux in Docker on Windows]]></title>
    <link href="http://nielsberglund.com/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/" rel="alternate" type="text/html"/>
    <updated>2018-09-29T12:06:09+02:00</updated>
    <id>http://nielsberglund.com/2018/09/29/sql-server-2019-for-linux-in-docker-on-windows/</id>
    <content type="html"><![CDATA[<p>By the time I publish this blog post <a href="https://www.microsoft.com/en-us/ignite">MS Ignite</a> is over. During Ignite, Microsoft announced quite a few new things, amongst them <strong>SQL Server 2019</strong> with a whole lot of new features and functionality.</p>

<p>I touched briefly on some of them in my <a href="/2018/09/24/what-is-new-in-sql-server-2019-public-preview/">What is New in SQL Server 2019 Public Preview</a> post. A couple of things that caught my eye were that <strong>SQL Server 2019 for Linux</strong> now supports In-Database analytics, what we know as <strong>SQL Server Machine Learning Services</strong> (R and Python), as well as the Java language extension.</p>

<blockquote>
<p><strong>NOTE:</strong> You may ask yourself what the Java language extension is; well, that is the ability to access Java code from T-SQL. It is a little bit like SQLCLR, but it executes outside of the SQL Server memory and process space.</p>
</blockquote>

<p>Seeing that I never really have played around with <em>SQL Server for Linux</em>, mostly due to that in previous versions (2017) it did not have support for In-Database analytics, I thought that now would be a good time to have a look.</p>

<p></p>

<p>Cool, so install <em>SQL Server 2019 for Linux</em> and start to play around! Hmm, what do I install it on - I am a Windows guy, this whole Linux thing is &ldquo;scary&rdquo;. Ok, I guess I could spin up a virtual machine and install it there, but I am lazy. Create a VM, and then install SQL Server seemed like too much work.</p>

<p>Then I thought about my mate and colleague <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>, a.k.a <a href="https://charlla.com/kafka-donuts/">The Donut Maker</a>, and how he raves about Docker. So maybe I should run <em>SQL Server 2019 for Linux</em> in a container, problem solved. Uh, maybe not; you see - I have never used Docker. I am an old guy (some would even call me a &ldquo;Grumpy Old Man&rdquo;, a <em>GOM</em>), and you know the saying about old dogs and new tricks.</p>

<p>So anyway, I decided to give it a go; how hard can it be (it turns out not hard at all), and this post is about the steps I took to get <em>SQL Server 2019 for Linux</em> running in Docker on Windows.</p>

<h2 id="docker-for-windows">Docker for Windows</h2>

<p>This post does not cover how to download and install Docker for Windows, as there are lots of posts out there about it. If you want somewhere to start; <a href="https://docs.docker.com/v17.09/docker-for-windows/">Get started with Docker for Windows</a> is an excellent starting point.</p>

<p>I do, however, want to point out a couple of things, that caught me out:</p>

<ul>
<li>Hyper-V needs to be enabled on your host computer. This means you cannot run Virtual Box VM&rsquo;s at the same time.</li>
<li>When you install Docker, you decide whether you want to run Linux or Windows containers. So, if you install Docker for Windows intending to run <em>SQL Server 2019 for Linux</em>, you choose Linux containers.</li>
</ul>

<p>You can change the choice between Linux and Windows containers from the Docker icon in the system tray (right click on the icon):</p>

<p><img src="/images/posts/sql_2k19Docker_container_type.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Change Container Type</em></p>

<p>In <em>Figure 1</em> we see the menu entry to change the Docker container type to Windows. To change container type works the other way around as well; changing from Windows to Linux.</p>

<h2 id="docker-basics">Docker Basics</h2>

<p>Before we look at how to get and install the SQL Server &ldquo;stuff&rdquo; let us discuss some basics, and let us start with some vocabulary:</p>

<ul>
<li>Layer: a set of read-only files or commands that describe how to set up the underlying system beneath the container</li>
<li>Image: this is the piece of &ldquo;something&rdquo;, in our case <em>SQL Server 2019 for Linux</em>, that you want to install. The image consists of one or more layers.</li>
<li>Container: you download an image and create a container, and this is what you interact with.</li>
<li>Registry: where images are stored and delivered from.</li>
</ul>

<p>In our case we:</p>

<ul>
<li>Connect to a registry which contains a <em>SQL Server 2019 for Linux</em> image.</li>
<li>We download the image and create a container.</li>
<li>We &ldquo;run&rdquo; the container and interact with SQL Server.</li>
</ul>

<p>The interaction with Docker (download image, create a container, etc.) is via CLI (Command Line Interface), using the <code>docker</code> base command followed by child commands and options/parameters (<code>docker childcommand</code>). Examples of child commands:</p>

<ul>
<li><code>login</code>: logs in to a Docker registry.</li>
<li><code>pull</code>: retrieve an image from a registry.</li>
<li><code>images</code>: returns a list of images on the machine.</li>
<li><code>run</code>: creates a new container from an image and starts it. If the image has not been <code>pull</code>:ed yet, it also pulls the image.</li>
<li><code>ps</code>: Lists containers.</li>
<li><code>exec</code>: executes a command in a container. For example, you want to run a command shell in the container.</li>
<li><code>stop</code>: stops a running container.</li>
<li><code>start</code>: starts up an existing stopped container.</li>
<li><code>rm</code>: removes a container.</li>
</ul>

<p>To see a full list of commands you can go <a href="https://docs.docker.com/engine/reference/commandline/docker/">here</a>.</p>

<p>As I mentioned above, we interact with Docker via the command line, and when you are on Windows, you most likely use <em>Powershell</em>. In this post I do it somewhat differently in that I do not use the actual <em>Powershell</em> shell, but instead <strong>Azure Data Studio</strong>.</p>

<h2 id="azure-data-studio">Azure Data Studio</h2>

<p>What is Azure Data Studio then? Well, it is the evolution of SQL Operations Studio. The blog post <a href="https://cloudblogs.microsoft.com/sqlserver/2018/09/25/azure-data-studio-for-sql-server/">Azure Data Studio for SQL Server</a>, introduces it like so:</p>

<p>*Azure Data Studio is a new cross-platform desktop environment for data professionals using the family of on-premises and cloud data platforms on Windows, MacOS, and Linux. Previously released under the preview name SQL Operations Studio, Azure Data Studio offers a modern editor experience with lightning fast IntelliSense, code snippets, source control integration, and an <strong>integrated terminal</strong>. It is engineered with the data platform user in mind, with built-in charting of query resultsets and customizable dashboards.*</p>

<p>We can think what we want about the &ldquo;blurb&rdquo; above, but <em>ADS</em> does have some interesting features, and for the Docker CLI work we use the integrated terminal:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Azure Data Studio and Integrated Terminal</em></p>

<p>What we see in <em>Figure 2</em> is <em>ADS</em> with visualised resultsets, some dashboards and - outlined in red - the integrated terminal. Now, let us get down to business.</p>

<h2 id="getting-the-sql-server-2019-for-linux-image">Getting the SQL Server 2019 for Linux Image</h2>

<p>We get the <em>SQL Server 2019 for Linux</em> Docker image from the <a href="https://azure.microsoft.com/en-us/blog/microsoft-syndicates-container-catalog/">Microsoft Container Registry</a> (MCR). MCR acts as a single download source for Microsoft’s container images. Regardless of where customers discover Microsoft images, the pull source is <a href="https://azure.microsoft.com/en-us/services/container-registry/">mcr.microsoft.com</a>.</p>

<p>To get the image I open <em>Azure Data Studio</em>:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio2.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Helper Files</em></p>

<p>We see in <em>Figure 3</em> how I have the <code>2k19_linux.ps</code> file open in the <em>ADS</em> editor, and how that file contains some Docker commands. I open the integrated terminal in <em>ADS</em> through <strong>Ctrl + `</strong>, or by using the menu: &ldquo;View | Command Palette | View: Toggle Integrated Terminal&rdquo;:</p>

<p><img src="/images/posts/sql_2k19Docker_azure_data_studio3.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Integrated Terminal</em></p>

<p>In <em>Figure 4</em> we see how the terminal is open (outlined in red) and it is the Powershell terminal (highlighted in red).</p>

<blockquote>
<p><strong>NOTE:</strong> The reason I use <em>ADS</em> is that I wanted to see what I can do with it, I could as easily have used the <em>Powershell</em> shell.</p>
</blockquote>

<p>Let us now get the SQL Server 2019 image, and I do it by copying the <code>docker pull ...</code>command from the file to the terminal and hit enter. In the terminal you now see something like so (output edited for readability):</p>

<pre><code class="language-bash">PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt; 
    docker pull mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
vNext-CTP2.0-ubuntu: Pulling from mssql/server
b234f539f7a1: Downloading [========&gt; ]  7.519MB/43.12MB
55172d420b43: Download complete
5ba5bbeb6b91: Download complete
43ae2841ad7a: Download complete
f6c9c6de4190: Download complete
28f02293f049: Download complete
5eb40916d530: Downloading [&gt;         ]   1.08MB/70.39MB
46e88947bdd0: Downloading [=&gt;        ]  8.634MB/414.5MB
26983ce22a89: Waiting
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Doing a Pull</em></p>

<p>We see in <em>Code Snippet 1</em> how Docker retrieves the image. In fact, it retrieves the layers the image consists of. The layers are identified by the <code>b234f539f7a1</code>, <code>55172d420b43</code>, and so forth as we see in <em>Code Snippet 1</em>. Eventually, the <code>pull</code> finishes, and we see in the terminal:</p>

<pre><code class="language-bash">PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt; 
    docker pull mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
vNext-CTP2.0-ubuntu: Pulling from mssql/server
b234f539f7a1: Pull complete
55172d420b43: Pull complete
5ba5bbeb6b91: Pull complete
43ae2841ad7a: Pull complete
f6c9c6de4190: Pull complete
28f02293f049: Pull complete
5eb40916d530: Pull complete
46e88947bdd0: Pull complete
26983ce22a89: Pull complete
Digest: sha256:87e691e2e5f738fd64a427ebe935e4e5ccd...
Status: Downloaded newer image for 
    mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
PS W:\nielsb-work\GitHub-Repos\sqlserver\dockerfiles&gt;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Pull Finished</em></p>

<p>After the <code>pull</code> command has finished, we can check what images we have by executing <code>docker images</code>. When I do it on my machine I see this:</p>

<p><img src="/images/posts/sql_2k19Docker_pulled images.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Pulled Docker Images</em></p>

<p>We see from <em>Figure 5</em> how the SQL Server image now exists on the machine.</p>

<h4 id="creating-a-container">Creating a Container</h4>

<p>Cool, we have an image. However, an image is just that, an image, and you cannot interact with it. To relate it to SQL Server, think about the image as an <code>.iso</code> install file. We need to &ldquo;install&rdquo; the image, e.g. create and run a container. For this we use the second <code>docker</code> command from  <em>Figure 3</em> above, and it looks like so:</p>

<pre><code class="language-bash">docker run -e &quot;ACCEPT_EULA=Y&quot; \ 
           -e &quot;SA_PASSWORD=&lt;Strong!Passw0rd&gt;&quot; \
           -p 1433:1433 \
           --name sql2k19_1 \
           -d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create &amp; Run a Container</em></p>

<p>In <em>Code Snippet 3</em> we see how we use the <code>docker run</code> command to create the container. Let us look at the options:</p>

<ul>
<li><code>-e &quot;ACCEPT_EULA=Y&quot;</code>: As creating the container also installs SQL Server, we need to accept the SQL Server EULA. The <code>-e</code> option (also <code>--env</code>) sets environment variables. In this case, environment variables SQL Server requires.</li>
<li><code>-e &quot;SA_PASSWORD=&lt;Strong!Passw0rd&gt;&quot;</code>: A second environment variable. When running SQL Server in a container, you need to set a password which follows the SQL Server default password policy. Otherwise, the container can not setup SQL server and will stop working. By default, the password must be at least 8 characters long and contain characters from three of the following four sets: Uppercase letters, Lowercase letters, Base 10 digits, and Symbols.</li>
<li><code>-p 1433:1433</code>: The <code>-p</code> (or <code>--expose</code>) option binds a port on the host machine (to the left of the colon) to a port on the container. If you run multiple SQL Server containers, the SQL Server container uses port 1433 by default, and you should use different port numbers for the host machine: <code>-p 1401:1433</code> for example.</li>
<li><code>--name sql2k19_1</code>: The <code>--name</code> option assigns a name to the container. This is like a SQL Server instance name.</li>
<li><code>-d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu</code>: This indicates which image to create a container from. The <code>-d</code> option tells Docker we want to run the container detached from the calling process. In other words, it is still up and running after you close the terminal.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> I mentioned above about the <code>-p</code> option that if you run multiple instances you should have different host ports. This is also true if you run a non Docker SQL Server instance on you machine.</p>
</blockquote>

<p>After we execute the code in <em>Code Snippet 3</em> we can check that we have a new container: <code>docker ps</code>:</p>

<p><img src="/images/posts/sql_2k19Docker_created_container.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Docker Container</em></p>

<p>From what we see in <em>Figure 6</em>, it looks like we are in business! If we want to we can connect into the container and, for example, run a bash shell:</p>

<pre><code class="language-bash">docker exec -it sql2k19_1  /bin/bash
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Run bash Shell in the Container</em></p>

<p>That is all well and good, but what about SQL Server?</p>

<h2 id="test-the-container">Test the Container</h2>

<p>Right, so now we have a container, and that container hopefully runs SQL Server. Let us try and connect to the SQL Server via <em>ADS</em>.</p>

<p>So I switch from the <em>Explorer</em> view to <em>Servers</em>: <strong>Ctrl + G</strong>, and I click <em>New Connection</em>:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_new_connection.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>New Connection</em></p>

<p>The <em>New Connection</em> is what is highlighted in red in <em>Figure 7</em>, and clicking it I get a connection dialog:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_connect.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>New Connection</em></p>

<p>In the connection dialog we see, in <em>Figure 8</em>, how I want to connect to localhost (the highlighted &ldquo;.&rdquo; in the <code>Server</code> text box), the password is whatever password I set in <em>Code Snippet 3</em>, and I chose to give the connection a name (the highlighted part in the <code>Name</code> text box). So if everything works, when I click on <em>Connect</em> I should see something like so:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_connected.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Successful Connection</em></p>

<p>As we see in <em>Figure 9</em> everything worked, and I am now connected to SQL Server 2019 for Linux, running in Docker container! To further prove all works I click on the &ldquo;New Query&rdquo; button (highlighted in red), and I execute a trivial <code>SELECT</code> statement: <code>SELECT * FROM sys.databases</code>:</p>

<p><img src="/images/posts/sql_2k19Docker_ADS_query_result.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Result from Select</em></p>

<p>In <em>Figure 10</em> we see how we get the result back! We can now continue working with <em>SQL Server 2019 for Linux</em>. If you for some reason want to shut down the container you run <code>docker stop &lt;containername&gt;</code> , and to start it up again - surprise, surprise - <code>docker start &lt;containername&gt;</code>.</p>

<h2 id="summary">Summary</h2>

<p>In this post we covered how we can run <em>SQL Server 2019 for Linux</em> in a Docker container on our Windows machine. We mentioned the Docker commands to use:</p>

<ul>
<li><code>docker pull</code></li>
<li><code>docker run</code></li>
<li><code>docker images</code></li>
<li><code>docker ps</code></li>
<li><code>docker stop</code></li>
<li><code>docker start</code></li>
</ul>

<p>We mentioned how we map a port on the hosting machine to a port on the container, and how we should use different host ports when we have multiple SQL Server instances. The SQL Server in the container is by default using port 1433.</p>

<p>In the post I also spoke about <em>Azure Data Studio</em> and some of its new functionality.</p>

<p>In future blog posts I will talk more about <em>SQL Server 2019 for Linux</em>, especially the In-Database analytics and the Java extensions, as well as <em>Azure Data Studio</em>.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is New in SQL Server 2019 Public Preview]]></title>
    <link href="http://nielsberglund.com/2018/09/24/what-is-new-in-sql-server-2019-public-preview/" rel="alternate" type="text/html"/>
    <updated>2018-09-24T19:17:06+02:00</updated>
    <id>http://nielsberglund.com/2018/09/24/what-is-new-in-sql-server-2019-public-preview/</id>
    <content type="html"><![CDATA[<p>If you read my roundup for <a href="/2018/09/23/interesting-stuff---week-38/">week 38</a>, which I published yesterday, you probably noticed that <a href="https://www.microsoft.com/en-us/ignite"><strong>MS Ignite</strong></a> started today. I mentioned in the post that I was particularly interested in some of the <strong>SQL Server</strong> sessions, as they looked very interesting.</p>

<p>However, even before the sessions started, Microsoft released SQL Server 2019 CTP 2.0 for public preview and, naturally, I jumped on the <a href="https://www.microsoft.com/en-us/evalcenter/evaluate-sql-server-2019-ctp">download link</a> and started downloading. I managed to get to the link in time before the rest of the world started the download, so I managed to get it down and then did an install.</p>

<p>The rest of this post is about my initial findings mostly in the SQL Server Machine Learning Services space.</p>

<blockquote>
<p><strong>NOTE:</strong> I have looked at SQL Server 2019 the grand total of an hour, so this is a short post.</p>
</blockquote>

<p></p>

<h2 id="installation-versions">Installation &amp; Versions</h2>

<p>First of all, the installation took forever, at least it felt that way. I believe it took around an hour, just for the install. So if you install, make sure you are not in a hurry.</p>

<p>I chose to install R and Python services in-database. After the installation finished, (finally), I enabled the machine learning services:</p>

<pre><code class="language-sql">EXEC sp_configure 'external scripts enabled', 1
RECONFIGURE WITH OVERRIDE
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Enable External Scripts</em></p>

<p>After executing the code in <em>Code Snippet 1</em>, I restarted the SQL Server 2019 instance, and then executed my regular &ldquo;check everything works&rdquo; code:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
              @language = N'R'
        , @script = N'd&lt;-42'

EXEC sp_execute_external_script 
              @language = N'Python'
        , @script = N'd=42'

EXEC sp_execute_external_script
                  @language = N'R' ,
                  @script = N'print(R.Version()$version)'

EXEC sp_execute_external_script 
              @language = N'Python'
, @script = N'
import sys
print (sys.version)'
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Test Code</em></p>

<p>As you see, the code is exceptionally advanced (not), but at least the code indicates if there are any issues. The last two <code>sp_execute_external_script</code> statements return the R and Python versions. For R the engine is now running on version <code>3.4.4</code> whereas in SQL Server 2017 it is <code>3.3.3</code>. For Python, it is the same version in both 2017 and 2019: <code>3.5.2</code>.</p>

<h2 id="extensibility-framework">Extensibility Framework</h2>

<p>So, when I read <a href="https://docs.microsoft.com/en-us/sql/sql-server/what-s-new-in-sql-server-ver15?view=sql-server-ver15">What&rsquo;s new in SQL Server 2019</a>, I came across a lot of interesting &ldquo;stuff&rdquo;, but one thing that stood out was <em>Java language programmability extensions</em>. In essence, it allows us to execute Java code in SQL Server by using a pre-built Java language extension! The way it works is as with R and Python; the code executes outside of the SQL Server engine, and you use <code>sp_execute_external_script</code> as the entry-point.</p>

<p>I haven&rsquo;t had time to execute any Java code as of yet, but in the coming days, I definitely will drill into this. Something I noticed is that the architecture for SQL Server Machine Learning Services has changed (or had additions to it). If you remember from my <a href="/sql_server_2k16_r_services">SQL Server Machine Learning Services</a> posts, the flow when executing <code>sp_execute_external_script</code> looked something like so:</p>

<ul>
<li>We execute <code>sp_execute_external_script</code>.</li>
<li>SQL Server connects to the Launchpad service.</li>
<li>Based on the <code>@language</code> parameter, Launchpad calls into either <code>rlauncher.dll</code> or <code>pythonlauncher.dll</code>.</li>
<li>The respective launcher then launches the external engine.</li>
</ul>

<p>If now Java is supported is there also a Java launcher? No, as it turns out, there is not, at least not what I could find. However what I did find was this:</p>

<p><img src="/images/posts/sql_2k19_ml_impr1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Common Launcher</em></p>

<p>In the same directory as the R and Python launchers, I see this new <code>commonlauncher.dll</code> together with a config file. When looking at the config file I did not see anything giving any hints to what goes on, but - as I said above - I will investigate.</p>

<p>At this stage I have two theories about what happens when you execute Java code:</p>

<ol>
<li>The Launchpad service knows about the Java extension: <code>javaextension.dll</code>, which is in the same directory as the launchers, and routes everything with <code>@language = Java</code> to the extension.</li>
<li>For any <code>@language</code> parameter that is not <code>R</code> or <code>Python</code>, the Launchpad service calls the <code>commonlauncher.dll</code>.</li>
</ol>

<p>That&rsquo;s more or less what I found out after an hours &ldquo;playing around&rdquo; with SQL Server 2019 CTP 2.0.</p>

<h2 id="other-interesting-stuff">Other Interesting Stuff</h2>

<p>In the beginning of this post I mentioned about interesting things I found in the <a href="https://docs.microsoft.com/en-us/sql/sql-server/what-s-new-in-sql-server-ver15?view=sql-server-ver15">What&rsquo;s new &hellip;</a> article. In no particular order:</p>

<h3 id="big-data-clusters">Big Data Clusters</h3>

<ul>
<li>Deploy a Big Data cluster with SQL Server and Spark Linux containers on Kubernetes</li>
<li>Access your big data from HDFS</li>
<li>Run Advanced analytics and machine learning with Spark</li>
<li>Use Spark streaming to data to SQL data pools</li>
<li>Run Query books that provide a notebook experience in Azure Data Studio.</li>
</ul>

<h3 id="data-discovery-and-classification">Data discovery and classification</h3>

<ul>
<li>Helps meet data privacy standards and regulatory compliance requirements.</li>
<li>Supports security scenarios, such as monitoring (auditing), and alerting on anomalous access to sensitive data.</li>
<li>Makes it easier to identify where sensitive data resides in the enterprise, so that administrators can take the right steps to secure the database.</li>
</ul>

<h3 id="sql-server-machine-learning-services-failover-clusters-and-partition-based-modeling">SQL Server Machine Learning Services failover clusters and partition based modeling</h3>

<ul>
<li>Partition-based modeling: Process external scripts per partition of your data using the new parameters added to <code>sp_execute_external_script</code>. This functionality supports training many small models (one model per partition of data) instead of one large model.</li>
<li>Windows Server Failover Cluster: Configure high availability for Machine Learning Services on a Windows Server Failover Cluster.</li>
</ul>

<h3 id="azure-data-studio">Azure Data Studio</h3>

<p>Previously released under the preview name SQL Operations Studio, Azure Data Studio is a lightweight, modern, open source, cross-platform desktop tool for the most common tasks in data development and administration. With Azure Data Studio you can connect to SQL Server on premises and in the cloud on Windows, macOS, and Linux.</p>

<h2 id="other-resources">Other Resources</h2>

<p><a href="https://twitter.com/aaronbertrand">Aaron Bertrand</a> has an <a href="https://www.mssqltips.com/sqlservertip/5710/whats-new-in-the-first-public-ctp-of-sql-server-2019/">awesome writeup</a> of what&rsquo;s new in SQL Server 2019 from a more database engine perspective. In that writeup he also points to more resources about SQL Server 2019.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 38]]></title>
    <link href="http://nielsberglund.com/2018/09/23/interesting-stuff---week-38/" rel="alternate" type="text/html"/>
    <updated>2018-09-23T06:47:28+02:00</updated>
    <id>http://nielsberglund.com/2018/09/23/interesting-stuff---week-38/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/changing-face-etl">The Changing Face of ETL</a>. An article by <a href="https://twitter.com/rmoff">Robin Moffat</a> about the &ldquo;new&rdquo; ETL, based on event-driven architectures and streaming platforms.</li>
<li><a href="https://charlla.com/kafka-donuts/">Kafka Donuts</a>. This post is the introduction and TOC to a series of posts about Kafka. The author is my colleague <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>, and in the series, he discusses the use of Kafka in a company who manufactures and sells Donuts. Reading the series introduction post, it is clear that this series is a <strong>MUST</strong> for everyone interested in Kafka. The first episode: <strong>Donut Broker</strong> is <a href="https://charlla.com/kafka-donuts-1/">here</a>, and the second episode <strong>Donut Baker</strong> is <a href="https://charlla.com/kafka-donuts-2/">here</a>.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.paraesthesia.com/archive/2018/09/20/docker-on-wsl-with-virtualbox-and-docker-machine/">Docker on Windows Subsystem for Linux using VirtualBox and Docker Machine</a>. This post by <a href="https://twitter.com/tillig">Travis Illig</a> discusses how you can enable both <strong>VirtualBox</strong> as well as <strong>Docker for Windows</strong> on the same Windows box.</li>
</ul>

<h2 id="microsoft-ignite">Microsoft Ignite</h2>

<p>So, <strong>Microsoft Ignite</strong> starts tomorrow (September 24). It looks to be an awesome conference with lots and lots of announcements of new &ldquo;stuff&rdquo;, I for one cannot wait!</p>

<p>If you, like me, are not attending but still want to follow the key-notes and various sessions, <a href="https://www.microsoft.com/en-us/ignite">this link</a> takes you to the live stream.</p>

<p>The other day I looked at the sessions and here are some that interests me:</p>

<ul>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65955?source=sessions">BRK2416 - The roadmap for SQL Server</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65356?source=sessions">BRK2183 - SQL Server Machine Learning Services: An E2E platform for machine learning</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65957?source=sessions">BRK3228 - What’s new in SQL Server on Linux and containers</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65956?source=sessions">BRK2229 - The future of SQL Server and big data</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66199?source=sessions">THR2168 - The next generation of SQL Server tools</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/65967?source=sessions">BRK4021 - Deep dive on SQL Server and big data</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66202?source=sessions">THR2171 - Deploying a highly available SQL Server solution in Kubernetes</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/64634?source=sessions">BRK3154 - SQL Server in containers for application development and DevOps</a></li>
<li><a href="https://myignite.techcommunity.microsoft.com/sessions/66961?source=sessions">THR2308 - SQL Server vNext meets AI and Big Data</a></li>
</ul>

<p>As you see, mostly SQL Server related sessions, and I must say that the sessions around SQL Server and Big Data intrigues me.</p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/how-machine-learning-works-3-resources-to-learn-and-develop-ml-applications">How Machine Learning Works: 3 Resources To Learn And Develop ML Applications</a>. The <a href="https://www.lightbend.com/">Lightbend</a> team has put together some resources about how to design, build, run and manage machine learning applications in production.</li>
<li><a href="https://databricks.com/blog/2018/09/18/simplify-market-basket-analysis-using-fp-growth-on-databricks.html">Simplify Market Basket Analysis using FP-growth on Databricks</a>. In retail, you want to recommend to shoppers what to purchase, and often you base the recommendations on items that are frequently purchased together. A key technique to uncover associations between different items is known as market basket analysis. This blog post talks about how you run your market basket analysis using <strong>Apache Spark MLlib</strong> <code>FP-growth</code> algorithm on <strong>Databricks</strong>.</li>
<li><a href="https://ziedhy.github.io/2018/08/Introduction_Deep_Learning.html">Introduction to Deep Learning</a>. This blog post is the first in a series about <strong>Deep Learning</strong>. At a quick glance, the series looks very informative.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I am still working on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it soon:ish.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 37]]></title>
    <link href="http://nielsberglund.com/2018/09/16/interesting-stuff---week-37/" rel="alternate" type="text/html"/>
    <updated>2018-09-16T08:17:48+02:00</updated>
    <id>http://nielsberglund.com/2018/09/16/interesting-stuff---week-37/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://www.red-gate.com/products/dba/sql-monitor/entrypage/execution-plans">SQL Server Execution Plans, 3rd Edition</a>. The third edition of <a href="https://twitter.com/gfritchey">Grant Fritchey&rsquo;s</a> excellent book about SQL Server Query Plans. If you are a developer or a DBA, you need to get this book (and read it).</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/event-flow-distributed-systems">Complex Event Flows in Distributed Systems</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation how lightweight and highly-scalable state machines ease the handling of complex logic and flows in distributed systems.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/real-time-data-analytics-and-azure-data-lake-storage-gen2/">Real-time data analytics and Azure Data Lake Storage Gen2</a>. Microsoft recently announced <a href="https://azure.microsoft.com/en-us/services/storage/data-lake-storage/">Azure Data Lake Storage Gen 2</a> (ADLS2), and this blog post looks at how ADLS2 can be used for real-time analytics. ADLS2 is at the moment in preview. I certainly hope that MS releases it soon.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/netflix-techblog/keystone-real-time-stream-processing-platform-a3ee651812a">Keystone Real-time Stream Processing Platform</a>. This is a blog post about Keystone; Netflix’s data backbone. It is an essential piece of infrastructure focusing on data analytics. I found this post very interesting, and if you are interested in stream processing, you should really read this post.</li>
<li><a href="https://www.confluent.io/blog/streams-tables-two-sides-same-coin">Streams and Tables: Two Sides of the Same Coin</a>. This blog post announces the availability of the white-paper <a href="https://www.confluent.io/thank-you/streams-and-tables-two-sides-of-the-same-coin/">Streams and Tables: Two Sides of the Same Coin</a>. The paper introduces the Dual Streaming Model, which is used to reason about physical and logical order in data stream processing. This is a <strong>MUST</strong> read!</li>
<li><a href="https://www.confluent.io/blog/building-streaming-application-ksql/">Hands on: Building a Streaming Application with KSQL</a>. In this blog post, we see how we can build a demo streaming application with KSQL, the streaming SQL engine for Apache Kafka. The application continuously computes, in real time, top music charts based on a stream of song play events.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>So the SQL Saturday &ldquo;season&rdquo; is over for me for this year. I did one talk in <a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Johannesburg</a>, two in Cape Town (<a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">this</a> and <a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">this</a>), and one in <a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">Durban</a>.</p>

<p>In addition to the conference talks I also did a full-day workshop in Cape Town and Durban about SQL Server Machine Learning Services: <strong><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">A Day of SQL Server Machine Learning Services with Niels Berglund</a></strong>.</p>

<p>When we talk about SQL Saturdays I want to thank the organisers in the various cities:</p>

<ul>
<li><a href="https://twitter.com/MikeJohnsonZA/">Michael Johnson</a> and team in Johannesburg.</li>
<li><a href="https://twitter.com/Jody_WP">Jody Roberts</a> and <a href="https://twitter.com/TheSQLGirl">Jeanne Combrink</a> and their team in Cape Town.</li>
<li><a href="https://www.linkedin.com/in/jodi-craig-1827b844/">Jodi Craig</a> and team in Durban.</li>
</ul>

<p>They are doing a fantastic work, entirely voluntarily. A HUGE, HUGE <strong>THANK YOU</strong> to all of you!</p>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>Now when SQL Saturday is over, I plan to get back to write about <strong>SQL Server Machine Learning Services</strong>. I am working right now on the third post in the <a href="/series/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it in a week or two.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 36]]></title>
    <link href="http://nielsberglund.com/2018/09/09/interesting-stuff---week-36/" rel="alternate" type="text/html"/>
    <updated>2018-09-09T21:11:32+02:00</updated>
    <id>http://nielsberglund.com/2018/09/09/interesting-stuff---week-36/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/articles/Async-Streams">Async Streams in C# 8</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about, as the title says, C# support for async streams. This, proposed, new functionality in C# is to combine the async/awaiting feature with a yielding operator. Very interesting!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/microservices-post-kubernetes">Microservices in a Post-Kubernetes Era</a>. Another article from <a href="https://www.infoq.com/">InfoQ</a>. This article questions some of the original microservices ideas and acknowledges the fact that they are not standing as strong in the post-Kubernetes era as they were before. Well worth a read!</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/09/05/introduction-to-azure-durable-functions/">Introduction to Azure Durable Functions</a>. This post, from one of the .NET engineering teams is about Azure Durable Functions. Azure Durable functions is a new programming model based on Microsoft serverless’ platform Azure Functions. It allows you to write a workflow as code and have the execution run with the scalability and the reliability of serverless with high throughput.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/category/devops/">DevOps for Data Science </a>. A series of posts by <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a> where he discusses various aspects of DevOps in a Data Science world. This is a must read!</li>
<li><a href="https://www.confluent.io/blog/putting-power-apache-kafka-hands-data-scientists/">Putting the Power of Apache Kafka into the Hands of Data Scientists</a>. A blogpost which combines two of my favorite topics: Kafka and Data Science, how good is that?! The post discusses how they at <a href="https://www.stitchfix.com/">Stitch Fix</a> exposes a multitude of data sources to their Data Scientists, and allow the Data Scientists to create new topics etc., on the fly. This is a <strong>MUST</strong> read post!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/ebook/i-heart-logs-event-data-stream-processing-and-data-integration/">I Heart Logs: Event Data, Stream Processing, and Data Integration</a>. Registration page for downloading <a href="https://twitter.com/jaykreps">Jay Kreps</a> e-book &ldquo;I Heart Logs&rdquo;. If you are interested in streaming in general, you <strong>SHOULD</strong> really get this book!</li>
<li><a href="https://data-artisans.com/blog/serializable-acid-transactions-on-streaming-data">Serializable ACID Transactions on Streaming Data</a>. Guys, (and girls), this is <strong>BIG</strong>. This blogpost introduces <em>data Artisans Streaming Ledger</em>, a new technology that brings serializable ACID transactions to applications built on a streaming architecture!</li>
<li><a href="https://www.confluent.io/blog/data-wrangling-apache-kafka-ksql">Data Wrangling with Apache Kafka and KSQL</a>. A post by <a href="https://twitter.com/rmoff">Robin Moffat</a> about how we can use Kafka and KSQL to manipulate data.</li>
<li><a href="https://www.infoq.com/articles/democratizing-stream-processing-kafka-ksql-part2">Democratizing Stream Processing with Apache Kafka® and KSQL - Part 2</a>. Part 2 in a series of posts by <a href="https://twitter.com/rmoff">Robin Moffat</a> about Kafka and KSQL. In this post Robin covers how Apache Kafka® and KSQL can be used to build powerful data integration and processing applications. You find Part 1 in the series <a href="https://www.infoq.com/articles/democratizing-stream-processing-apache-kafka-ksql">here</a>.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>This weekend I did two talks at SQL Saturday in Cape Town:</p>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>I also did a full day workshop on the Friday (September 7) before the event: <strong><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a></strong>. I had 16 people attending, and I believe it went quite well!</p>

<p>Now there is only one SQL Saturday left, in Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<p>In Durban I also do a full day workshop on the Friday, (September 14), before the event: <strong><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">A Day of SQL Server Machine Learning Services with Niels Berglund</a></strong>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 35]]></title>
    <link href="http://nielsberglund.com/2018/09/02/interesting-stuff---week-35/" rel="alternate" type="text/html"/>
    <updated>2018-09-02T08:38:38+02:00</updated>
    <id>http://nielsberglund.com/2018/09/02/interesting-stuff---week-35/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<p>The content this week is a bit meagre, due to me not having had time to browse around that much, as I have been &ldquo;prepping&rdquo; for the upcoming <strong>SQL Saturdays</strong>.</p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://www.microsoft.com/en-us/research/blog/optimizing-imperative-functions-in-relational-databases-with-froid/">Optimizing imperative functions in relational databases with Froid</a>. A post from Microsoft Research, where they discuss <strong>Froid</strong>. Froid is an extensible framework for optimising imperative programs in relational databases, and the purpose is to enable developers to use the abstraction of UDFs without compromising on performance.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/hadoop-for-beginners">Hadoop for Beginners- Part 1</a>. The first post in a series about, as the title says, Hadoop. This series is really worthwhile reading if you are interested in what Hadoop is and what it can do for you.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/apache-kafka-talk-series/">Apache Kafka: Online Talk Series</a>. This is a registration page for an on-demand video series about Kafka. I bring the popcorn, and you bring the coke!</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>In a couple of previous roundups I have mentioned that the SQL Saturday &ldquo;season&rdquo; is here, and yesterday, (September, 1), I flew out to Johannesburg and did a presentation about SQL Server Machine Learning Services, <a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a> to around 40 people.</p>

<p>The event was a smashing success thanks to the awesome arrangements by <a href="https://twitter.com/MikeJohnsonZA/">Michael Johnson</a> and his fellow volunteers!</p>

<p>Having done Johannesburg, next in turn is Cape Town:</p>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>and finally Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 34]]></title>
    <link href="http://nielsberglund.com/2018/08/26/interesting-stuff---week-34/" rel="alternate" type="text/html"/>
    <updated>2018-08-26T10:22:37+02:00</updated>
    <id>http://nielsberglund.com/2018/08/26/interesting-stuff---week-34/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="http://mattwarren.org/2018/08/21/Monitoring-and-Observability-in-the-.NET-Runtime/">Monitoring and Observability in the .NET Runtime</a>. Yet another awesome blog-post by <a href="https://twitter.com/matthewwarren">Matthew</a>. This post covers how we can monitor .NET through <em>Diagnostics</em>, <em>Profiling</em> and <em>Debugging</em>.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/chaos-engineering-introduction">Chaos Engineering: Building Immunity in Production Systems</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation discussing Chaos Engineering, its purpose, how to go about it, metrics to collect, the purpose of monitoring and logging, etc.</li>
</ul>

<h2 id="databases-storage">Databases / Storage</h2>

<ul>
<li><a href="https://queue.acm.org/detail.cfm?id=3236388">Mind Your State for Your State of Mind</a>. A paper by <a href="https://twitter.com/pathelland">Pat Helland</a>, where Pat explores the evolution of computation from a single process to microservices, the evolution of storage from files to key-value, and how they interact. Just as a side-note, you should read anything by Pat. He certainly knows what he is talking about!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/pakk-your-alpakka-reactive-streams-integrations-for-aws-azure-google-cloud">Pakk Your Alpakka: Reactive Streams Integrations For AWS, Azure, &amp; Google Cloud</a>. The link of this post had Cloud in the title, but I believe it fits better into <em>Streaming</em>. Anyway, this post links to a webinar about <a href="https://akka.io/blog/2016/08/23/intro-alpakka">Alpakka</a>. Alpakka is an integration framework for Akka Streams and the webinar looks at how Alpakka can be used for integrations with other systems.</li>
<li><a href="https://www.youtube.com/watch?v=p9LBi11KR2c">Pat Helland | Kafka Summit 2017 Keynote (Standing on the Distributed Shoulders of Giants)</a>. Speaking about <a href="https://twitter.com/pathelland">Pat</a>. Here is a YouTube video from his Kafka Summit keynote in 2017. It is based on a paper he published in 2016: <a href="https://queue.acm.org/detail.cfm?id=2953944">Standing on Distributed Shoulders of Giants</a>.</li>
</ul>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/logical-index-organization-in-cosmos-db.html">Logical index organization in Cosmos DB</a>. Another Cosmos DB post by <a href="https://twitter.com/muratdemirbas">Murat</a>. In this post, he looks at Cosmos DB&rsquo;s logical indexing subsystem.</li>
<li><a href="https://towardsdatascience.com/from-big-data-to-micro-services-how-to-serve-spark-trained-models-through-aws-lambdas-ebe129f4849c">From Big Data to micro-services: how to serve Spark-trained models through AWS lambdas</a>. This blog-post looks at how you take a Spark trained model, deploy it to AWS and expose it as an AWS Lambda endpoint. Very cool!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/08/22/snorkel-rapid-training-data-creation-with-weak-supervision/">Snorkel: rapid training data creation with weak supervision</a>. In this post <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white paper which tackles one of the central questions in supervised machine learning: how do you get a large enough set of training data to power modern deep models?</li>
<li><a href="http://luisquintanilla.me/2018/08/21/serverless-machine-learning-mlnet-azure-functions/">Serverless Machine Learning with ML.NET and Azure Functions</a>. Earlier in this weeks roundup, I linked to a post about Spark models and AWS Lambdas. This post talks about training a classification model using <a href="https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet">ML.NET</a> and deploy it with <a href="https://azure.microsoft.com/en-us/services/functions/">Azure Functions</a>.</li>
<li><a href="https://blogs.msdn.microsoft.com/data_insights_global_practice/2018/08/22/measuring-model-goodness-part-1/">Measuring Model Goodness – Part 1</a>. This post, which is part one of two-part series, is focused on measuring model goodness, specifically looking at quantifying business value and converting typical machine learning performance metrics (like precision, recall, RMSE, etc.) to business metrics.</li>
<li><a href="http://101.datascience.community/2018/08/24/microsoft-weekly-data-science-news-for-august-24-2018">MICROSOFT WEEKLY DATA SCIENCE NEWS FOR AUGUST 24, 2018</a>. I found the <a href="https://blogs.msdn.microsoft.com/data_insights_global_practice/2018/08/22/measuring-model-goodness-part-1/">Measuring Model Goodness</a> post above thanks to <a href="https://twitter.com/ryanswanstrom">Ryan&rsquo;s</a> blog and this post. Ryan&rsquo;s blog is awesome if you are interested in what Microsoft does in data science and AI!</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>The third post in the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series, which I promised a couple of weeks ago would soon be finished has to wait a bit. Reason for this is my prep for the upcoming SQL Saturdays.</p>

<p>As usual I present in Johannesburg, Cape Town and Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/785/EventHome.aspx">Johannesburg</a>, September 1:

<ul>
<li><a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a>.</li>
</ul></li>
</ul>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

